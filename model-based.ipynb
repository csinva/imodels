{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import time, sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# short decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 9.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "boston_data = pd.read_csv(\"data/boston.csv\", index_col=0)\n",
    "\n",
    "y = boston_data.medv.values\n",
    "X = boston_data.drop(\"medv\", axis=1)\n",
    "features = X.columns\n",
    "X = X.values\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4)\n",
    "dt.fit(X, y)\n",
    "\n",
    "preds = dt.predict(X)\n",
    "print(f'train mse: {np.mean(np.square(preds-y)):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integer linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tmse:  2.09\t[ 1  1 -1  0  0  0  0  0  0  0]\n",
      "0.01\tmse:  2.09\t[ 1  1 -1  0  0  0  0  0  0  0]\n",
      "0.05\tmse:  1.02\t[ 1  2 -1  0  0  0  0  0  0  0]\n",
      "0.1\tmse:  1.02\t[ 1  2 -1  0  0  0  0  0  0  0]\n",
      "1\tmse:  3.08\t[ 0  1 -1  0  0  0  0  0  0  0]\n",
      "2\tmse:  1.90\t[ 0  2 -1  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy\n",
    "\n",
    "np.random.seed(123) # for reproducability\n",
    "\n",
    "# generate X and y\n",
    "n, p = 1000, 10\n",
    "X = np.random.randn(n, p)\n",
    "y = X[:, 0] + 2 * X[:, 1] - 1 * X[:, 2] + np.random.randn(n)\n",
    "\n",
    "\n",
    "def fit_integer_linear_model(X, y, lambda_reg=0):\n",
    "    # declare the integer-valued optimization variable\n",
    "    w = cvxpy.Variable(X.shape[1], integer=True)\n",
    "\n",
    "    # set up the minimization problem\n",
    "    obj = cvxpy.Minimize(cvxpy.norm(X * w - y, 2) + lambda_reg * cvxpy.norm(w, 1))\n",
    "    prob = cvxpy.Problem(obj)\n",
    "\n",
    "    # solve the problem using an appropriate solver\n",
    "    sol = prob.solve(solver = 'ECOS_BB')\n",
    "\n",
    "    # the optimal value\n",
    "    return w.value.astype(np.int)\n",
    "\n",
    "for lambda_reg in [0, 1e-2, 5e-2, 1e-1, 1, 2]:\n",
    "    w = fit_integer_linear_model(X, y, lambda_reg)\n",
    "    mse = np.mean(np.square(y - X @ w))\n",
    "    print(f'{lambda_reg}\\tmse: {mse: 0.2f}\\t{w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rulefit\n",
    "- must be installed with: `pip install git+git://github.com/christophM/rulefit.git`\n",
    "- some docs: `https://github.com/christophM/rulefit`\n",
    "- original paper: `http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rulefit import RuleFit\n",
    "\n",
    "boston_data = pd.read_csv(\"data/boston.csv\", index_col=0)\n",
    "\n",
    "y = boston_data.medv.values\n",
    "X = boston_data.drop(\"medv\", axis=1)\n",
    "features = X.columns\n",
    "X = X.values\n",
    "\n",
    "rf = RuleFit()\n",
    "rf.fit(X, y, feature_names=features)\n",
    "\n",
    "preds = rf.predict(X)\n",
    "print(f'train mse: {np.mean(np.square(preds-y)):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*inspect the rules*\n",
    "- rule is how the feature is constructed\n",
    "- coef is its weight in the final linear model\n",
    "- support is how many points it applies to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = rf.get_rules()\n",
    "\n",
    "rules = rules[rules.coef != 0].sort_values(\"support\", ascending=False)\n",
    "\n",
    "print(rules[['rule', 'coef', 'support']]) #.sort_values('coef'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scalable bayesian rule lists (in C)\n",
    "- docs at `https://github.com/myaooo/pysbrl`\n",
    "- requires installing some `c` packages (ex. instructions [here](https://coral.ise.lehigh.edu/jild13/2016/07/11/hello/))\n",
    "- note: input format is pretty strange (it is sensible in [R](https://rdrr.io/cran/sbrl/man/sbrl.html))\n",
    "- only works with categorical variables (continuous must be discretized somehow)\n",
    "- there is another implementation in the [skater](https://oracle.github.io/Skater/install.html) package as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if {c4=o,c5=o,c6=o}: 0.04\n",
      "elif {c1=x,c7=x}: 0.86\n",
      "elif {c5=x,c7=o,c9=o}: 0.58\n",
      "elif {c1=x,c2=x,c3=x}: 0.98\n",
      "elif {c5=o,c9=o}: 0.02\n",
      "elif {c1=o,c2=o,c3=o}: 0.04\n",
      "elif {c3=x,c4=b,c9=x}: 0.93\n",
      "elif {c7=o,c9=o}: 0.08\n",
      "elif {c1=x,c2=b,c3=x}: 0.88\n",
      "elif {c5=o,c7=o}: 0.17\n",
      "elif {c6=o,c9=o}: 0.43\n",
      "elif {c4=x,c5=o}: 0.33\n",
      "elif {c1=o,c4=o,c7=o}: 0.05\n",
      " default: 0.98\n"
     ]
    }
   ],
   "source": [
    "# this example has categorical variables c1, c2, ... that take on values 'o', 'x', 'b'\n",
    "import pysbrl\n",
    "\n",
    "rule_ids, outputs, rule_strings = pysbrl.train_sbrl(\"data/train.out\", \n",
    "                                                    \"data/train.label\", \n",
    "                                                    20.0, eta=2.0, max_iters=2000)\n",
    "\n",
    "class_num = 1\n",
    "for i, rule_id in enumerate(rule_ids):\n",
    "    prefix = 'if' if i == 0 else 'elif' if i < len(rule_ids) -1 else ''\n",
    "    print(prefix, rule_strings[rule_id] + f': {outputs[i, class_num]:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal classification tree\n",
    "- docs here: `https://github.com/pan5431333/pyoptree`\n",
    "- note: this implementation is still unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyoptree.optree import OptimalHyperTreeModel, OptimalTreeModel\n",
    "\n",
    "data = pd.DataFrame({\n",
    "        \"index\": ['A', 'C', 'D', 'E', 'F'],\n",
    "        \"x1\": [1, 2, 2, 2, 3],\n",
    "        \"x2\": [1, 2, 1, 0, 1],\n",
    "        \"y\": [1, 1, -1, -1, -1]\n",
    "    })\n",
    "test_data = pd.DataFrame({\n",
    "    \"index\": ['A', 'B', 'C', 'D', 'E', 'F', 'G'],\n",
    "    \"x1\": [1, 1, 2, 2, 2, 3, 3],\n",
    "    \"x2\": [1, 2, 2, 1, 0, 1, 0],\n",
    "    \"y\": [1, 1, 1, -1, -1, -1, -1]\n",
    "})\n",
    "model = OptimalHyperTreeModel([\"x1\", \"x2\"], \"y\", tree_depth=2, N_min=1, alpha=0.1, solver_name=\"cplex\")\n",
    "model.train(data)\n",
    "\n",
    "print(model.predict(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
