<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>imodels.rule_list.bayesian_rule_list.brl_util API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodels.rule_list.bayesian_rule_list.brl_util</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#######Supplement for &#34;Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.&#34;

###LICENSE
#
# This software is released under the MIT license.
#
# Copyright (c) 2013-14 Ben Letham
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the &#34;Software&#34;), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#
# The author/copyright holder can be contacted at bletham@mit.edu

####README
# It is specific to binary classification with binary features (although could
# easily be extended to multiclass).
#
#
# ##OUTPUT
#
# The highest level function, &#34;topscript,&#34; returns:
#
# - permsdic - Contains the important information from the MCMC sampling. A
# dictionary whose keys are a string Pickle-dump of the antecedent list d, and
# whose values are a list [a,b] where a is (proportional to) the log posterior of
# d, and b is the number of times d is present in the MCMC samples.
# - d_star - the BRL-point antecedent list. A list of indicies corresponding to
# variable &#34;itemsets.&#34;
# - itemsets - A list of itemsets. itemsets[d_star[i]] is the antecedent in
# position i on the BRL-point list
# - theta - A list of the expected value of the posterior consequent
# distribution for each entry in BRL-point.
# - ci_theta - A list of tuples, each the 95% credible interval for the
# corresponding theta.
# - preds_d_star - Predictions on the demo data made using d_star and theta.
# - accur_d_star - The accuracy of the BRL-point predictions, with the decision
# boundary at 0.5.
# - preds_fullpost - Predictions on the demo data using the full posterior
# (BRL-post)
# - accur_fullpost - The accuracy of the BRL-post predictions, decision boundary
# at 0.5.
#
import pickle as Pickle
import time
from collections import defaultdict

from numpy import *
from scipy.special import gammaln
from scipy.stats import poisson, beta

try:
    from matplotlib import pyplot as plt
except:
    pass


###############BRL


def default_permsdic():
    &#39;&#39;&#39;For producing the defaultdict used for storing MCMC results
    &#39;&#39;&#39;
    return [0., 0.]


def reset_permsdic(permsdic):
    &#39;&#39;&#39;Resets the number of MCMC samples stored (value[1]) while maintaining the
    log-posterior value (so it doesn&#39;t need to be re-computed in future chains).
    &#39;&#39;&#39;
    for perm in permsdic:
        permsdic[perm][1] = 0.
    return permsdic


def run_bdl_multichain_serial(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin,
                              nchains, d_inits, verbose=True, seed=42):
    &#39;&#39;&#39;Run mcmc for each of the chains in serial
    &#39;&#39;&#39;
    # random seed
    random.seed(seed)

    # Run each chain
    t1 = time.process_time()
    if verbose:
        print(&#39;Starting mcmc chains&#39;)
    res = {}
    for n in range(nchains):
        res[n] = mcmcchain(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin,
                           nchains, d_inits[n])

    if verbose:
        print(&#39;Elapsed CPU time&#39;, time.process_time() - t1)

    # Check convergence
    Rhat = gelmanrubin(res)

    if verbose:
        print(&#39;Rhat for convergence:&#39;, Rhat)
    ##plot?
    # plot_chains(res)
    return res, Rhat


def mcmcchain(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin, nchains,
              d_init):
    &#39;&#39;&#39;Run and store mcmc chain
    &#39;&#39;&#39;
    res = {}
    permsdic, res[&#39;perms&#39;] = bayesdl_mcmc(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs,
                                          permsdic, burnin, None, d_init)
    # Store the permsdic results
    res[&#39;permsdic&#39;] = {perm: list(vals) for perm, vals in permsdic.items() if vals[1] &gt; 0}
    # Reset the permsdic
    permsdic = reset_permsdic(permsdic)
    return res


def gelmanrubin(res):
    &#39;&#39;&#39;Check convergence with GR diagnostic
    &#39;&#39;&#39;
    n = 0  # number of samples per chain - to be computed
    m = len(res)  # number of chains
    phi_bar_j = {}
    for chain in res:
        phi_bar_j[chain] = 0.
        for val in res[chain][&#39;permsdic&#39;].values():
            phi_bar_j[chain] += val[1] * val[0]  # numsamples*log posterior
            n += val[1]
    # And normalize
    n = n // m  # Number of samples per chain (assuming all m chains have same number of samples)
    # Normalize, and compute phi_bar
    phi_bar = 0.
    for chain in phi_bar_j:
        phi_bar_j[chain] = phi_bar_j[chain] / float(n)  # normalize
        phi_bar += phi_bar_j[chain]
    phi_bar = phi_bar / float(m)  # phi_bar = average of phi_bar_j
    # Now B
    B = 0.
    for chain in phi_bar_j:
        B += (phi_bar_j[chain] - phi_bar) ** 2
    B = B * (n / float(m - 1))
    # Now W.
    W = 0.
    for chain in res:
        s2_j = 0.
        for val in res[chain][&#39;permsdic&#39;].values():
            s2_j += val[1] * (val[0] - phi_bar_j[chain]) ** 2
        s2_j = (1. / float(n - 1)) * s2_j
        W += s2_j
    W = W * (1. / float(m))
    # Next varhat
    varhat = ((n - 1) / float(n)) * W + (1. / float(n)) * B
    # And finally,
    try:
        Rhat = sqrt(varhat / float(W))
    except:
        print(&#39;RuntimeWarning computing Rhat, W=&#39; + str(W) + &#39;, B=&#39; + str(B))
        Rhat = 0.
    return Rhat


def plot_chains(res):
    &#39;&#39;&#39;Plot the logposterior values for the samples in the chains.
    &#39;&#39;&#39;
    for chain in res:
        plt.plot([res[chain][&#39;permsdic&#39;][a][0] for a in res[chain][&#39;perms&#39;]])
    plt.show()
    return


def merge_chains(res):
    &#39;&#39;&#39;Merge chains into a single collection of posterior samples
    &#39;&#39;&#39;
    permsdic = defaultdict(default_permsdic)
    for n in res:
        for perm, vals in res[n][&#39;permsdic&#39;].items():
            permsdic[perm][0] = vals[0]
            permsdic[perm][1] += vals[1]
    return permsdic


def get_point_estimate(permsdic, lhs_len, X, Y, alpha, nruleslen, maxlhs, lbda, eta, verbose=True):
    &#39;&#39;&#39;Get a point estimate with length and width similar to the posterior average, with highest likelihood
    &#39;&#39;&#39;
    # Figure out the posterior expected list length and average rule size
    listlens = []
    rulesizes = []
    for perm in permsdic:
        #         with open(perm, &#39;rb&#39;) as file:
        #             d_t = pickle.loads(file)
        #         print(&#39;perm&#39;, perm, type(perm))
        #         print(&#39;perm list&#39;, list(perm))
        #         d_t = Pickle.loads(bytes(perm, encoding=&#34;latin1&#34;)) #, encoding=&#39;bytes&#39;)
        d_t = Pickle.loads(perm)  # , encoding=&#39;bytes&#39;)

        listlens.extend([len(d_t)] * int(permsdic[perm][1]))
        rulesizes.extend([lhs_len[j] for j in d_t[:-1]] * int(permsdic[perm][1]))

    # Now compute average
    avglistlen = average(listlens)
    if verbose:
        print(&#39;Posterior average length:&#39;, avglistlen)
    try:
        avgrulesize = average(rulesizes)
        if verbose:
            print(&#39;Posterior average width:&#39;, avgrulesize)
        # Prepare the intervals
        minlen = int(floor(avglistlen))
        maxlen = int(ceil(avglistlen))
        minrulesize = int(floor(avgrulesize))
        maxrulesize = int(ceil(avgrulesize))
        # Run through all perms again
        likelihds = []
        d_ts = []
        beta_Z, logalpha_pmf, logbeta_pmf = prior_calculations(lbda, len(X), eta,
                                                               maxlhs)  # get the constants needed to compute the prior
        for perm in permsdic:
            if permsdic[perm][1] &gt; 0:
                d_t = Pickle.loads(perm)  # this is the antecedent list

                # Check the list length
                if len(d_t) &gt;= minlen and len(d_t) &lt;= maxlen:

                    # Check the rule size
                    rulesize = average([lhs_len[j] for j in d_t[:-1]])
                    if rulesize &gt;= minrulesize and rulesize &lt;= maxrulesize:
                        d_ts.append(d_t)

                        # Compute the likelihood
                        R_t = d_t.index(0)
                        N_t = compute_rule_usage(d_t, R_t, X, Y)
                        likelihds.append(
                            fn_logposterior(d_t, R_t, N_t, alpha, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen,
                                            lhs_len))
        likelihds = array(likelihds)
        d_star = d_ts[likelihds.argmax()]
    except RuntimeWarning:
        # This can happen if all perms are identically [0], or if no soln is found within the len and width bounds (probably the chains didn&#39;t converge)
        print(&#39;No suitable point estimate found&#39;)
        d_star = None
    return d_star


#################COMPUTING RESULTS
def get_rule_rhs(Xtrain, Ytrain, d_t, alpha, intervals):
    &#39;&#39;&#39;Compute the posterior consequent distributions
    (Basically compute points in each part of rule)
    &#39;&#39;&#39;
    N_t = compute_rule_usage(d_t, d_t.index(0), Xtrain, Ytrain)
    theta = []  # P(Y=1)
    ci_theta = []  # confidence interval for Y=1
    for i, j in enumerate(d_t):
        # theta ~ Dirichlet(n_rules[j,:] + alpha)
        # E[theta] = (n_rules[j,:] + alpha)/float(sum(n_rules[j,:] + alpha))
        # NOTE this result is only for binary classification
        # theta = p(y=1)
        theta.append((N_t[i, 1] + alpha[1]) / float(sum(N_t[i, :] + alpha)))
        # And now the 95% interval, for Beta(n_rules[j,1] + alpha[1], n_rules[j,0] + alpha[0])
        if intervals:
            ci_theta.append(beta.interval(0.95, N_t[i, 1] + alpha[1], N_t[i, 0] + alpha[0]))
    return theta, ci_theta


def preds_d_t(X, Y, d_t, theta):
    &#39;&#39;&#39;Get predictions from the list d_t
    &#39;&#39;&#39;
    # this is binary only. The score is the Prob of 1.
    unused = set(range(Y.shape[0]))
    preds = -1 * ones(Y.shape[0])
    for i, j in enumerate(d_t):
        usedj = unused.intersection(X[j])  # these are the observations in X that make it to rule j
        preds[list(usedj)] = theta[i]
        unused = unused.difference(set(usedj))
    if preds.min() &lt; 0:
        raise Exception  # this means some observation wasn&#39;t given a prediction - shouldn&#39;t happen
    return preds


##############MCMC core 
def bayesdl_mcmc(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin, rseed,
                 d_init):
    &#39;&#39;&#39;Run Metropolis-Hastings algorithm
    &#39;&#39;&#39;
    # initialize
    perms = []
    if rseed:
        random.seed(rseed)

    # Do some pre-computation for the prior
    beta_Z, logalpha_pmf, logbeta_pmf = prior_calculations(lbda, len(X), eta, maxlhs)
    if d_init:  # If we want to begin our chain at a specific place (e.g. to continue a chain)
        d_t = Pickle.loads(d_init)
        d_t.extend([i for i in range(len(X)) if i not in d_t])
        R_t = d_t.index(0)
        N_t = compute_rule_usage(d_t, R_t, X, Y)
    else:
        d_t, R_t, N_t = initialize_d(X, Y, lbda, eta, lhs_len, maxlhs,
                                     nruleslen)  # Otherwise sample the initial value from the prior

    # Add to dictionary which will store the sampling results
    a_t = Pickle.dumps(d_t[:R_t + 1])  # The antecedent list in string form
    if a_t not in permsdic:
        permsdic[a_t][0] = fn_logposterior(d_t, R_t, N_t, alpha, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen,
                                           lhs_len)  # Compute its logposterior
    if burnin == 0:
        permsdic[a_t][1] += 1  # store the initialization sample

    # iterate!
    for itr in range(numiters):
        # Sample from proposal distribution
        d_star, Jratio, R_star, step = proposal(d_t, R_t, X, Y, alpha)
        # Compute the new posterior value, if necessary
        a_star = Pickle.dumps(d_star[:R_star + 1])
        if a_star not in permsdic:
            N_star = compute_rule_usage(d_star, R_star, X, Y)
            permsdic[a_star][0] = fn_logposterior(d_star, R_star, N_star, alpha, logalpha_pmf, logbeta_pmf, maxlhs,
                                                  beta_Z, nruleslen, lhs_len)
        # Compute the metropolis acceptance probability
        q = exp(permsdic[a_star][0] - permsdic[a_t][0] + Jratio)
        u = random.random()
        if u &lt; q:
            # then we accept the move
            d_t = list(d_star)
            R_t = int(R_star)
            a_t = a_star
            # else: pass
        if itr &gt; burnin and itr % thinning == 0:
            ##store
            permsdic[a_t][1] += 1
            perms.append(a_t)
    return permsdic, perms


def initialize_d(X, Y, lbda, eta, lhs_len, maxlhs, nruleslen):
    &#39;&#39;&#39;Samples a list from the prior
    &#39;&#39;&#39;
    m = Inf
    while m &gt;= len(X):
        m = poisson.rvs(lbda)  # sample the length of the list from Poisson(lbda), truncated at len(X)
    # prepare the list
    d_t = []
    empty_rulelens = [r for r in range(1, maxlhs + 1) if r not in nruleslen]
    used_rules = []
    for i in range(m):
        # Sample a rule size.
        r = 0
        while r == 0 or r &gt; maxlhs or r in empty_rulelens:
            r = poisson.rvs(
                eta)  # Sample the rule size from Poisson(eta), truncated at 0 and maxlhs and not using empty rule lens
        # Now sample a rule of that size uniformly at random
        rule_cands = [j for j, lhslen in enumerate(lhs_len) if lhslen == r and j not in used_rules]
        random.shuffle(rule_cands)
        j = rule_cands[0]
        # And add it in
        d_t.append(j)
        used_rules.append(j)
        assert lhs_len[j] == r
        if len(rule_cands) == 1:
            empty_rulelens.append(r)
    # Done adding rules. We have added m rules. Finish up.
    d_t.append(0)  # all done
    d_t.extend([i for i in range(len(X)) if i not in d_t])
    R_t = d_t.index(0)
    assert R_t == m
    # Figure out what rules are used to classify what points
    N_t = compute_rule_usage(d_t, R_t, X, Y)
    return d_t, R_t, N_t


def proposal(d_t, R_t, X, Y, alpha):
    &#39;&#39;&#39;Propose a new d_star
    &#39;&#39;&#39;
    d_star = list(d_t)
    R_star = int(R_t)
    # We begin with these as the move probabilities, but will renormalize as needed if certain moves are unavailable.
    move_probs_default = array([0.3333333333, 0.3333333333, 0.3333333333])
    # We have 3 moves: move, add, cut. Define the pdf for the probabilities of the moves, in that order:
    if R_t == 0:
        # List is empty. We must add.
        move_probs = array([0., 1., 0.])
        # This is an add transition. The probability of the reverse cut move is the prob of a list of len 1 having
        # a cut (other option for list of len 1 is an add).
        Jratios = array([0., move_probs_default[2] / float(move_probs_default[1] + move_probs_default[2]), 0.])
    elif R_t == 1:
        # List has one rule on it. We cannot move, must add or cut.
        move_probs = array(move_probs_default)  # copy
        move_probs[0] = 0.  # drop move move.
        move_probs = move_probs / sum(move_probs)  # renormalize
        # If add, probability of the reverse cut is the default cut probability
        # If cut, probability of the reverse add is 1.
        inv_move_probs = array([0., move_probs_default[2], 1.])
        Jratios = zeros_like(move_probs)
        Jratios[1:] = inv_move_probs[1:] / move_probs[1:]  # array elementwise division
    elif R_t == len(d_t) - 1:
        # List has all rules on it. We cannot add, must move or cut.
        move_probs = array(move_probs_default)  # copy
        move_probs[1] = 0.  # drop add move.
        move_probs = move_probs / sum(move_probs)  # renormalize
        # If move, probability of reverse move is move_probs[0], so Jratio = 1.
        # if cut, probability of reverse add is move_probs_default
        Jratios = array([1., 0., move_probs_default[1] / move_probs[2]])
    elif R_t == len(d_t) - 2:
        # List has all rules but 1 on it.
        # Move probabilities are the default, but the inverse are a little different.
        move_probs = array(move_probs_default)
        # If move, probability of reverse move is still default, so Jratio = 1.
        # if cut, probability of reverse add is move_probs_default[1],
        # if add, probability of reverse cut is,
        Jratios = array([1., move_probs_default[2] / float(move_probs_default[0] + move_probs_default[2]) / float(
            move_probs_default[1]), move_probs_default[1] / float(move_probs_default[2])])
    else:
        move_probs = array(move_probs_default)
        Jratios = array([1., move_probs[2] / float(move_probs[1]), move_probs[1] / float(move_probs[2])])
    u = random.random()
    # First we will find the indicies for the insertion-deletion. indx1 is the item to be moved, indx2 is the new location
    if u &lt; sum(move_probs[:1]):
        # This is an on-list move.
        step = &#39;move&#39;
        [indx1, indx2] = random.permutation(range(len(d_t[:R_t])))[:2]  # value error if there are no on list entries
        # print &#39;move&#39;,indx1,indx2
        Jratio = Jratios[0]  # ratio of move/move probabilities is 1.
    elif u &lt; sum(move_probs[:2]):
        # this is an add
        step = &#39;add&#39;
        indx1 = R_t + 1 + random.randint(0, len(
            d_t[R_t + 1:]))  # this will throw ValueError if there are no off list entries
        indx2 = random.randint(0, len(d_t[:R_t + 1]))  # this one will always work
        # print &#39;add&#39;,indx1,indx2
        # the probability of going from d_star back to d_t is the probability of the corresponding cut.
        # p(d*-&gt;d|cut) = 1/|d*| = 1/(|d|+1) = 1./float(R_t+1)
        # p(d-&gt;d*|add) = 1/((|a|-|d|)(|d|+1)) = 1./(float(len(d_t)-1-R_t)*float(R_t+1))
        Jratio = Jratios[1] * float(len(d_t) - 1 - R_t)
        R_star += 1
    elif u &lt; sum(move_probs[:3]):
        # this is a cut
        step = &#39;cut&#39;
        indx1 = random.randint(0, len(d_t[:R_t]))  # this will throw ValueError if there are no on list entries
        indx2 = R_t + random.randint(0, len(d_t[R_t:]))  # this one will always work
        # print &#39;cut&#39;,indx1,indx2
        # the probability of going from d_star back to d_t is the probability of the corresponding add.
        # p(d*-&gt;d|add) = 1/((|a|-|d*|)(|d*|+1)) = 1/((|a|-|d|+1)(|d|))
        # p(d-&gt;d*|cut) = 1/|d|
        # Jratio =
        Jratio = Jratios[2] * (1. / float(len(d_t) - 1 - R_t + 1))
        R_star -= 1
    else:
        raise Exception
    # Now do the insertion-deletion
    d_star.insert(indx2, d_star.pop(indx1))
    return d_star, log(Jratio), R_star, step


def prior_calculations(lbda, maxlen, eta, maxlhs):
    &#39;&#39;&#39;Compute the normalization constants for the prior on rule cardinality
    &#39;&#39;&#39;
    # First normalization constants for beta
    beta_Z = poisson.cdf(maxlhs, eta) - poisson.pmf(0, eta)
    # Then the actual un-normalized pmfs
    logalpha_pmf = {}
    for i in range(maxlen + 1):
        try:
            logalpha_pmf[i] = poisson.logpmf(i, lbda)
        except RuntimeWarning:
            logalpha_pmf[i] = -inf
    logbeta_pmf = {}
    for i in range(1, maxlhs + 1):
        logbeta_pmf[i] = poisson.logpmf(i, eta)
    return beta_Z, logalpha_pmf, logbeta_pmf


def fn_logposterior(d_t, R_t, N_t, alpha, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len):
    &#39;&#39;&#39;# Compute log posterior
    &#39;&#39;&#39;
    logliklihood = fn_logliklihood(d_t, N_t, R_t, alpha)
    logprior = fn_logprior(d_t, R_t, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len)
    return logliklihood + logprior


def fn_logliklihood(d_t, N_t, R_t, alpha):
    &#39;&#39;&#39;Compute log likelihood
    &#39;&#39;&#39;
    gammaln_Nt_jk = gammaln(N_t + alpha)
    gammaln_Nt_j = gammaln(sum(N_t + alpha, 1))
    logliklihood = sum(gammaln_Nt_jk) - sum(gammaln_Nt_j)
    return logliklihood


def fn_logprior(d_t, R_t, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len):
    &#39;&#39;&#39;# Compute log prior
    The prior will be _proportional_ to this -&gt; we drop the normalization for alpha
    beta_Z is the normalization for beta, except the terms that need to be dropped due to running out of rules.
    log p(d_star) = log \alpha(m|lbda) + sum_{i=1...m} log beta(l_i | eta) + log gamma(r_i | l_i)
    The length of the list (m) is R_t
    Get logalpha (length of list) (overloaded notation in this code, unrelated to the prior hyperparameter alpha)
    &#39;&#39;&#39;
    logprior = 0.
    logalpha = logalpha_pmf[
        R_t]  # this is proportional to logalpha - we have dropped the normalization for truncating based on total number of rules
    logprior += logalpha
    empty_rulelens = []
    nlens = zeros(maxlhs + 1)
    for i in range(R_t):
        l_i = lhs_len[d_t[i]]
        logbeta = logbeta_pmf[l_i] - log(
            beta_Z - sum([logbeta_pmf[l_j] for l_j in empty_rulelens]))  # The correction for exhausted rule lengths
        # Finally loggamma
        loggamma = -log(nruleslen[l_i] - nlens[l_i])
        # And now check if we have exhausted all rules of a certain size
        nlens[l_i] += 1
        if nlens[l_i] == nruleslen[l_i]:
            empty_rulelens.append(l_i)
        elif nlens[l_i] &gt; nruleslen[l_i]:
            raise Exception
        # Add &#39;em in
        logprior += logbeta
        logprior += loggamma
    # All done
    return logprior


def compute_rule_usage(d_star, R_star, X, Y):
    &#39;&#39;&#39;Compute which rules are being used to classify data points with what labels
    &#39;&#39;&#39;
    N_star = zeros((R_star + 1, Y.shape[1]))
    remaining_unused = set(range(Y.shape[0]))
    i = 0
    while remaining_unused:
        j = d_star[i]
        usedj = remaining_unused.intersection(X[j])
        remaining_unused = remaining_unused.difference(set(usedj))
        N_star[i, :] = Y[list(usedj), :].sum(0)
        i += 1
    if int(sum(N_star)) != Y.shape[0]:
        raise Exception  # bug check
    return N_star</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.bayesdl_mcmc"><code class="name flex">
<span>def <span class="ident">bayesdl_mcmc</span></span>(<span>numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin, rseed, d_init)</span>
</code></dt>
<dd>
<div class="desc"><p>Run Metropolis-Hastings algorithm</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bayesdl_mcmc(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin, rseed,
                 d_init):
    &#39;&#39;&#39;Run Metropolis-Hastings algorithm
    &#39;&#39;&#39;
    # initialize
    perms = []
    if rseed:
        random.seed(rseed)

    # Do some pre-computation for the prior
    beta_Z, logalpha_pmf, logbeta_pmf = prior_calculations(lbda, len(X), eta, maxlhs)
    if d_init:  # If we want to begin our chain at a specific place (e.g. to continue a chain)
        d_t = Pickle.loads(d_init)
        d_t.extend([i for i in range(len(X)) if i not in d_t])
        R_t = d_t.index(0)
        N_t = compute_rule_usage(d_t, R_t, X, Y)
    else:
        d_t, R_t, N_t = initialize_d(X, Y, lbda, eta, lhs_len, maxlhs,
                                     nruleslen)  # Otherwise sample the initial value from the prior

    # Add to dictionary which will store the sampling results
    a_t = Pickle.dumps(d_t[:R_t + 1])  # The antecedent list in string form
    if a_t not in permsdic:
        permsdic[a_t][0] = fn_logposterior(d_t, R_t, N_t, alpha, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen,
                                           lhs_len)  # Compute its logposterior
    if burnin == 0:
        permsdic[a_t][1] += 1  # store the initialization sample

    # iterate!
    for itr in range(numiters):
        # Sample from proposal distribution
        d_star, Jratio, R_star, step = proposal(d_t, R_t, X, Y, alpha)
        # Compute the new posterior value, if necessary
        a_star = Pickle.dumps(d_star[:R_star + 1])
        if a_star not in permsdic:
            N_star = compute_rule_usage(d_star, R_star, X, Y)
            permsdic[a_star][0] = fn_logposterior(d_star, R_star, N_star, alpha, logalpha_pmf, logbeta_pmf, maxlhs,
                                                  beta_Z, nruleslen, lhs_len)
        # Compute the metropolis acceptance probability
        q = exp(permsdic[a_star][0] - permsdic[a_t][0] + Jratio)
        u = random.random()
        if u &lt; q:
            # then we accept the move
            d_t = list(d_star)
            R_t = int(R_star)
            a_t = a_star
            # else: pass
        if itr &gt; burnin and itr % thinning == 0:
            ##store
            permsdic[a_t][1] += 1
            perms.append(a_t)
    return permsdic, perms</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.compute_rule_usage"><code class="name flex">
<span>def <span class="ident">compute_rule_usage</span></span>(<span>d_star, R_star, X, Y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute which rules are being used to classify data points with what labels</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_rule_usage(d_star, R_star, X, Y):
    &#39;&#39;&#39;Compute which rules are being used to classify data points with what labels
    &#39;&#39;&#39;
    N_star = zeros((R_star + 1, Y.shape[1]))
    remaining_unused = set(range(Y.shape[0]))
    i = 0
    while remaining_unused:
        j = d_star[i]
        usedj = remaining_unused.intersection(X[j])
        remaining_unused = remaining_unused.difference(set(usedj))
        N_star[i, :] = Y[list(usedj), :].sum(0)
        i += 1
    if int(sum(N_star)) != Y.shape[0]:
        raise Exception  # bug check
    return N_star</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.default_permsdic"><code class="name flex">
<span>def <span class="ident">default_permsdic</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>For producing the defaultdict used for storing MCMC results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def default_permsdic():
    &#39;&#39;&#39;For producing the defaultdict used for storing MCMC results
    &#39;&#39;&#39;
    return [0., 0.]</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.fn_logliklihood"><code class="name flex">
<span>def <span class="ident">fn_logliklihood</span></span>(<span>d_t, N_t, R_t, alpha)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute log likelihood</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fn_logliklihood(d_t, N_t, R_t, alpha):
    &#39;&#39;&#39;Compute log likelihood
    &#39;&#39;&#39;
    gammaln_Nt_jk = gammaln(N_t + alpha)
    gammaln_Nt_j = gammaln(sum(N_t + alpha, 1))
    logliklihood = sum(gammaln_Nt_jk) - sum(gammaln_Nt_j)
    return logliklihood</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.fn_logposterior"><code class="name flex">
<span>def <span class="ident">fn_logposterior</span></span>(<span>d_t, R_t, N_t, alpha, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len)</span>
</code></dt>
<dd>
<div class="desc"><h1 id="compute-log-posterior">Compute log posterior</h1></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fn_logposterior(d_t, R_t, N_t, alpha, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len):
    &#39;&#39;&#39;# Compute log posterior
    &#39;&#39;&#39;
    logliklihood = fn_logliklihood(d_t, N_t, R_t, alpha)
    logprior = fn_logprior(d_t, R_t, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len)
    return logliklihood + logprior</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.fn_logprior"><code class="name flex">
<span>def <span class="ident">fn_logprior</span></span>(<span>d_t, R_t, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len)</span>
</code></dt>
<dd>
<div class="desc"><h1 id="compute-log-prior">Compute log prior</h1>
<p>The prior will be <em>proportional</em> to this -&gt; we drop the normalization for alpha
beta_Z is the normalization for beta, except the terms that need to be dropped due to running out of rules.
log p(d_star) = log lpha(m|lbda) + sum_{i=1&hellip;m} log beta(l_i | eta) + log gamma(r_i | l_i)
The length of the list (m) is R_t
Get logalpha (length of list) (overloaded notation in this code, unrelated to the prior hyperparameter alpha)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fn_logprior(d_t, R_t, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen, lhs_len):
    &#39;&#39;&#39;# Compute log prior
    The prior will be _proportional_ to this -&gt; we drop the normalization for alpha
    beta_Z is the normalization for beta, except the terms that need to be dropped due to running out of rules.
    log p(d_star) = log \alpha(m|lbda) + sum_{i=1...m} log beta(l_i | eta) + log gamma(r_i | l_i)
    The length of the list (m) is R_t
    Get logalpha (length of list) (overloaded notation in this code, unrelated to the prior hyperparameter alpha)
    &#39;&#39;&#39;
    logprior = 0.
    logalpha = logalpha_pmf[
        R_t]  # this is proportional to logalpha - we have dropped the normalization for truncating based on total number of rules
    logprior += logalpha
    empty_rulelens = []
    nlens = zeros(maxlhs + 1)
    for i in range(R_t):
        l_i = lhs_len[d_t[i]]
        logbeta = logbeta_pmf[l_i] - log(
            beta_Z - sum([logbeta_pmf[l_j] for l_j in empty_rulelens]))  # The correction for exhausted rule lengths
        # Finally loggamma
        loggamma = -log(nruleslen[l_i] - nlens[l_i])
        # And now check if we have exhausted all rules of a certain size
        nlens[l_i] += 1
        if nlens[l_i] == nruleslen[l_i]:
            empty_rulelens.append(l_i)
        elif nlens[l_i] &gt; nruleslen[l_i]:
            raise Exception
        # Add &#39;em in
        logprior += logbeta
        logprior += loggamma
    # All done
    return logprior</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.gelmanrubin"><code class="name flex">
<span>def <span class="ident">gelmanrubin</span></span>(<span>res)</span>
</code></dt>
<dd>
<div class="desc"><p>Check convergence with GR diagnostic</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gelmanrubin(res):
    &#39;&#39;&#39;Check convergence with GR diagnostic
    &#39;&#39;&#39;
    n = 0  # number of samples per chain - to be computed
    m = len(res)  # number of chains
    phi_bar_j = {}
    for chain in res:
        phi_bar_j[chain] = 0.
        for val in res[chain][&#39;permsdic&#39;].values():
            phi_bar_j[chain] += val[1] * val[0]  # numsamples*log posterior
            n += val[1]
    # And normalize
    n = n // m  # Number of samples per chain (assuming all m chains have same number of samples)
    # Normalize, and compute phi_bar
    phi_bar = 0.
    for chain in phi_bar_j:
        phi_bar_j[chain] = phi_bar_j[chain] / float(n)  # normalize
        phi_bar += phi_bar_j[chain]
    phi_bar = phi_bar / float(m)  # phi_bar = average of phi_bar_j
    # Now B
    B = 0.
    for chain in phi_bar_j:
        B += (phi_bar_j[chain] - phi_bar) ** 2
    B = B * (n / float(m - 1))
    # Now W.
    W = 0.
    for chain in res:
        s2_j = 0.
        for val in res[chain][&#39;permsdic&#39;].values():
            s2_j += val[1] * (val[0] - phi_bar_j[chain]) ** 2
        s2_j = (1. / float(n - 1)) * s2_j
        W += s2_j
    W = W * (1. / float(m))
    # Next varhat
    varhat = ((n - 1) / float(n)) * W + (1. / float(n)) * B
    # And finally,
    try:
        Rhat = sqrt(varhat / float(W))
    except:
        print(&#39;RuntimeWarning computing Rhat, W=&#39; + str(W) + &#39;, B=&#39; + str(B))
        Rhat = 0.
    return Rhat</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.get_point_estimate"><code class="name flex">
<span>def <span class="ident">get_point_estimate</span></span>(<span>permsdic, lhs_len, X, Y, alpha, nruleslen, maxlhs, lbda, eta, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a point estimate with length and width similar to the posterior average, with highest likelihood</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_point_estimate(permsdic, lhs_len, X, Y, alpha, nruleslen, maxlhs, lbda, eta, verbose=True):
    &#39;&#39;&#39;Get a point estimate with length and width similar to the posterior average, with highest likelihood
    &#39;&#39;&#39;
    # Figure out the posterior expected list length and average rule size
    listlens = []
    rulesizes = []
    for perm in permsdic:
        #         with open(perm, &#39;rb&#39;) as file:
        #             d_t = pickle.loads(file)
        #         print(&#39;perm&#39;, perm, type(perm))
        #         print(&#39;perm list&#39;, list(perm))
        #         d_t = Pickle.loads(bytes(perm, encoding=&#34;latin1&#34;)) #, encoding=&#39;bytes&#39;)
        d_t = Pickle.loads(perm)  # , encoding=&#39;bytes&#39;)

        listlens.extend([len(d_t)] * int(permsdic[perm][1]))
        rulesizes.extend([lhs_len[j] for j in d_t[:-1]] * int(permsdic[perm][1]))

    # Now compute average
    avglistlen = average(listlens)
    if verbose:
        print(&#39;Posterior average length:&#39;, avglistlen)
    try:
        avgrulesize = average(rulesizes)
        if verbose:
            print(&#39;Posterior average width:&#39;, avgrulesize)
        # Prepare the intervals
        minlen = int(floor(avglistlen))
        maxlen = int(ceil(avglistlen))
        minrulesize = int(floor(avgrulesize))
        maxrulesize = int(ceil(avgrulesize))
        # Run through all perms again
        likelihds = []
        d_ts = []
        beta_Z, logalpha_pmf, logbeta_pmf = prior_calculations(lbda, len(X), eta,
                                                               maxlhs)  # get the constants needed to compute the prior
        for perm in permsdic:
            if permsdic[perm][1] &gt; 0:
                d_t = Pickle.loads(perm)  # this is the antecedent list

                # Check the list length
                if len(d_t) &gt;= minlen and len(d_t) &lt;= maxlen:

                    # Check the rule size
                    rulesize = average([lhs_len[j] for j in d_t[:-1]])
                    if rulesize &gt;= minrulesize and rulesize &lt;= maxrulesize:
                        d_ts.append(d_t)

                        # Compute the likelihood
                        R_t = d_t.index(0)
                        N_t = compute_rule_usage(d_t, R_t, X, Y)
                        likelihds.append(
                            fn_logposterior(d_t, R_t, N_t, alpha, logalpha_pmf, logbeta_pmf, maxlhs, beta_Z, nruleslen,
                                            lhs_len))
        likelihds = array(likelihds)
        d_star = d_ts[likelihds.argmax()]
    except RuntimeWarning:
        # This can happen if all perms are identically [0], or if no soln is found within the len and width bounds (probably the chains didn&#39;t converge)
        print(&#39;No suitable point estimate found&#39;)
        d_star = None
    return d_star</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.get_rule_rhs"><code class="name flex">
<span>def <span class="ident">get_rule_rhs</span></span>(<span>Xtrain, Ytrain, d_t, alpha, intervals)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the posterior consequent distributions
(Basically compute points in each part of rule)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rule_rhs(Xtrain, Ytrain, d_t, alpha, intervals):
    &#39;&#39;&#39;Compute the posterior consequent distributions
    (Basically compute points in each part of rule)
    &#39;&#39;&#39;
    N_t = compute_rule_usage(d_t, d_t.index(0), Xtrain, Ytrain)
    theta = []  # P(Y=1)
    ci_theta = []  # confidence interval for Y=1
    for i, j in enumerate(d_t):
        # theta ~ Dirichlet(n_rules[j,:] + alpha)
        # E[theta] = (n_rules[j,:] + alpha)/float(sum(n_rules[j,:] + alpha))
        # NOTE this result is only for binary classification
        # theta = p(y=1)
        theta.append((N_t[i, 1] + alpha[1]) / float(sum(N_t[i, :] + alpha)))
        # And now the 95% interval, for Beta(n_rules[j,1] + alpha[1], n_rules[j,0] + alpha[0])
        if intervals:
            ci_theta.append(beta.interval(0.95, N_t[i, 1] + alpha[1], N_t[i, 0] + alpha[0]))
    return theta, ci_theta</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.initialize_d"><code class="name flex">
<span>def <span class="ident">initialize_d</span></span>(<span>X, Y, lbda, eta, lhs_len, maxlhs, nruleslen)</span>
</code></dt>
<dd>
<div class="desc"><p>Samples a list from the prior</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_d(X, Y, lbda, eta, lhs_len, maxlhs, nruleslen):
    &#39;&#39;&#39;Samples a list from the prior
    &#39;&#39;&#39;
    m = Inf
    while m &gt;= len(X):
        m = poisson.rvs(lbda)  # sample the length of the list from Poisson(lbda), truncated at len(X)
    # prepare the list
    d_t = []
    empty_rulelens = [r for r in range(1, maxlhs + 1) if r not in nruleslen]
    used_rules = []
    for i in range(m):
        # Sample a rule size.
        r = 0
        while r == 0 or r &gt; maxlhs or r in empty_rulelens:
            r = poisson.rvs(
                eta)  # Sample the rule size from Poisson(eta), truncated at 0 and maxlhs and not using empty rule lens
        # Now sample a rule of that size uniformly at random
        rule_cands = [j for j, lhslen in enumerate(lhs_len) if lhslen == r and j not in used_rules]
        random.shuffle(rule_cands)
        j = rule_cands[0]
        # And add it in
        d_t.append(j)
        used_rules.append(j)
        assert lhs_len[j] == r
        if len(rule_cands) == 1:
            empty_rulelens.append(r)
    # Done adding rules. We have added m rules. Finish up.
    d_t.append(0)  # all done
    d_t.extend([i for i in range(len(X)) if i not in d_t])
    R_t = d_t.index(0)
    assert R_t == m
    # Figure out what rules are used to classify what points
    N_t = compute_rule_usage(d_t, R_t, X, Y)
    return d_t, R_t, N_t</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.mcmcchain"><code class="name flex">
<span>def <span class="ident">mcmcchain</span></span>(<span>numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin, nchains, d_init)</span>
</code></dt>
<dd>
<div class="desc"><p>Run and store mcmc chain</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mcmcchain(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin, nchains,
              d_init):
    &#39;&#39;&#39;Run and store mcmc chain
    &#39;&#39;&#39;
    res = {}
    permsdic, res[&#39;perms&#39;] = bayesdl_mcmc(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs,
                                          permsdic, burnin, None, d_init)
    # Store the permsdic results
    res[&#39;permsdic&#39;] = {perm: list(vals) for perm, vals in permsdic.items() if vals[1] &gt; 0}
    # Reset the permsdic
    permsdic = reset_permsdic(permsdic)
    return res</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.merge_chains"><code class="name flex">
<span>def <span class="ident">merge_chains</span></span>(<span>res)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge chains into a single collection of posterior samples</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_chains(res):
    &#39;&#39;&#39;Merge chains into a single collection of posterior samples
    &#39;&#39;&#39;
    permsdic = defaultdict(default_permsdic)
    for n in res:
        for perm, vals in res[n][&#39;permsdic&#39;].items():
            permsdic[perm][0] = vals[0]
            permsdic[perm][1] += vals[1]
    return permsdic</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.plot_chains"><code class="name flex">
<span>def <span class="ident">plot_chains</span></span>(<span>res)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the logposterior values for the samples in the chains.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_chains(res):
    &#39;&#39;&#39;Plot the logposterior values for the samples in the chains.
    &#39;&#39;&#39;
    for chain in res:
        plt.plot([res[chain][&#39;permsdic&#39;][a][0] for a in res[chain][&#39;perms&#39;]])
    plt.show()
    return</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.preds_d_t"><code class="name flex">
<span>def <span class="ident">preds_d_t</span></span>(<span>X, Y, d_t, theta)</span>
</code></dt>
<dd>
<div class="desc"><p>Get predictions from the list d_t</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preds_d_t(X, Y, d_t, theta):
    &#39;&#39;&#39;Get predictions from the list d_t
    &#39;&#39;&#39;
    # this is binary only. The score is the Prob of 1.
    unused = set(range(Y.shape[0]))
    preds = -1 * ones(Y.shape[0])
    for i, j in enumerate(d_t):
        usedj = unused.intersection(X[j])  # these are the observations in X that make it to rule j
        preds[list(usedj)] = theta[i]
        unused = unused.difference(set(usedj))
    if preds.min() &lt; 0:
        raise Exception  # this means some observation wasn&#39;t given a prediction - shouldn&#39;t happen
    return preds</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.prior_calculations"><code class="name flex">
<span>def <span class="ident">prior_calculations</span></span>(<span>lbda, maxlen, eta, maxlhs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the normalization constants for the prior on rule cardinality</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prior_calculations(lbda, maxlen, eta, maxlhs):
    &#39;&#39;&#39;Compute the normalization constants for the prior on rule cardinality
    &#39;&#39;&#39;
    # First normalization constants for beta
    beta_Z = poisson.cdf(maxlhs, eta) - poisson.pmf(0, eta)
    # Then the actual un-normalized pmfs
    logalpha_pmf = {}
    for i in range(maxlen + 1):
        try:
            logalpha_pmf[i] = poisson.logpmf(i, lbda)
        except RuntimeWarning:
            logalpha_pmf[i] = -inf
    logbeta_pmf = {}
    for i in range(1, maxlhs + 1):
        logbeta_pmf[i] = poisson.logpmf(i, eta)
    return beta_Z, logalpha_pmf, logbeta_pmf</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.proposal"><code class="name flex">
<span>def <span class="ident">proposal</span></span>(<span>d_t, R_t, X, Y, alpha)</span>
</code></dt>
<dd>
<div class="desc"><p>Propose a new d_star</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def proposal(d_t, R_t, X, Y, alpha):
    &#39;&#39;&#39;Propose a new d_star
    &#39;&#39;&#39;
    d_star = list(d_t)
    R_star = int(R_t)
    # We begin with these as the move probabilities, but will renormalize as needed if certain moves are unavailable.
    move_probs_default = array([0.3333333333, 0.3333333333, 0.3333333333])
    # We have 3 moves: move, add, cut. Define the pdf for the probabilities of the moves, in that order:
    if R_t == 0:
        # List is empty. We must add.
        move_probs = array([0., 1., 0.])
        # This is an add transition. The probability of the reverse cut move is the prob of a list of len 1 having
        # a cut (other option for list of len 1 is an add).
        Jratios = array([0., move_probs_default[2] / float(move_probs_default[1] + move_probs_default[2]), 0.])
    elif R_t == 1:
        # List has one rule on it. We cannot move, must add or cut.
        move_probs = array(move_probs_default)  # copy
        move_probs[0] = 0.  # drop move move.
        move_probs = move_probs / sum(move_probs)  # renormalize
        # If add, probability of the reverse cut is the default cut probability
        # If cut, probability of the reverse add is 1.
        inv_move_probs = array([0., move_probs_default[2], 1.])
        Jratios = zeros_like(move_probs)
        Jratios[1:] = inv_move_probs[1:] / move_probs[1:]  # array elementwise division
    elif R_t == len(d_t) - 1:
        # List has all rules on it. We cannot add, must move or cut.
        move_probs = array(move_probs_default)  # copy
        move_probs[1] = 0.  # drop add move.
        move_probs = move_probs / sum(move_probs)  # renormalize
        # If move, probability of reverse move is move_probs[0], so Jratio = 1.
        # if cut, probability of reverse add is move_probs_default
        Jratios = array([1., 0., move_probs_default[1] / move_probs[2]])
    elif R_t == len(d_t) - 2:
        # List has all rules but 1 on it.
        # Move probabilities are the default, but the inverse are a little different.
        move_probs = array(move_probs_default)
        # If move, probability of reverse move is still default, so Jratio = 1.
        # if cut, probability of reverse add is move_probs_default[1],
        # if add, probability of reverse cut is,
        Jratios = array([1., move_probs_default[2] / float(move_probs_default[0] + move_probs_default[2]) / float(
            move_probs_default[1]), move_probs_default[1] / float(move_probs_default[2])])
    else:
        move_probs = array(move_probs_default)
        Jratios = array([1., move_probs[2] / float(move_probs[1]), move_probs[1] / float(move_probs[2])])
    u = random.random()
    # First we will find the indicies for the insertion-deletion. indx1 is the item to be moved, indx2 is the new location
    if u &lt; sum(move_probs[:1]):
        # This is an on-list move.
        step = &#39;move&#39;
        [indx1, indx2] = random.permutation(range(len(d_t[:R_t])))[:2]  # value error if there are no on list entries
        # print &#39;move&#39;,indx1,indx2
        Jratio = Jratios[0]  # ratio of move/move probabilities is 1.
    elif u &lt; sum(move_probs[:2]):
        # this is an add
        step = &#39;add&#39;
        indx1 = R_t + 1 + random.randint(0, len(
            d_t[R_t + 1:]))  # this will throw ValueError if there are no off list entries
        indx2 = random.randint(0, len(d_t[:R_t + 1]))  # this one will always work
        # print &#39;add&#39;,indx1,indx2
        # the probability of going from d_star back to d_t is the probability of the corresponding cut.
        # p(d*-&gt;d|cut) = 1/|d*| = 1/(|d|+1) = 1./float(R_t+1)
        # p(d-&gt;d*|add) = 1/((|a|-|d|)(|d|+1)) = 1./(float(len(d_t)-1-R_t)*float(R_t+1))
        Jratio = Jratios[1] * float(len(d_t) - 1 - R_t)
        R_star += 1
    elif u &lt; sum(move_probs[:3]):
        # this is a cut
        step = &#39;cut&#39;
        indx1 = random.randint(0, len(d_t[:R_t]))  # this will throw ValueError if there are no on list entries
        indx2 = R_t + random.randint(0, len(d_t[R_t:]))  # this one will always work
        # print &#39;cut&#39;,indx1,indx2
        # the probability of going from d_star back to d_t is the probability of the corresponding add.
        # p(d*-&gt;d|add) = 1/((|a|-|d*|)(|d*|+1)) = 1/((|a|-|d|+1)(|d|))
        # p(d-&gt;d*|cut) = 1/|d|
        # Jratio =
        Jratio = Jratios[2] * (1. / float(len(d_t) - 1 - R_t + 1))
        R_star -= 1
    else:
        raise Exception
    # Now do the insertion-deletion
    d_star.insert(indx2, d_star.pop(indx1))
    return d_star, log(Jratio), R_star, step</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.reset_permsdic"><code class="name flex">
<span>def <span class="ident">reset_permsdic</span></span>(<span>permsdic)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the number of MCMC samples stored (value[1]) while maintaining the
log-posterior value (so it doesn't need to be re-computed in future chains).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_permsdic(permsdic):
    &#39;&#39;&#39;Resets the number of MCMC samples stored (value[1]) while maintaining the
    log-posterior value (so it doesn&#39;t need to be re-computed in future chains).
    &#39;&#39;&#39;
    for perm in permsdic:
        permsdic[perm][1] = 0.
    return permsdic</code></pre>
</details>
</dd>
<dt id="imodels.rule_list.bayesian_rule_list.brl_util.run_bdl_multichain_serial"><code class="name flex">
<span>def <span class="ident">run_bdl_multichain_serial</span></span>(<span>numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin, nchains, d_inits, verbose=True, seed=42)</span>
</code></dt>
<dd>
<div class="desc"><p>Run mcmc for each of the chains in serial</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_bdl_multichain_serial(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin,
                              nchains, d_inits, verbose=True, seed=42):
    &#39;&#39;&#39;Run mcmc for each of the chains in serial
    &#39;&#39;&#39;
    # random seed
    random.seed(seed)

    # Run each chain
    t1 = time.process_time()
    if verbose:
        print(&#39;Starting mcmc chains&#39;)
    res = {}
    for n in range(nchains):
        res[n] = mcmcchain(numiters, thinning, alpha, lbda, eta, X, Y, nruleslen, lhs_len, maxlhs, permsdic, burnin,
                           nchains, d_inits[n])

    if verbose:
        print(&#39;Elapsed CPU time&#39;, time.process_time() - t1)

    # Check convergence
    Rhat = gelmanrubin(res)

    if verbose:
        print(&#39;Rhat for convergence:&#39;, Rhat)
    ##plot?
    # plot_chains(res)
    return res, Rhat</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodels.rule_list.bayesian_rule_list" href="index.html">imodels.rule_list.bayesian_rule_list</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.bayesdl_mcmc" href="#imodels.rule_list.bayesian_rule_list.brl_util.bayesdl_mcmc">bayesdl_mcmc</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.compute_rule_usage" href="#imodels.rule_list.bayesian_rule_list.brl_util.compute_rule_usage">compute_rule_usage</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.default_permsdic" href="#imodels.rule_list.bayesian_rule_list.brl_util.default_permsdic">default_permsdic</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.fn_logliklihood" href="#imodels.rule_list.bayesian_rule_list.brl_util.fn_logliklihood">fn_logliklihood</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.fn_logposterior" href="#imodels.rule_list.bayesian_rule_list.brl_util.fn_logposterior">fn_logposterior</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.fn_logprior" href="#imodels.rule_list.bayesian_rule_list.brl_util.fn_logprior">fn_logprior</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.gelmanrubin" href="#imodels.rule_list.bayesian_rule_list.brl_util.gelmanrubin">gelmanrubin</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.get_point_estimate" href="#imodels.rule_list.bayesian_rule_list.brl_util.get_point_estimate">get_point_estimate</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.get_rule_rhs" href="#imodels.rule_list.bayesian_rule_list.brl_util.get_rule_rhs">get_rule_rhs</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.initialize_d" href="#imodels.rule_list.bayesian_rule_list.brl_util.initialize_d">initialize_d</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.mcmcchain" href="#imodels.rule_list.bayesian_rule_list.brl_util.mcmcchain">mcmcchain</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.merge_chains" href="#imodels.rule_list.bayesian_rule_list.brl_util.merge_chains">merge_chains</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.plot_chains" href="#imodels.rule_list.bayesian_rule_list.brl_util.plot_chains">plot_chains</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.preds_d_t" href="#imodels.rule_list.bayesian_rule_list.brl_util.preds_d_t">preds_d_t</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.prior_calculations" href="#imodels.rule_list.bayesian_rule_list.brl_util.prior_calculations">prior_calculations</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.proposal" href="#imodels.rule_list.bayesian_rule_list.brl_util.proposal">proposal</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.reset_permsdic" href="#imodels.rule_list.bayesian_rule_list.brl_util.reset_permsdic">reset_permsdic</a></code></li>
<li><code><a title="imodels.rule_list.bayesian_rule_list.brl_util.run_bdl_multichain_serial" href="#imodels.rule_list.bayesian_rule_list.brl_util.run_bdl_multichain_serial">run_bdl_multichain_serial</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>