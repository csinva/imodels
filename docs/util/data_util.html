<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os.path
from os.path import join as oj
from typing import Tuple

import numpy as np
import pandas as pd
import requests
import sklearn.datasets
from scipy.sparse import issparse
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

from imodels.util.tree_interaction_utils import make_rj, make_vp

DSET_CLASSIFICATION_KWARGS = {
    # classification
    &#34;pima_diabetes&#34;: {&#34;dataset_name&#34;: &#34;40715&#34;, &#34;data_source&#34;: &#34;openml&#34;},
    &#34;sonar&#34;: {&#34;dataset_name&#34;: &#34;sonar&#34;, &#34;data_source&#34;: &#34;pmlb&#34;},
    &#34;heart&#34;: {&#34;dataset_name&#34;: &#34;heart&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
    &#34;diabetes&#34;: {&#34;dataset_name&#34;: &#34;diabetes&#34;, &#34;data_source&#34;: &#34;pmlb&#34;},
    &#34;breast_cancer_recurrence&#34;: {
        &#34;dataset_name&#34;: &#34;breast_cancer&#34;,
        &#34;data_source&#34;: &#34;imodels&#34;,
    },
    &#34;breast_cancer_wisconsin&#34;: {
        &#34;dataset_name&#34;: &#34;breast_cancer&#34;,
        &#34;data_source&#34;: &#34;sklearn&#34;,
    },
    &#34;credit_g&#34;: {&#34;dataset_name&#34;: &#34;credit_g&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
    &#34;juvenile&#34;: {&#34;dataset_name&#34;: &#34;juvenile_clean&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
    &#34;compas&#34;: {&#34;dataset_name&#34;: &#34;compas_two_year_clean&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
    &#34;fico&#34;: {&#34;dataset_name&#34;: &#34;fico&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
    &#34;readmission&#34;: {
        &#34;dataset_name&#34;: &#34;readmission_clean&#34;,
        &#34;data_source&#34;: &#34;imodels&#34;,
    },  # big, 100k points
    &#34;adult&#34;: {&#34;dataset_name&#34;: &#34;1182&#34;, &#34;data_source&#34;: &#34;openml&#34;},  # big, 1e6 points
    # CDI classification
    &#34;csi_pecarn&#34;: {&#34;dataset_name&#34;: &#34;csi_pecarn_pred&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
    &#34;iai_pecarn&#34;: {&#34;dataset_name&#34;: &#34;iai_pecarn_pred&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
    &#34;tbi_pecarn&#34;: {&#34;dataset_name&#34;: &#34;tbi_pecarn_pred&#34;, &#34;data_source&#34;: &#34;imodels&#34;},
}

DSET_REGRESSION_KWARGS = {
    # regression
    &#34;bike_sharing&#34;: {&#34;dataset_name&#34;: &#34;42712&#34;, &#34;data_source&#34;: &#34;openml&#34;},
    &#34;friedman1&#34;: {&#34;dataset_name&#34;: &#34;friedman1&#34;, &#34;data_source&#34;: &#34;synthetic&#34;},
    &#34;friedman2&#34;: {&#34;dataset_name&#34;: &#34;friedman2&#34;, &#34;data_source&#34;: &#34;synthetic&#34;},
    &#34;friedman3&#34;: {&#34;dataset_name&#34;: &#34;friedman3&#34;, &#34;data_source&#34;: &#34;synthetic&#34;},
    &#34;diabetes_regr&#34;: {&#34;dataset_name&#34;: &#34;diabetes&#34;, &#34;data_source&#34;: &#34;sklearn&#34;},
    &#34;abalone&#34;: {&#34;dataset_name&#34;: &#34;183&#34;, &#34;data_source&#34;: &#34;openml&#34;},
    &#34;echo_months&#34;: {&#34;dataset_name&#34;: &#34;1199_BNG_echoMonths&#34;, &#34;data_source&#34;: &#34;pmlb&#34;},
    &#34;satellite_image&#34;: {&#34;dataset_name&#34;: &#34;294_satellite_image&#34;, &#34;data_source&#34;: &#34;pmlb&#34;},
    &#34;california_housing&#34;: {
        &#34;dataset_name&#34;: &#34;california_housing&#34;,
        &#34;data_source&#34;: &#34;sklearn&#34;,
    },
    # &#39;breast_tumor&#39;: {&#39;dataset_name&#39;: &#39;1201_BNG_breastTumor&#39;, &#39;data_source&#39;: &#39;pmlb&#39; # v big
}
DSET_KWARGS = {**DSET_CLASSIFICATION_KWARGS, **DSET_REGRESSION_KWARGS}


def _define_openml_outcomes(y, data_id: str):
    if data_id == &#34;59&#34;:  # ionosphere, positive is &#34;good&#34; class
        y = (y == &#34;g&#34;).astype(int)
    if data_id == &#34;183&#34;:  # abalone, need to convert strings to floats
        y = y.astype(float)
    if data_id == &#34;1182&#34;:  # adult, positive is &#34;&gt;50K&#34;
        y = (y == &#34;&gt;50K&#34;).astype(int)
    return y


def _clean_feat_names(feature_names):
    # shouldn&#39;t start with a digit
    return [&#34;X_&#34; + x if x[0].isdigit() else x for x in feature_names]


def _clean_features(X):
    if issparse(X):
        X = X.toarray()
    try:
        return X.astype(float)
    except:
        for j in range(X.shape[1]):
            try:
                X[:, j].astype(float)
            except:
                # non-numeric get replaced with numerical values
                classes, X[:, j] = np.unique(X[:, j], return_inverse=True)
    return X.astype(float)


def get_clean_dataset(
    dataset_name: str,
    data_source: str = &#34;imodels&#34;,
    data_path=&#34;data&#34;,
    convertna: bool = True,
    test_size: float = None,
    random_state: int = 42,
    verbose=True,
) -&gt; Tuple[np.ndarray, np.ndarray, list]:
    &#34;&#34;&#34;Fetch clean data (as numpy arrays) from various sources including imodels, pmlb, openml, and sklearn.
    If data is not downloaded, will download and cache. Otherwise will load locally.
    Cleans features so that they are type float and features names don&#39;t start with a digit.

    Parameters
    ----------
    dataset_name: str
        Checks for unique identifier in imodels.util.data_util.DSET_KWARGS
        Otherwise, unique dataset identifier (see https://github.com/csinva/imodels-data for unique identifiers)
    data_source: str
        options: &#39;imodels&#39;, &#39;pmlb&#39;, &#39;sklearn&#39;, &#39;openml&#39;, &#39;synthetic&#39;
    data_path: str
        path to load/save data (default: &#39;data&#39;)
    test_size: float, optional
        if not None, will split data into train and test sets (with fraction test_size in test set)
        &amp; change the return signature to `X_train, X_test, y_train, y_test, feature_names`
    random_state: int, optional
        if test_size is not None, will use this random state to split data


    Returns
    -------
    X: np.ndarray
        features
    y: np.ndarray
        outcome
    feature_names: list

    Example
    -------
    ```
    # download compas dataset from imodels
    X, y, feature_names = imodels.get_clean_dataset(&#39;compas_two_year_clean&#39;, data_source=&#39;imodels&#39;)
    # download ionosphere dataset from pmlb
    X, y, feature_names = imodels.get_clean_dataset(&#39;ionosphere&#39;, data_source=&#39;pmlb&#39;)
    # download liver dataset from openml
    X, y, feature_names = imodels.get_clean_dataset(&#39;8&#39;, data_source=&#39;openml&#39;)
    # download ca housing from sklearn
    X, y, feature_names = imodels.get_clean_dataset(&#39;california_housing&#39;, data_source=&#39;sklearn&#39;)
    ```
    &#34;&#34;&#34;
    if dataset_name in DSET_KWARGS:
        if verbose:
            data_source = DSET_KWARGS[dataset_name][&#34;data_source&#34;]
            dataset_name = DSET_KWARGS[dataset_name][&#34;dataset_name&#34;]
            print(f&#34;fetching {dataset_name} from {data_source}&#34;)
    assert data_source in [&#34;imodels&#34;, &#34;pmlb&#34;, &#34;sklearn&#34;, &#34;openml&#34;, &#34;synthetic&#34;], (
        data_source + &#34; not correct&#34;
    )
    if test_size is not None:

        def _split(X, y, feature_names):
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            return X_train, X_test, y_train, y_test, feature_names

    else:

        def _split(X, y, feature_names):
            return X, y, feature_names

    if data_source == &#34;imodels&#34;:
        if not dataset_name.endswith(&#34;csv&#34;):
            dataset_name = dataset_name + &#34;.csv&#34;
        if not os.path.isfile(dataset_name):
            _download_imodels_dataset(dataset_name, data_path)
        df = pd.read_csv(oj(data_path, &#34;imodels_data&#34;, dataset_name))
        X, y = df.iloc[:, :-1].values, df.iloc[:, -1].values
        feature_names = df.columns.values[:-1]
        if convertna:
            X = np.nan_to_num(X.astype(&#34;float32&#34;))
        return _split(X, y, _clean_feat_names(feature_names))
    elif data_source == &#34;pmlb&#34;:
        from pmlb import fetch_data

        feature_names = list(
            fetch_data(
                dataset_name,
                return_X_y=False,
                local_cache_dir=oj(data_path, &#34;pmlb_data&#34;),
            ).columns
        )
        feature_names.remove(&#34;target&#34;)
        X, y = fetch_data(
            dataset_name, return_X_y=True, local_cache_dir=oj(data_path, &#34;pmlb_data&#34;)
        )
        if (
            np.unique(y).size == 2
        ):  # if binary classification, ensure that the classes are 0 and 1
            y -= np.min(y)
        return _split(_clean_features(X), y, _clean_feat_names(feature_names))
    elif data_source == &#34;sklearn&#34;:
        if dataset_name == &#34;diabetes&#34;:
            data = sklearn.datasets.load_diabetes()
        elif dataset_name == &#34;california_housing&#34;:
            data = sklearn.datasets.fetch_california_housing(
                data_home=oj(data_path, &#34;sklearn_data&#34;)
            )
        elif dataset_name == &#34;breast_cancer&#34;:
            data = sklearn.datasets.load_breast_cancer()
        return data[&#34;data&#34;], data[&#34;target&#34;], _clean_feat_names(data[&#34;feature_names&#34;])
    elif (
        data_source == &#34;openml&#34;
    ):  # note this api might change in newer sklearn - should give dataset-id not name
        data = sklearn.datasets.fetch_openml(
            data_id=dataset_name, data_home=oj(data_path, &#34;openml_data&#34;), parser=&#34;auto&#34;
        )
        X, y, feature_names = (
            data[&#34;data&#34;],
            data[&#34;target&#34;],
            _clean_feat_names(data[&#34;feature_names&#34;]),
        )
        if isinstance(X, pd.DataFrame):
            X = X.values
        if isinstance(y, pd.Series):
            y = y.values
        y = _define_openml_outcomes(y, dataset_name)
        return _split(_clean_features(X), y, _clean_feat_names(feature_names))
    elif data_source == &#34;synthetic&#34;:
        if dataset_name == &#34;friedman1&#34;:
            X, y = sklearn.datasets.make_friedman1(n_samples=200, n_features=10)
        elif dataset_name == &#34;friedman2&#34;:
            X, y = sklearn.datasets.make_friedman2(n_samples=200)
        elif dataset_name == &#34;friedman3&#34;:
            X, y = sklearn.datasets.make_friedman3(n_samples=200)
        elif dataset_name == &#34;radchenko_james&#34;:
            X, y = make_rj()
        elif dataset_name == &#34;vo_pati&#34;:
            X, y = make_vp()
        return _split(X, y, [&#34;X_&#34; + str(i + 1) for i in range(X.shape[1])])


def _download_imodels_dataset(dataset_fname, data_path: str):
    dataset_fname = dataset_fname.split(&#34;/&#34;)[-1]  # remove anything about the path
    download_path = f&#34;https://raw.githubusercontent.com/csinva/imodels-data/master/data_cleaned/{dataset_fname}&#34;
    r = requests.get(download_path)
    if r.status_code == 404:
        raise Exception(
            f&#34;404 Error for dataset {dataset_fname} (see valid files at https://github.com/csinva/imodels-data/tree/master/data_cleaned)&#34;
        )

    os.makedirs(oj(data_path, &#34;imodels_data&#34;), exist_ok=True)
    with open(oj(data_path, &#34;imodels_data&#34;, dataset_fname), &#34;w&#34;) as f:
        f.write(r.text)


def encode_categories(X, features, encoder=None):
    columns_to_keep = list(set(X.columns).difference(features))
    X_encoded = X.loc[:, columns_to_keep]
    X_cat = pd.DataFrame({f: X.loc[:, f] for f in features})

    if encoder is None:
        one_hot_encoder = OneHotEncoder(sparse=False, categories=&#34;auto&#34;)
        X_one_hot = pd.DataFrame(one_hot_encoder.fit_transform(X_cat))
    else:
        one_hot_encoder = encoder
        X_one_hot = pd.DataFrame(one_hot_encoder.transform(X_cat))
    X_one_hot.columns = one_hot_encoder.get_feature_names_out(features)
    X_encoded = pd.concat([X_encoded, X_one_hot], axis=1)
    if encoder is not None:
        return X_encoded
    return X_encoded, one_hot_encoder


if __name__ == &#34;__main__&#34;:
    import imodels

    # X, y, feature_names = imodels.get_clean_dataset(&#39;compas_two_year_clean&#39;, data_source=&#39;imodels&#39;, test_size=0.5)
    X_train, X_test, y_train, y_test, feature_names = imodels.get_clean_dataset(
        &#34;compas_two_year_clean&#34;, data_source=&#34;imodels&#34;, test_size=0.5
    )
    print(X_train.shape, y_train.shape)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imodels.util.data_util.encode_categories"><code class="name flex">
<span>def <span class="ident">encode_categories</span></span>(<span>X, features, encoder=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode_categories(X, features, encoder=None):
    columns_to_keep = list(set(X.columns).difference(features))
    X_encoded = X.loc[:, columns_to_keep]
    X_cat = pd.DataFrame({f: X.loc[:, f] for f in features})

    if encoder is None:
        one_hot_encoder = OneHotEncoder(sparse=False, categories=&#34;auto&#34;)
        X_one_hot = pd.DataFrame(one_hot_encoder.fit_transform(X_cat))
    else:
        one_hot_encoder = encoder
        X_one_hot = pd.DataFrame(one_hot_encoder.transform(X_cat))
    X_one_hot.columns = one_hot_encoder.get_feature_names_out(features)
    X_encoded = pd.concat([X_encoded, X_one_hot], axis=1)
    if encoder is not None:
        return X_encoded
    return X_encoded, one_hot_encoder</code></pre>
</details>
</dd>
<dt id="imodels.util.data_util.get_clean_dataset"><code class="name flex">
<span>def <span class="ident">get_clean_dataset</span></span>(<span>dataset_name: str, data_source: str = 'imodels', data_path='data', convertna: bool = True, test_size: float = None, random_state: int = 42, verbose=True) ‑> Tuple[numpy.ndarray, numpy.ndarray, list]</span>
</code></dt>
<dd>
<div class="desc"><p>Fetch clean data (as numpy arrays) from various sources including imodels, pmlb, openml, and sklearn.
If data is not downloaded, will download and cache. Otherwise will load locally.
Cleans features so that they are type float and features names don't start with a digit.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Checks for unique identifier in imodels.util.data_util.DSET_KWARGS
Otherwise, unique dataset identifier (see <a href="https://github.com/csinva/imodels-data">https://github.com/csinva/imodels-data</a> for unique identifiers)</dd>
<dt><strong><code>data_source</code></strong> :&ensp;<code>str</code></dt>
<dd>options: 'imodels', 'pmlb', 'sklearn', 'openml', 'synthetic'</dd>
<dt><strong><code>data_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to load/save data (default: 'data')</dd>
<dt><strong><code>test_size</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>if not None, will split data into train and test sets (with fraction test_size in test set)
&amp; change the return signature to <code>X_train, X_test, y_train, y_test, feature_names</code></dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>if test_size is not None, will use this random state to split data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>features</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>outcome</dd>
<dt><strong><code>feature_names</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code># download compas dataset from imodels
X, y, feature_names = imodels.get_clean_dataset('compas_two_year_clean', data_source='imodels')
# download ionosphere dataset from pmlb
X, y, feature_names = imodels.get_clean_dataset('ionosphere', data_source='pmlb')
# download liver dataset from openml
X, y, feature_names = imodels.get_clean_dataset('8', data_source='openml')
# download ca housing from sklearn
X, y, feature_names = imodels.get_clean_dataset('california_housing', data_source='sklearn')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_clean_dataset(
    dataset_name: str,
    data_source: str = &#34;imodels&#34;,
    data_path=&#34;data&#34;,
    convertna: bool = True,
    test_size: float = None,
    random_state: int = 42,
    verbose=True,
) -&gt; Tuple[np.ndarray, np.ndarray, list]:
    &#34;&#34;&#34;Fetch clean data (as numpy arrays) from various sources including imodels, pmlb, openml, and sklearn.
    If data is not downloaded, will download and cache. Otherwise will load locally.
    Cleans features so that they are type float and features names don&#39;t start with a digit.

    Parameters
    ----------
    dataset_name: str
        Checks for unique identifier in imodels.util.data_util.DSET_KWARGS
        Otherwise, unique dataset identifier (see https://github.com/csinva/imodels-data for unique identifiers)
    data_source: str
        options: &#39;imodels&#39;, &#39;pmlb&#39;, &#39;sklearn&#39;, &#39;openml&#39;, &#39;synthetic&#39;
    data_path: str
        path to load/save data (default: &#39;data&#39;)
    test_size: float, optional
        if not None, will split data into train and test sets (with fraction test_size in test set)
        &amp; change the return signature to `X_train, X_test, y_train, y_test, feature_names`
    random_state: int, optional
        if test_size is not None, will use this random state to split data


    Returns
    -------
    X: np.ndarray
        features
    y: np.ndarray
        outcome
    feature_names: list

    Example
    -------
    ```
    # download compas dataset from imodels
    X, y, feature_names = imodels.get_clean_dataset(&#39;compas_two_year_clean&#39;, data_source=&#39;imodels&#39;)
    # download ionosphere dataset from pmlb
    X, y, feature_names = imodels.get_clean_dataset(&#39;ionosphere&#39;, data_source=&#39;pmlb&#39;)
    # download liver dataset from openml
    X, y, feature_names = imodels.get_clean_dataset(&#39;8&#39;, data_source=&#39;openml&#39;)
    # download ca housing from sklearn
    X, y, feature_names = imodels.get_clean_dataset(&#39;california_housing&#39;, data_source=&#39;sklearn&#39;)
    ```
    &#34;&#34;&#34;
    if dataset_name in DSET_KWARGS:
        if verbose:
            data_source = DSET_KWARGS[dataset_name][&#34;data_source&#34;]
            dataset_name = DSET_KWARGS[dataset_name][&#34;dataset_name&#34;]
            print(f&#34;fetching {dataset_name} from {data_source}&#34;)
    assert data_source in [&#34;imodels&#34;, &#34;pmlb&#34;, &#34;sklearn&#34;, &#34;openml&#34;, &#34;synthetic&#34;], (
        data_source + &#34; not correct&#34;
    )
    if test_size is not None:

        def _split(X, y, feature_names):
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            return X_train, X_test, y_train, y_test, feature_names

    else:

        def _split(X, y, feature_names):
            return X, y, feature_names

    if data_source == &#34;imodels&#34;:
        if not dataset_name.endswith(&#34;csv&#34;):
            dataset_name = dataset_name + &#34;.csv&#34;
        if not os.path.isfile(dataset_name):
            _download_imodels_dataset(dataset_name, data_path)
        df = pd.read_csv(oj(data_path, &#34;imodels_data&#34;, dataset_name))
        X, y = df.iloc[:, :-1].values, df.iloc[:, -1].values
        feature_names = df.columns.values[:-1]
        if convertna:
            X = np.nan_to_num(X.astype(&#34;float32&#34;))
        return _split(X, y, _clean_feat_names(feature_names))
    elif data_source == &#34;pmlb&#34;:
        from pmlb import fetch_data

        feature_names = list(
            fetch_data(
                dataset_name,
                return_X_y=False,
                local_cache_dir=oj(data_path, &#34;pmlb_data&#34;),
            ).columns
        )
        feature_names.remove(&#34;target&#34;)
        X, y = fetch_data(
            dataset_name, return_X_y=True, local_cache_dir=oj(data_path, &#34;pmlb_data&#34;)
        )
        if (
            np.unique(y).size == 2
        ):  # if binary classification, ensure that the classes are 0 and 1
            y -= np.min(y)
        return _split(_clean_features(X), y, _clean_feat_names(feature_names))
    elif data_source == &#34;sklearn&#34;:
        if dataset_name == &#34;diabetes&#34;:
            data = sklearn.datasets.load_diabetes()
        elif dataset_name == &#34;california_housing&#34;:
            data = sklearn.datasets.fetch_california_housing(
                data_home=oj(data_path, &#34;sklearn_data&#34;)
            )
        elif dataset_name == &#34;breast_cancer&#34;:
            data = sklearn.datasets.load_breast_cancer()
        return data[&#34;data&#34;], data[&#34;target&#34;], _clean_feat_names(data[&#34;feature_names&#34;])
    elif (
        data_source == &#34;openml&#34;
    ):  # note this api might change in newer sklearn - should give dataset-id not name
        data = sklearn.datasets.fetch_openml(
            data_id=dataset_name, data_home=oj(data_path, &#34;openml_data&#34;), parser=&#34;auto&#34;
        )
        X, y, feature_names = (
            data[&#34;data&#34;],
            data[&#34;target&#34;],
            _clean_feat_names(data[&#34;feature_names&#34;]),
        )
        if isinstance(X, pd.DataFrame):
            X = X.values
        if isinstance(y, pd.Series):
            y = y.values
        y = _define_openml_outcomes(y, dataset_name)
        return _split(_clean_features(X), y, _clean_feat_names(feature_names))
    elif data_source == &#34;synthetic&#34;:
        if dataset_name == &#34;friedman1&#34;:
            X, y = sklearn.datasets.make_friedman1(n_samples=200, n_features=10)
        elif dataset_name == &#34;friedman2&#34;:
            X, y = sklearn.datasets.make_friedman2(n_samples=200)
        elif dataset_name == &#34;friedman3&#34;:
            X, y = sklearn.datasets.make_friedman3(n_samples=200)
        elif dataset_name == &#34;radchenko_james&#34;:
            X, y = make_rj()
        elif dataset_name == &#34;vo_pati&#34;:
            X, y = make_vp()
        return _split(X, y, [&#34;X_&#34; + str(i + 1) for i in range(X.shape[1])])</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index 🔍</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodels.util" href="index.html">imodels.util</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imodels.util.data_util.encode_categories" href="#imodels.util.data_util.encode_categories">encode_categories</a></code></li>
<li><code><a title="imodels.util.data_util.get_clean_dataset" href="#imodels.util.data_util.get_clean_dataset">get_clean_dataset</a></code></li>
</ul>
</li>
</ul>
<p><img align="center" width=100% src="https://csinva.io/imodels/img/anim.gif"> </img></p>
<!-- add wave animation -->
</nav>
</main>
<footer id="footer">
</footer>
</body>
</html>
<!-- add github corner -->
<a href="https://github.com/csinva/imodels" class="github-corner" aria-label="View source on GitHub"><svg width="120" height="120" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="m128.3,109.0 c113.8,99.7 119.0,89.6 119.0,89.6 c122.0,82.7 120.5,78.6 120.5,78.6 c119.2,72.0 123.4,76.3 123.4,76.3 c127.3,80.9 125.5,87.3 125.5,87.3 c122.9,97.6 130.6,101.9 134.4,103.2" fill="currentcolor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<!-- add wave animation stylesheet -->
<link rel="stylesheet" href="github.css">