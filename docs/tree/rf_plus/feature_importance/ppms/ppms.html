<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># generic imports
import copy
from abc import ABC
import numpy as np
from scipy.special import expit
from joblib import Parallel, delayed

class MDIPlusGenericRegressorPPM(ABC):
    &#34;&#34;&#34;
    Partial prediction model for arbitrary estimators. Parallelized for speedup.
    &#34;&#34;&#34;

    def __init__(self, estimator):
        &#34;&#34;&#34;
        Constructor for the MDIPlusGenericRegressorPPM class.
        
        Args:
            estimator (object): A regression estimator object with a
                                &#39;predict&#39; method.
        &#34;&#34;&#34;
        self.estimator = copy.deepcopy(estimator)

    def predict_full(self, blocked_data):
        &#34;&#34;&#34;
        Gets the full predictions for the model.
        
        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
        
        Returns:
            np.ndarray: Full predictions for the model.
        &#34;&#34;&#34;
        return self.estimator.predict(blocked_data.get_all_data())
    
    def predict_partial(self, blocked_data, mode, l2norm):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we want to incorporate the
        intercept of the regression model into the partial predictions.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        n_blocks = blocked_data.n_blocks
        partial_preds = {}
        for k in range(n_blocks):
            partial_preds[k] = self.predict_partial_k(blocked_data, k, mode,
                                                      l2norm)
        return partial_preds
    
    def predict_partial_subtract_intercept(self, blocked_data, njobs = 1):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we do not want to consider
        the intercept of the regression model, such as the partial linear LMDI+
        implementation.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sign (bool): indicator for if we want to retain the direction of
                         the partial prediction.
            normalize (bool): indicator for if we want to normalize the partial
                              predictions by the size of the full prediction.
            njobs (int): number of jobs to run in parallel.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        n_blocks = blocked_data.n_blocks
        
        # helper function to parallelize the partial prediction computation
        def predict_wrapper(k):
            psibeta = self.predict_partial_k_subtract_intercept(blocked_data, k)
            return psibeta
        
        # delayed makes sure that predictions get arranged in the correct order
        partial_preds = Parallel(n_jobs=njobs)(delayed(predict_wrapper)(k)
                                               for k in range(n_blocks))
        
        # parse through the outputs of the parallel data structure
        partial_pred_storage = {}
        for k in range(len(partial_preds)):
            partial_pred_storage[k] = partial_preds[k]

        return partial_pred_storage
    
    def predict_partial_k(self, blocked_data, k, mode, l2norm):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, including the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        modified_data = blocked_data.get_modified_data(k, &#34;keep_rest_zero&#34;)
        return self.estimator.predict(modified_data)
    
    def predict_partial_k_subtract_intercept(self, blocked_data, k):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, omitting the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        psi_k = blocked_data.get_modified_data(k, &#34;only_k&#34;)
        coefs = self.estimator.coef_
        return psi_k @ coefs

class MDIPlusGenericClassifierPPM(ABC):
    &#34;&#34;&#34;
    Partial prediction model for arbitrary classification estimators. May be slow.
    &#34;&#34;&#34;
    def __init__(self, estimator):
        &#34;&#34;&#34;
        Constructor for the MDIPlusGenericClassifierPPM class.
        
        Args:
            estimator (object): A classification estimator object with a
                                &#39;predict_proba&#39; method.
        &#34;&#34;&#34;
        self.estimator = copy.deepcopy(estimator)

    def predict_full(self, blocked_data):
        &#34;&#34;&#34;
        Gets the full predictions for the model.
        
        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            
        Returns:
            np.ndarray: Full predictions for the model.
        &#34;&#34;&#34;
        return self.estimator.predict_proba(blocked_data.get_all_data())[:,1]
    
    def predict_partial(self, blocked_data, mode, l2norm, sigmoid):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we want to incorporate the
        intercept of the regression model into the partial predictions.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sigmoid (bool): indicator for if we want to apply the sigmoid
                            function to our classification outcome.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        n_blocks = blocked_data.n_blocks
        partial_preds = {}
        for k in range(n_blocks):
            partial_preds[k] = self.predict_partial_k(blocked_data, k, mode,
                                                      l2norm, sigmoid)
        return partial_preds
    
    def predict_partial_subtract_intercept(self, blocked_data, njobs = 1):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we do not want to consider
        the intercept of the regression model, such as the partial linear LMDI+
        implementation.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sign (bool): indicator for if we want to retain the direction of
                         the partial prediction.
            sigmoid (bool): indicator for if we want to apply the sigmoid
                            function to our classification outcome.
            normalize (bool): indicator for if we want to normalize the partial
                              predictions by the size of the full prediction.
            njobs (int): number of jobs to run in parallel.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        n_blocks = blocked_data.n_blocks
        
        # helper function to parallelize the partial prediction computation
        def predict_wrapper(k):
            psibeta = self.predict_partial_k_subtract_intercept(blocked_data, k)
            return psibeta
        
        # delayed makes sure that predictions get arranged in the correct order
        partial_preds = Parallel(n_jobs=njobs)(delayed(predict_wrapper)(k)
                                               for k in range(n_blocks))
        
        # parse through the outputs of the parallel data structure
        partial_pred_storage = {}
        for k in range(len(partial_preds)):
            partial_pred_storage[k] = partial_preds[k]

        return partial_pred_storage
    
    def predict_partial_k_subtract_intercept(self, blocked_data, k):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, omitting the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        psi_k = blocked_data.get_modified_data(k, &#34;only_k&#34;)
        coefs = self.estimator.coef_
        # reshape coefs if necessary
        if coefs.shape[0] != psi_k.shape[1]:
            coefs = coefs.reshape(-1,1).flatten()
        return psi_k @ coefs

    def predict_partial_k(self, blocked_data, k, mode, l2norm, sigmoid):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, including the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sigmoid (bool): indicator for if we want to apply the sigmoid
                            function to our classification outcome.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        modified_data = blocked_data.get_modified_data(k, mode)
        coefs = self.estimator.coef_
        return modified_data @ coefs + self.estimator.intercept_</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM"><code class="flex name class">
<span>class <span class="ident">MDIPlusGenericClassifierPPM</span></span>
<span>(</span><span>estimator)</span>
</code></dt>
<dd>
<div class="desc"><p>Partial prediction model for arbitrary classification estimators. May be slow.</p>
<p>Constructor for the MDIPlusGenericClassifierPPM class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>estimator</code></strong> :&ensp;<code>object</code></dt>
<dd>A classification estimator object with a
'predict_proba' method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MDIPlusGenericClassifierPPM(ABC):
    &#34;&#34;&#34;
    Partial prediction model for arbitrary classification estimators. May be slow.
    &#34;&#34;&#34;
    def __init__(self, estimator):
        &#34;&#34;&#34;
        Constructor for the MDIPlusGenericClassifierPPM class.
        
        Args:
            estimator (object): A classification estimator object with a
                                &#39;predict_proba&#39; method.
        &#34;&#34;&#34;
        self.estimator = copy.deepcopy(estimator)

    def predict_full(self, blocked_data):
        &#34;&#34;&#34;
        Gets the full predictions for the model.
        
        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            
        Returns:
            np.ndarray: Full predictions for the model.
        &#34;&#34;&#34;
        return self.estimator.predict_proba(blocked_data.get_all_data())[:,1]
    
    def predict_partial(self, blocked_data, mode, l2norm, sigmoid):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we want to incorporate the
        intercept of the regression model into the partial predictions.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sigmoid (bool): indicator for if we want to apply the sigmoid
                            function to our classification outcome.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        n_blocks = blocked_data.n_blocks
        partial_preds = {}
        for k in range(n_blocks):
            partial_preds[k] = self.predict_partial_k(blocked_data, k, mode,
                                                      l2norm, sigmoid)
        return partial_preds
    
    def predict_partial_subtract_intercept(self, blocked_data, njobs = 1):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we do not want to consider
        the intercept of the regression model, such as the partial linear LMDI+
        implementation.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sign (bool): indicator for if we want to retain the direction of
                         the partial prediction.
            sigmoid (bool): indicator for if we want to apply the sigmoid
                            function to our classification outcome.
            normalize (bool): indicator for if we want to normalize the partial
                              predictions by the size of the full prediction.
            njobs (int): number of jobs to run in parallel.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        n_blocks = blocked_data.n_blocks
        
        # helper function to parallelize the partial prediction computation
        def predict_wrapper(k):
            psibeta = self.predict_partial_k_subtract_intercept(blocked_data, k)
            return psibeta
        
        # delayed makes sure that predictions get arranged in the correct order
        partial_preds = Parallel(n_jobs=njobs)(delayed(predict_wrapper)(k)
                                               for k in range(n_blocks))
        
        # parse through the outputs of the parallel data structure
        partial_pred_storage = {}
        for k in range(len(partial_preds)):
            partial_pred_storage[k] = partial_preds[k]

        return partial_pred_storage
    
    def predict_partial_k_subtract_intercept(self, blocked_data, k):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, omitting the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        psi_k = blocked_data.get_modified_data(k, &#34;only_k&#34;)
        coefs = self.estimator.coef_
        # reshape coefs if necessary
        if coefs.shape[0] != psi_k.shape[1]:
            coefs = coefs.reshape(-1,1).flatten()
        return psi_k @ coefs

    def predict_partial_k(self, blocked_data, k, mode, l2norm, sigmoid):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, including the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sigmoid (bool): indicator for if we want to apply the sigmoid
                            function to our classification outcome.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        modified_data = blocked_data.get_modified_data(k, mode)
        coefs = self.estimator.coef_
        return modified_data @ coefs + self.estimator.intercept_</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_full"><code class="name flex">
<span>def <span class="ident">predict_full</span></span>(<span>self, blocked_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the full predictions for the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Full predictions for the model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_full(self, blocked_data):
    &#34;&#34;&#34;
    Gets the full predictions for the model.
    
    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        
    Returns:
        np.ndarray: Full predictions for the model.
    &#34;&#34;&#34;
    return self.estimator.predict_proba(blocked_data.get_all_data())[:,1]</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial"><code class="name flex">
<span>def <span class="ident">predict_partial</span></span>(<span>self, blocked_data, mode, l2norm, sigmoid)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions. To be used when we want to incorporate the
intercept of the regression model into the partial predictions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>either {"keep_k", "keep_rest"}, see BlockPartitionedData</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
<dt><strong><code>sigmoid</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to apply the sigmoid
function to our classification outcome.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial(self, blocked_data, mode, l2norm, sigmoid):
    &#34;&#34;&#34;
    Gets the partial predictions. To be used when we want to incorporate the
    intercept of the regression model into the partial predictions.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.
        sigmoid (bool): indicator for if we want to apply the sigmoid
                        function to our classification outcome.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    n_blocks = blocked_data.n_blocks
    partial_preds = {}
    for k in range(n_blocks):
        partial_preds[k] = self.predict_partial_k(blocked_data, k, mode,
                                                  l2norm, sigmoid)
    return partial_preds</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_k"><code class="name flex">
<span>def <span class="ident">predict_partial_k</span></span>(<span>self, blocked_data, k, mode, l2norm, sigmoid)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions for an individual feature k, including the
regression intercept in the predictions for the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>feature index.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>either {"keep_k", "keep_rest"}, see BlockPartitionedData</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
<dt><strong><code>sigmoid</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to apply the sigmoid
function to our classification outcome.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial_k(self, blocked_data, k, mode, l2norm, sigmoid):
    &#34;&#34;&#34;
    Gets the partial predictions for an individual feature k, including the
    regression intercept in the predictions for the model.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        k (int): feature index.
        mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.
        sigmoid (bool): indicator for if we want to apply the sigmoid
                        function to our classification outcome.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    
    modified_data = blocked_data.get_modified_data(k, mode)
    coefs = self.estimator.coef_
    return modified_data @ coefs + self.estimator.intercept_</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_k_subtract_intercept"><code class="name flex">
<span>def <span class="ident">predict_partial_k_subtract_intercept</span></span>(<span>self, blocked_data, k)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions for an individual feature k, omitting the
regression intercept in the predictions for the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>feature index.</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial_k_subtract_intercept(self, blocked_data, k):
    &#34;&#34;&#34;
    Gets the partial predictions for an individual feature k, omitting the
    regression intercept in the predictions for the model.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        k (int): feature index.
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    
    psi_k = blocked_data.get_modified_data(k, &#34;only_k&#34;)
    coefs = self.estimator.coef_
    # reshape coefs if necessary
    if coefs.shape[0] != psi_k.shape[1]:
        coefs = coefs.reshape(-1,1).flatten()
    return psi_k @ coefs</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_subtract_intercept"><code class="name flex">
<span>def <span class="ident">predict_partial_subtract_intercept</span></span>(<span>self, blocked_data, njobs=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions. To be used when we do not want to consider
the intercept of the regression model, such as the partial linear LMDI+
implementation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
<dt><strong><code>sign</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to retain the direction of
the partial prediction.</dd>
<dt><strong><code>sigmoid</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to apply the sigmoid
function to our classification outcome.</dd>
<dt><strong><code>normalize</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to normalize the partial
predictions by the size of the full prediction.</dd>
<dt><strong><code>njobs</code></strong> :&ensp;<code>int</code></dt>
<dd>number of jobs to run in parallel.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial_subtract_intercept(self, blocked_data, njobs = 1):
    &#34;&#34;&#34;
    Gets the partial predictions. To be used when we do not want to consider
    the intercept of the regression model, such as the partial linear LMDI+
    implementation.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.
        sign (bool): indicator for if we want to retain the direction of
                     the partial prediction.
        sigmoid (bool): indicator for if we want to apply the sigmoid
                        function to our classification outcome.
        normalize (bool): indicator for if we want to normalize the partial
                          predictions by the size of the full prediction.
        njobs (int): number of jobs to run in parallel.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    
    n_blocks = blocked_data.n_blocks
    
    # helper function to parallelize the partial prediction computation
    def predict_wrapper(k):
        psibeta = self.predict_partial_k_subtract_intercept(blocked_data, k)
        return psibeta
    
    # delayed makes sure that predictions get arranged in the correct order
    partial_preds = Parallel(n_jobs=njobs)(delayed(predict_wrapper)(k)
                                           for k in range(n_blocks))
    
    # parse through the outputs of the parallel data structure
    partial_pred_storage = {}
    for k in range(len(partial_preds)):
        partial_pred_storage[k] = partial_preds[k]

    return partial_pred_storage</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM"><code class="flex name class">
<span>class <span class="ident">MDIPlusGenericRegressorPPM</span></span>
<span>(</span><span>estimator)</span>
</code></dt>
<dd>
<div class="desc"><p>Partial prediction model for arbitrary estimators. Parallelized for speedup.</p>
<p>Constructor for the MDIPlusGenericRegressorPPM class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>estimator</code></strong> :&ensp;<code>object</code></dt>
<dd>A regression estimator object with a
'predict' method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MDIPlusGenericRegressorPPM(ABC):
    &#34;&#34;&#34;
    Partial prediction model for arbitrary estimators. Parallelized for speedup.
    &#34;&#34;&#34;

    def __init__(self, estimator):
        &#34;&#34;&#34;
        Constructor for the MDIPlusGenericRegressorPPM class.
        
        Args:
            estimator (object): A regression estimator object with a
                                &#39;predict&#39; method.
        &#34;&#34;&#34;
        self.estimator = copy.deepcopy(estimator)

    def predict_full(self, blocked_data):
        &#34;&#34;&#34;
        Gets the full predictions for the model.
        
        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
        
        Returns:
            np.ndarray: Full predictions for the model.
        &#34;&#34;&#34;
        return self.estimator.predict(blocked_data.get_all_data())
    
    def predict_partial(self, blocked_data, mode, l2norm):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we want to incorporate the
        intercept of the regression model into the partial predictions.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        n_blocks = blocked_data.n_blocks
        partial_preds = {}
        for k in range(n_blocks):
            partial_preds[k] = self.predict_partial_k(blocked_data, k, mode,
                                                      l2norm)
        return partial_preds
    
    def predict_partial_subtract_intercept(self, blocked_data, njobs = 1):
        &#34;&#34;&#34;
        Gets the partial predictions. To be used when we do not want to consider
        the intercept of the regression model, such as the partial linear LMDI+
        implementation.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.
            sign (bool): indicator for if we want to retain the direction of
                         the partial prediction.
            normalize (bool): indicator for if we want to normalize the partial
                              predictions by the size of the full prediction.
            njobs (int): number of jobs to run in parallel.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        n_blocks = blocked_data.n_blocks
        
        # helper function to parallelize the partial prediction computation
        def predict_wrapper(k):
            psibeta = self.predict_partial_k_subtract_intercept(blocked_data, k)
            return psibeta
        
        # delayed makes sure that predictions get arranged in the correct order
        partial_preds = Parallel(n_jobs=njobs)(delayed(predict_wrapper)(k)
                                               for k in range(n_blocks))
        
        # parse through the outputs of the parallel data structure
        partial_pred_storage = {}
        for k in range(len(partial_preds)):
            partial_pred_storage[k] = partial_preds[k]

        return partial_pred_storage
    
    def predict_partial_k(self, blocked_data, k, mode, l2norm):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, including the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        modified_data = blocked_data.get_modified_data(k, &#34;keep_rest_zero&#34;)
        return self.estimator.predict(modified_data)
    
    def predict_partial_k_subtract_intercept(self, blocked_data, k):
        &#34;&#34;&#34;
        Gets the partial predictions for an individual feature k, omitting the
        regression intercept in the predictions for the model.

        Args:
            blocked_data (BlockedPartitionData): Psi(X) data.
            k (int): feature index.
            l2norm (bool): indicator for if we want to take the l2-normed
                           product of the data and the coefficients.

        Returns:
            dict: mapping of feature index to partial predictions.
        &#34;&#34;&#34;
        
        psi_k = blocked_data.get_modified_data(k, &#34;only_k&#34;)
        coefs = self.estimator.coef_
        return psi_k @ coefs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_full"><code class="name flex">
<span>def <span class="ident">predict_full</span></span>(<span>self, blocked_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the full predictions for the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Full predictions for the model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_full(self, blocked_data):
    &#34;&#34;&#34;
    Gets the full predictions for the model.
    
    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
    
    Returns:
        np.ndarray: Full predictions for the model.
    &#34;&#34;&#34;
    return self.estimator.predict(blocked_data.get_all_data())</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial"><code class="name flex">
<span>def <span class="ident">predict_partial</span></span>(<span>self, blocked_data, mode, l2norm)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions. To be used when we want to incorporate the
intercept of the regression model into the partial predictions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>either {"keep_k", "keep_rest"}, see BlockPartitionedData</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial(self, blocked_data, mode, l2norm):
    &#34;&#34;&#34;
    Gets the partial predictions. To be used when we want to incorporate the
    intercept of the regression model into the partial predictions.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    
    n_blocks = blocked_data.n_blocks
    partial_preds = {}
    for k in range(n_blocks):
        partial_preds[k] = self.predict_partial_k(blocked_data, k, mode,
                                                  l2norm)
    return partial_preds</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_k"><code class="name flex">
<span>def <span class="ident">predict_partial_k</span></span>(<span>self, blocked_data, k, mode, l2norm)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions for an individual feature k, including the
regression intercept in the predictions for the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>feature index.</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>either {"keep_k", "keep_rest"}, see BlockPartitionedData</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial_k(self, blocked_data, k, mode, l2norm):
    &#34;&#34;&#34;
    Gets the partial predictions for an individual feature k, including the
    regression intercept in the predictions for the model.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        k (int): feature index.
        mode (str): either {&#34;keep_k&#34;, &#34;keep_rest&#34;}, see BlockPartitionedData
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    
    modified_data = blocked_data.get_modified_data(k, &#34;keep_rest_zero&#34;)
    return self.estimator.predict(modified_data)</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_k_subtract_intercept"><code class="name flex">
<span>def <span class="ident">predict_partial_k_subtract_intercept</span></span>(<span>self, blocked_data, k)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions for an individual feature k, omitting the
regression intercept in the predictions for the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>feature index.</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial_k_subtract_intercept(self, blocked_data, k):
    &#34;&#34;&#34;
    Gets the partial predictions for an individual feature k, omitting the
    regression intercept in the predictions for the model.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        k (int): feature index.
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    
    psi_k = blocked_data.get_modified_data(k, &#34;only_k&#34;)
    coefs = self.estimator.coef_
    return psi_k @ coefs</code></pre>
</details>
</dd>
<dt id="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_subtract_intercept"><code class="name flex">
<span>def <span class="ident">predict_partial_subtract_intercept</span></span>(<span>self, blocked_data, njobs=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the partial predictions. To be used when we do not want to consider
the intercept of the regression model, such as the partial linear LMDI+
implementation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>blocked_data</code></strong> :&ensp;<code>BlockedPartitionData</code></dt>
<dd>Psi(X) data.</dd>
<dt><strong><code>l2norm</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to take the l2-normed
product of the data and the coefficients.</dd>
<dt><strong><code>sign</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to retain the direction of
the partial prediction.</dd>
<dt><strong><code>normalize</code></strong> :&ensp;<code>bool</code></dt>
<dd>indicator for if we want to normalize the partial
predictions by the size of the full prediction.</dd>
<dt><strong><code>njobs</code></strong> :&ensp;<code>int</code></dt>
<dd>number of jobs to run in parallel.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>mapping of feature index to partial predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_partial_subtract_intercept(self, blocked_data, njobs = 1):
    &#34;&#34;&#34;
    Gets the partial predictions. To be used when we do not want to consider
    the intercept of the regression model, such as the partial linear LMDI+
    implementation.

    Args:
        blocked_data (BlockedPartitionData): Psi(X) data.
        l2norm (bool): indicator for if we want to take the l2-normed
                       product of the data and the coefficients.
        sign (bool): indicator for if we want to retain the direction of
                     the partial prediction.
        normalize (bool): indicator for if we want to normalize the partial
                          predictions by the size of the full prediction.
        njobs (int): number of jobs to run in parallel.

    Returns:
        dict: mapping of feature index to partial predictions.
    &#34;&#34;&#34;
    
    n_blocks = blocked_data.n_blocks
    
    # helper function to parallelize the partial prediction computation
    def predict_wrapper(k):
        psibeta = self.predict_partial_k_subtract_intercept(blocked_data, k)
        return psibeta
    
    # delayed makes sure that predictions get arranged in the correct order
    partial_preds = Parallel(n_jobs=njobs)(delayed(predict_wrapper)(k)
                                           for k in range(n_blocks))
    
    # parse through the outputs of the parallel data structure
    partial_pred_storage = {}
    for k in range(len(partial_preds)):
        partial_pred_storage[k] = partial_preds[k]

    return partial_pred_storage</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index </h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms" href="index.html">imodels.tree.rf_plus.feature_importance.ppms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM">MDIPlusGenericClassifierPPM</a></code></h4>
<ul class="">
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_full" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_full">predict_full</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial">predict_partial</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_k" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_k">predict_partial_k</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_k_subtract_intercept" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_k_subtract_intercept">predict_partial_k_subtract_intercept</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_subtract_intercept" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericClassifierPPM.predict_partial_subtract_intercept">predict_partial_subtract_intercept</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM">MDIPlusGenericRegressorPPM</a></code></h4>
<ul class="">
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_full" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_full">predict_full</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial">predict_partial</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_k" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_k">predict_partial_k</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_k_subtract_intercept" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_k_subtract_intercept">predict_partial_k_subtract_intercept</a></code></li>
<li><code><a title="imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_subtract_intercept" href="#imodels.tree.rf_plus.feature_importance.ppms.ppms.MDIPlusGenericRegressorPPM.predict_partial_subtract_intercept">predict_partial_subtract_intercept</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img align="center" width=100% src="https://csinva.io/imodels/img/anim.gif"> </img></p>
<!-- add wave animation -->
</nav>
</main>
<footer id="footer">
</footer>
</body>
</html>
<!-- add github corner -->
<a href="https://github.com/csinva/imodels" class="github-corner" aria-label="View source on GitHub"><svg width="120" height="120" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="m128.3,109.0 c113.8,99.7 119.0,89.6 119.0,89.6 c122.0,82.7 120.5,78.6 120.5,78.6 c119.2,72.0 123.4,76.3 123.4,76.3 c127.3,80.9 125.5,87.3 125.5,87.3 c122.9,97.6 130.6,101.9 134.4,103.2" fill="currentcolor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<!-- add wave animation stylesheet -->
<link rel="stylesheet" href="github.css">
