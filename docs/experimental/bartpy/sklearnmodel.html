<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import copy
from copy import deepcopy
from typing import List, Callable, Mapping, Union, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats
from joblib import Parallel, delayed
from sklearn.base import RegressorMixin, BaseEstimator
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets, model_selection

from .data import Data
from .initializers.initializer import Initializer
from .model import Model
from .node import LeafNode, DecisionNode
from .samplers.leafnode import LeafNodeSampler
from .samplers.modelsampler import ModelSampler, Chain
from .samplers.schedule import SampleSchedule
from .samplers.sigma import SigmaSampler
from .samplers.treemutation import TreeMutationSampler
from .samplers.unconstrainedtree.treemutation import get_tree_sampler
from .sigma import Sigma


def run_chain(model: &#39;SklearnModel&#39;, X: np.ndarray, y: np.ndarray):
    &#34;&#34;&#34;
    Run a single chain for a model
    Primarily used as a building block for constructing a parallel run of multiple chains
    &#34;&#34;&#34;

    # TODO: support for classification F^{-1} (y) ~ N(G(x), 1)

    # if model.classification:
    #     z = np.random.normal(loc=model.predict(X))
    #     z[y == 1] = np.maximum(z[y == 1], 0)
    #     z[y == 0] = np.minimum(z[y == 0], 0)
    #     y = z

    model.model = model._construct_model(X, y)

    return model.sampler.samples(model.model,
                                 model.n_samples,
                                 model.n_burn,
                                 model.thin,
                                 model.store_in_sample_predictions,
                                 model.store_acceptance_trace)


def delayed_run_chain():
    return run_chain


def get_nodes(root_node):
    decision_nodes = []
    leaf_nodes = []

    def _add_nodes(n):
        if type(n) == LeafNode:
            leaf_nodes.append(n)
        elif type(n) == DecisionNode:
            decision_nodes.append(n)
        if n.left_child:
            _add_nodes(n.left_child)
        if n.right_child:
            _add_nodes(n.right_child)

    _add_nodes(root_node)
    return decision_nodes, leaf_nodes


def get_root_node(tree):
    for n in tree.decision_nodes:
        if n.depth == 0:
            return n


def shrink_tree(tree, reg_param):
    root = get_root_node(tree)
    tree_d_node = shrink_node(root, reg_param)
    d, l = get_nodes(tree_d_node)
    tree._nodes = d + l
    return tree


def expand_node(node):
    mask_int = 1 - node.data.mask.astype(int)
    # y = node.data.y.values
    val = np.sum(node.data.y.values
                 * mask_int) / np.sum(mask_int)
    node.set_value(val)
    return node


def expand_tree(tree):
    decision, leaves = get_nodes(get_root_node(tree))
    leaves_new = [expand_node(n) for n in leaves]
    tree._nodes = decision + leaves_new
    return tree


def shrink_node(node, reg_param, parent_val, parent_num, cum_sum, scheme, constant):
    &#34;&#34;&#34;Shrink the tree
    &#34;&#34;&#34;

    # node.set_value(node.mean_response)

    left = node.left_child
    right = node.right_child
    is_leaf = type(node) == LeafNode
    # if self.prediction_task == &#39;regression&#39;:
    val = node.current_value
    is_root = parent_val is None and parent_num is None
    n_samples = node.n_obs if (scheme != &#34;leaf_based&#34; or is_root) else parent_num

    if is_root:
        val_new = val

    else:
        reg_term = reg_param if scheme == &#34;constant&#34; else reg_param / parent_num

        val_new = (val - parent_val) / (1 + reg_term)

    cum_sum += val_new

    if is_leaf:
        if scheme == &#34;leaf_based&#34;:
            v = constant + (val - constant) / (1 + reg_param / node.n_obs)
            node.set_value(v)
        else:
            node.set_value(cum_sum)

    else:
        shrink_node(left, reg_param, val, parent_num=n_samples, cum_sum=cum_sum, scheme=scheme, constant=constant)
        shrink_node(right, reg_param, val, parent_num=n_samples, cum_sum=cum_sum, scheme=scheme, constant=constant)

    return node


class SklearnModel(BaseEstimator, RegressorMixin):
    &#34;&#34;&#34;
    The main access point to building BART models in BartPy

    Parameters
    ----------
    n_trees: int
        the number of trees to use, more trees will make a smoother fit, but slow training and fitting
    n_chains: int
        the number of independent chains to run
        more chains will improve the quality of the samples, but will require more computation
    sigma_a: float
        shape parameter of the prior on sigma
    sigma_b: float
        scale parameter of the prior on sigma
    n_samples: int
        how many recorded samples to take
    n_burn: int
        how many samples to run without recording to reach convergence
    thin: float
        percentage of samples to store.
        use this to save memory when running large models
    p_grow: float
        probability of choosing a grow mutation in tree mutation sampling
    p_prune: float
        probability of choosing a prune mutation in tree mutation sampling
    alpha: float
        prior parameter on tree structure
    beta: float
        prior parameter on tree structure
    store_in_sample_predictions: bool
        whether to store full prediction samples
        set to False if you don&#39;t need in sample results - saves a lot of memory
    store_acceptance_trace: bool
        whether to store acceptance rates of the gibbs samples
        unless you&#39;re very memory constrained, you wouldn&#39;t want to set this to false
        useful for diagnostics
    tree_sampler: TreeMutationSampler
        Method of sampling used on trees
        defaults to `bartpy.samplers.unconstrainedtree`
    initializer: Initializer
        Class that handles the initialization of tree structure and leaf values
    n_jobs: int
        how many cores to use when computing MCMC samples
        set to `-1` to use all cores
    &#34;&#34;&#34;

    def __init__(self,
                 n_trees: int = 200,
                 n_chains: int = 4,
                 sigma_a: float = 0.001,
                 sigma_b: float = 0.001,
                 n_samples: int = 200,
                 n_burn: int = 200,
                 thin: float = 0.1,
                 alpha: float = 0.95,
                 beta: float = 2.,
                 store_in_sample_predictions: bool = False,
                 store_acceptance_trace: bool = False,
                 tree_sampler: TreeMutationSampler = get_tree_sampler(0.5, 0.5),
                 initializer: Optional[Initializer] = None,
                 n_jobs=-1,
                 classification: bool = False,
                 max_rules=None):
        self.n_trees = n_trees
        self.n_chains = n_chains
        self.sigma_a = sigma_a
        self.sigma_b = sigma_b
        self.n_burn = n_burn
        self.n_samples = n_samples
        self.p_grow = 0.5
        self.p_prune = 0.5
        self.alpha = alpha
        self.beta = beta
        self.thin = thin
        self.n_jobs = n_jobs
        self.store_in_sample_predictions = store_in_sample_predictions
        self.store_acceptance_trace = store_acceptance_trace
        self.columns = None
        self.tree_sampler = tree_sampler
        self.initializer = initializer
        self.schedule = SampleSchedule(self.tree_sampler, LeafNodeSampler(), SigmaSampler())
        self.sampler = ModelSampler(self.schedule)
        self.classification = classification
        self.max_rules = max_rules

        self.sigma, self.data, self.model, self._prediction_samples, self._model_samples, self.extract = [None] * 6

    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: np.ndarray) -&gt; &#39;SklearnModel&#39;:
        &#34;&#34;&#34;
        Learn the model based on training data

        Parameters
        ----------
        X: pd.DataFrame
            training covariates
        y: np.ndarray
            training targets

        Returns
        -------
        SklearnModel
            self with trained parameter values
        &#34;&#34;&#34;
        self.model = self._construct_model(X, y)
        self.extract = Parallel(n_jobs=self.n_jobs)(self.f_delayed_chains(X, y))
        self.combined_chains = self._combine_chains(self.extract)
        self._model_samples, self._prediction_samples = self.combined_chains[&#34;model&#34;], self.combined_chains[
            &#34;in_sample_predictions&#34;]
        self._acceptance_trace = self.combined_chains[&#34;acceptance&#34;]
        self._likelihood = self.combined_chains[&#34;likelihood&#34;]
        self._probs = self.combined_chains[&#34;probs&#34;]

        self.fitted_ = True
        return self

    @property
    def fitted(self):
        if hasattr(self, &#34;fitted_&#34;):
            return self.fitted_
        return False

    @property
    def complexity_(self):
        if hasattr(self.initializer, &#34;_tree&#34;):
            estimator = self.initializer._tree
            if hasattr(estimator, &#39;complexity_&#39;):
                return estimator.complexity_

    @staticmethod
    def _combine_chains(extract: List[Chain]) -&gt; Chain:
        keys = list(extract[0].keys())
        combined = {}
        for key in keys:
            combined[key] = np.concatenate([chain[key] for chain in extract], axis=0)
        return combined

    @staticmethod
    def _convert_covariates_to_data(X: np.ndarray, y: np.ndarray) -&gt; Data:
        from copy import deepcopy
        if type(X) == pd.DataFrame:
            X: pd.DataFrame = X
            X = X.values
        return Data(deepcopy(X), deepcopy(y), normalize=True)

    def _construct_model(self, X: np.ndarray, y: np.ndarray) -&gt; Model:
        if len(X) == 0 or X.shape[1] == 0:
            raise ValueError(&#34;Empty covariate matrix passed&#34;)
        self.data = self._convert_covariates_to_data(X, y)
        self.sigma = Sigma(self.sigma_a, self.sigma_b, self.data.y.normalizing_scale, self.classification)
        self.model = Model(self.data,
                           self.sigma,
                           n_trees=self.n_trees,
                           alpha=self.alpha,
                           beta=self.beta,
                           initializer=self.initializer,
                           classification=self.classification)
        n_trees = self.n_trees if self.initializer is None else self.initializer.n_trees
        self.n_trees = n_trees
        return self.model

    def f_delayed_chains(self, X: np.ndarray, y: np.ndarray):
        &#34;&#34;&#34;
        Access point for getting access to delayed methods for running chains
        Useful for when you want to run multiple instances of the model in parallel
        e.g. when calculating a null distribution for feature importance

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array

        Returns
        -------
        List[Callable[[], ChainExtract]]
        &#34;&#34;&#34;
        return [delayed(x)(self, X, y) for x in self.f_chains()]

    def f_chains(self) -&gt; List[Callable[[], Chain]]:
        &#34;&#34;&#34;
        List of methods to run MCMC chains
        Useful for running multiple models in parallel

        Returns
        -------
        List[Callable[[], Extract]]
            List of method to run individual chains
            Length of n_chains
        &#34;&#34;&#34;
        return [delayed_run_chain() for _ in range(self.n_chains)]

    def predict(self, X: np.ndarray = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Predict the target corresponding to the provided covariate matrix
        If X is None, will predict based on training covariates

        Prediction is based on the mean of all samples

        Parameters
        ----------
        X: pd.DataFrame
            covariates to predict from

        Returns
        -------
        np.ndarray
            predictions for the X covariates
        &#34;&#34;&#34;
        if X is None and self.store_in_sample_predictions:
            return self.data.y.unnormalize_y(np.mean(self._prediction_samples, axis=0))
        elif X is None and not self.store_in_sample_predictions:
            raise ValueError(
                &#34;In sample predictions only possible if model.store_in_sample_predictions is `True`.  Either set the parameter to True or pass a non-None X parameter&#34;)
        else:
            predictions = self._out_of_sample_predict(X)
            if self.classification:
                return np.round(predictions, 0)
            return predictions

    def predict_proba(self, X: np.ndarray = None) -&gt; np.ndarray:
        preds = self._out_of_sample_predict(X)
        return np.stack([preds, 1 - preds], axis=1)

    def residuals(self, X=None, y=None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Array of error for each observation

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array

        Returns
        -------
        np.ndarray
            Error for each observation
        &#34;&#34;&#34;
        if y is None:
            return self.model.data.y.unnormalized_y - self.predict(X)
        else:
            return y - self.predict(X)

    def l2_error(self, X=None, y=None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Calculate the squared errors for each row in the covariate matrix

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array
        Returns
        -------
        np.ndarray
            Squared error for each observation
        &#34;&#34;&#34;
        return np.square(self.residuals(X, y))

    def rmse(self, X, y) -&gt; float:
        &#34;&#34;&#34;
        The total RMSE error of the model
        The sum of squared errors over all observations

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array

        Returns
        -------
        float
            The total summed L2 error for the model
        &#34;&#34;&#34;
        return np.sqrt(np.sum(self.l2_error(X, y)))

    def _chain_pred_arr(self, X, chain_number):
        chain_len = int(self.n_samples)
        samples_chain = self._model_samples[chain_number * chain_len: (chain_number + 1) * chain_len]
        predictions_transformed = [x.predict(X) for x in samples_chain]
        return predictions_transformed

    def predict_chain(self, X, chain_number):
        predictions_transformed = self._chain_pred_arr(X, chain_number)
        predictions = self.data.y.unnormalize_y(np.mean(predictions_transformed, axis=0))
        if self.classification:
            predictions = scipy.stats.norm.cdf(predictions)
        return predictions

    def chain_mse_std(self, X, y, chain_number):
        predictions_transformed = self._chain_pred_arr(X, chain_number)
        predictions_std = np.std(
            [mean_squared_error(self.data.y.unnormalize_y(preds), y) for preds in predictions_transformed])
        return predictions_std

    def chain_predictions(self, X, chain_number):
        predictions_transformed = self._chain_pred_arr(X, chain_number)
        preds_arr = [self.data.y.unnormalize_y(preds) for preds in predictions_transformed]
        return preds_arr

    def between_chains_var(self, X):
        all_predictions = np.stack([self.data.y.unnormalize_y(x.predict(X)) for x in self._model_samples], axis=1)

        def _get_var(preds_arr):
            mean_pred = preds_arr.mean(axis=1)
            var = np.mean((preds_arr - np.expand_dims(mean_pred, 1)) ** 2)
            return var

        total_var = _get_var(all_predictions)
        within_chain_var = 0
        for c in range(self.n_chains):
            chain_preds = self._chain_pred_arr(X, c)
            within_chain_var += _get_var(np.stack(chain_preds, axis=1))
        return total_var - within_chain_var

    def _out_of_sample_predict(self, X):
        samples = self._model_samples
        predictions_transformed = [x.predict(X) for x in samples]
        predictions = self.data.y.unnormalize_y(np.mean(predictions_transformed, axis=0))
        if self.classification:
            predictions = scipy.stats.norm.cdf(predictions)
        return predictions

    def fit_predict(self, X, y):
        self.fit(X, y)
        if self.store_in_sample_predictions:
            return self.predict()
        else:
            return self.predict(X)

    @property
    def model_samples(self) -&gt; List[Model]:
        &#34;&#34;&#34;
        Array of the model as it was after each sample.
        Useful for examining for:

         - examining the state of trees, nodes and sigma throughout the sampling
         - out of sample prediction

        Returns None if the model hasn&#39;t been fit

        Returns
        -------
        List[Model]
        &#34;&#34;&#34;
        return self._model_samples

    @property
    def acceptance_trace(self) -&gt; List[Mapping[str, float]]:
        &#34;&#34;&#34;
        List of Mappings from variable name to acceptance rates

        Each entry is the acceptance rate of the variable in each iteration of the model

        Returns
        -------
        List[Mapping[str, float]]
        &#34;&#34;&#34;
        return self._acceptance_trace

    @property
    def likelihood(self) -&gt; List:
        &#34;&#34;&#34;
        List of Mappings from variable name to likelihood

        Each entry is the acceptance rate of the variable in each iteration of the model

        Returns
        -------
        List[Mapping[str, float]]
        &#34;&#34;&#34;
        return self._likelihood

    @property
    def probs(self) -&gt; List:
        &#34;&#34;&#34;
        List of Mappings from variable name to likelihood

        Each entry is the acceptance rate of the variable in each iteration of the model

        Returns
        -------
        List[Mapping[str, float]]
        &#34;&#34;&#34;
        return self._probs

    @property
    def prediction_samples(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Matrix of prediction samples at each point in sampling
        Useful for assessing convergence, calculating point estimates etc.

        Returns
        -------
        np.ndarray
            prediction samples with dimensionality n_samples * n_points
        &#34;&#34;&#34;
        return self.prediction_samples

    def from_extract(self, extract: List[Chain], X: np.ndarray, y: np.ndarray) -&gt; &#39;SklearnModel&#39;:
        &#34;&#34;&#34;
        Create a copy of the model using an extract
        Useful for doing operations on extracts created in external processes like feature selection
        Parameters
        ----------
        extract: Extract
            samples produced by delayed chain methods
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target variable

        Returns
        -------
        SklearnModel
            Copy of the current model with samples
        &#34;&#34;&#34;
        new_model = deepcopy(self)
        combined_chain = self._combine_chains(extract)
        self._model_samples, self._prediction_samples = combined_chain[&#34;model&#34;], combined_chain[&#34;in_sample_predictions&#34;]
        self._acceptance_trace = combined_chain[&#34;acceptance&#34;]
        new_model.data = self._convert_covariates_to_data(X, y)
        return new_model


class BART(SklearnModel):

    @staticmethod
    def _get_n_nodes(trees):
        nodes = 0
        for tree in trees:
            nodes += len(tree.decision_nodes)
        return nodes

    @property
    def sample_complexity(self):
        # samples = self._model_samples
        # trees = [s.trees for s in samples]
        complexities = [self._get_n_nodes(t) for t in self.trees]
        return np.sum(complexities)

    @staticmethod
    def sub_forest(trees, n_nodes):
        nodes = 0
        for i, tree in enumerate(trees):
            nodes += len(tree.decision_nodes)
            if nodes &gt;= n_nodes:
                return trees[0:i + 1]

    @property
    def trees(self):
        trs = [s.trees for s in self._model_samples]
        return trs

    def update_complexity(self, i):
        samples_complexity = [self._get_n_nodes(t) for t in self.trees]

        # complexity_sum = 0
        arg_sort_complexity = np.argsort(samples_complexity)
        self._model_samples = self._model_samples[arg_sort_complexity[:i + 1]]

        return self


class ImputedBART(BaseEstimator):
    def __init__(self, estimator_):
        # super(ShrunkBARTRegressor, self).__init__()
        self.estimator_ = estimator_

    def predict(self, *args, **kwargs):
        return self.estimator_.predict(*args, **kwargs)

    def predict_proba(self, *args, **kwargs):
        if hasattr(self.estimator_, &#39;predict_proba&#39;):
            return self.estimator_.predict_proba(*args, **kwargs)
        else:
            return NotImplemented

    def score(self, *args, **kwargs):
        if hasattr(self.estimator_, &#39;score&#39;):
            return self.estimator_.score(*args, **kwargs)
        else:
            return NotImplemented


class ShrunkBART(ImputedBART):

    def __init__(self, estimator_, reg_param, scheme):
        super(ShrunkBART, self).__init__(estimator_)
        self.reg_param = reg_param
        self.scheme = scheme

    def shrink_tree(self, tree):
        root = get_root_node(tree)
        tree_d_node = shrink_node(root, self.reg_param, parent_val=None, parent_num=None, cum_sum=0, scheme=self.scheme,
                                  constant=np.mean(self.estimator_.data.y.values))
        d, l = get_nodes(tree_d_node)
        tree._nodes = d + l
        return tree

    def fit(self, *args, **kwargs):
        if not self.estimator_.fitted:
            self.estimator_.fit(*args, **kwargs)
        samples = []
        for s in self.estimator_.model_samples:
            for i, tree in enumerate(s._trees):
                s_tree = self.shrink_tree(expand_tree(copy.deepcopy(tree)))
                s._trees[i] = s_tree
            samples.append(s)
        self.estimator_._model_samples = samples
        self.fitted_ = True


class ExpandedBART(ImputedBART):

    def fit(self, *args, **kwargs):
        if not self.estimator_.fitted:
            self.estimator_.fit(*args, **kwargs)
        samples = []
        for s in self.estimator_.model_samples:
            for i, tree in enumerate(s._trees):
                s_tree = expand_tree(copy.deepcopy(tree))
                s._trees[i] = s_tree
            samples.append(s)
        self.estimator_._model_samples = samples
        self.fitted_ = True


# class ExpandedBARTRegressor(ImputedBARTRegressor):
#
#     def fit(self, *args, **kwargs):
#         if not self.estimator_.fitted:
#             self.estimator_.fit(*args, **kwargs)
#         samples = []
#         for s in self.estimator_.model_samples:
#             for i, tree in enumerate(s._trees):
#                 s_tree = expend_tree(copy.deepcopy(tree), args[1])
#                 s._trees[i] = s_tree
#             samples.append(s)
#         self.estimator_._model_samples = samples
#         self.fitted_ = True


class ShrunkBARTCV(ShrunkBART):
    def __init__(self, estimator_: BaseEstimator, scheme: str,
                 reg_param_list: List[float] = [0.1, 1, 10, 50, 100, 500],
                 cv: int = 3, scoring=None):
        super(ShrunkBARTCV, self).__init__(estimator_, None, scheme)
        self.reg_param_list = np.array(reg_param_list)
        self.cv = cv
        self.scoring = scoring

    def fit(self, X, y, *args, **kwargs):
        self.scores_ = []
        for reg_param in self.reg_param_list:
            est = ShrunkBART(deepcopy(self.estimator_), reg_param, self.scheme)
            cv_scores = cross_val_score(est, X, y, cv=self.cv, scoring=self.scoring)
            self.scores_.append(np.mean(cv_scores))
        self.reg_param = self.reg_param_list[np.argmax(self.scores_)]
        super().fit(X=X, y=y)


def main():
    # iris = datasets.load_iris()
    # idx = np.logical_or(iris.target == 0, iris.target == 1)
    # X, y = iris.data[idx, ...], iris.target[idx]
    X, y = datasets.load_diabetes(return_X_y=True)

    X_train, X_test, y_train, y_test = model_selection.train_test_split(
        X, y, test_size=0.3, random_state=1)
    bart = BART(classification=False)
    bart.fit(X_train, y_train)
    preds_org = bart.predict(X_test)
    mse = np.linalg.norm(preds_org - y_test)
    print(mse)
    # tree = DecisionTreeClassifier()
    # tree.fit(X, y)
    # preds_tree = tree.predict_proba(X)
    # bart_s = ShrunkBARTCV(copy.deepcopy(bart), scheme=&#34;node_based&#34;)
    # bart_s.fit(X, y)
    #
    # # bart_s_c = ShrunkBART(copy.deepcopy(bart), reg_param=2, scheme=&#34;constant&#34;)
    # # bart_s_c.fit(X, y)
    # #
    # # bart_s_l = ShrunkBART(copy.deepcopy(bart), reg_param=2, scheme=&#34;leaf_based&#34;)
    # # bart_s_l.fit(X, y)
    # # bart_s_cv = ShrunkBARTRegressorCV(estimator_=copy.deepcopy(bart))
    # # bart_s_cv.fit(X, y)
    # e_bart = ExpandedBART(estimator_=copy.deepcopy(bart))
    # e_bart.fit(X, y)
    #
    # preds = bart_s.predict(X)
    #
    # # preds_c = bart_s_c.predict(X)
    # # preds_l = bart_s_l.predict(X)
    # # # preds_cv = bart_s_cv.predict(X)
    # preds_bart_e = e_bart.predict(X)
    # fig, ax = plt.subplots(1)
    #
    # ax.scatter(np.arange(len(y)), preds_org, c=&#34;orange&#34;, label=&#34;bart&#34;)
    # ax.scatter(np.arange(len(y)), preds, c=&#34;purple&#34;, alpha=0.3, label=&#34;shrunk node&#34;)
    # # ax.scatter(np.arange(len(y)), preds_c, c=&#34;blue&#34;, alpha=0.3, label=&#34;shrunk constant&#34;)
    # # ax.scatter(np.arange(len(y)), preds_l, c=&#34;red&#34;, alpha=0.3, label=&#34;shrunk leaf&#34;)
    # ax.scatter(np.arange(len(y)), preds_bart_e, c=&#34;green&#34;, alpha=0.3, label=&#34;average&#34;)
    # # preds_all = [preds_org, preds_c, preds_l, preds_bart_e, preds]
    # # shift = 0.5
    # # rng = (np.min([np.min(p) for p in preds_all]) - shift, np.max([np.max(p) for p in preds_all]) + shift)
    # # n_bins = 200
    # # alpha = 0.8
    # # ax.hist(preds_org, color=&#34;orange&#34;, alpha=alpha, label=&#34;bart&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds, color=&#34;purple&#34;, alpha=alpha, label=&#34;shrunk node&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds_c, color=&#34;blue&#34;, alpha=alpha, label=&#34;shrunk constant&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds_l, color=&#34;red&#34;, alpha=alpha, label=&#34;shrunk leaf&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds_bart_e, color=&#34;green&#34;, alpha=alpha, label=&#34;average&#34;, bins=n_bins, range=rng)
    # #
    # # ax.set_xlabel(&#34;Predicted Value&#34;)
    # # ax.set_ylabel(&#34;Count&#34;)
    #
    # plt.title(np.mean(y))
    #
    # plt.legend(loc=&#34;upper left&#34;)
    # plt.savefig(&#34;bart_shrink.png&#34;)
    # # plt.show()
    # #
    # # plt.close()
    #


if __name__ == &#39;__main__&#39;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.delayed_run_chain"><code class="name flex">
<span>def <span class="ident">delayed_run_chain</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delayed_run_chain():
    return run_chain</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.expand_node"><code class="name flex">
<span>def <span class="ident">expand_node</span></span>(<span>node)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_node(node):
    mask_int = 1 - node.data.mask.astype(int)
    # y = node.data.y.values
    val = np.sum(node.data.y.values
                 * mask_int) / np.sum(mask_int)
    node.set_value(val)
    return node</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.expand_tree"><code class="name flex">
<span>def <span class="ident">expand_tree</span></span>(<span>tree)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_tree(tree):
    decision, leaves = get_nodes(get_root_node(tree))
    leaves_new = [expand_node(n) for n in leaves]
    tree._nodes = decision + leaves_new
    return tree</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.get_nodes"><code class="name flex">
<span>def <span class="ident">get_nodes</span></span>(<span>root_node)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_nodes(root_node):
    decision_nodes = []
    leaf_nodes = []

    def _add_nodes(n):
        if type(n) == LeafNode:
            leaf_nodes.append(n)
        elif type(n) == DecisionNode:
            decision_nodes.append(n)
        if n.left_child:
            _add_nodes(n.left_child)
        if n.right_child:
            _add_nodes(n.right_child)

    _add_nodes(root_node)
    return decision_nodes, leaf_nodes</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.get_root_node"><code class="name flex">
<span>def <span class="ident">get_root_node</span></span>(<span>tree)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_root_node(tree):
    for n in tree.decision_nodes:
        if n.depth == 0:
            return n</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    # iris = datasets.load_iris()
    # idx = np.logical_or(iris.target == 0, iris.target == 1)
    # X, y = iris.data[idx, ...], iris.target[idx]
    X, y = datasets.load_diabetes(return_X_y=True)

    X_train, X_test, y_train, y_test = model_selection.train_test_split(
        X, y, test_size=0.3, random_state=1)
    bart = BART(classification=False)
    bart.fit(X_train, y_train)
    preds_org = bart.predict(X_test)
    mse = np.linalg.norm(preds_org - y_test)
    print(mse)
    # tree = DecisionTreeClassifier()
    # tree.fit(X, y)
    # preds_tree = tree.predict_proba(X)
    # bart_s = ShrunkBARTCV(copy.deepcopy(bart), scheme=&#34;node_based&#34;)
    # bart_s.fit(X, y)
    #
    # # bart_s_c = ShrunkBART(copy.deepcopy(bart), reg_param=2, scheme=&#34;constant&#34;)
    # # bart_s_c.fit(X, y)
    # #
    # # bart_s_l = ShrunkBART(copy.deepcopy(bart), reg_param=2, scheme=&#34;leaf_based&#34;)
    # # bart_s_l.fit(X, y)
    # # bart_s_cv = ShrunkBARTRegressorCV(estimator_=copy.deepcopy(bart))
    # # bart_s_cv.fit(X, y)
    # e_bart = ExpandedBART(estimator_=copy.deepcopy(bart))
    # e_bart.fit(X, y)
    #
    # preds = bart_s.predict(X)
    #
    # # preds_c = bart_s_c.predict(X)
    # # preds_l = bart_s_l.predict(X)
    # # # preds_cv = bart_s_cv.predict(X)
    # preds_bart_e = e_bart.predict(X)
    # fig, ax = plt.subplots(1)
    #
    # ax.scatter(np.arange(len(y)), preds_org, c=&#34;orange&#34;, label=&#34;bart&#34;)
    # ax.scatter(np.arange(len(y)), preds, c=&#34;purple&#34;, alpha=0.3, label=&#34;shrunk node&#34;)
    # # ax.scatter(np.arange(len(y)), preds_c, c=&#34;blue&#34;, alpha=0.3, label=&#34;shrunk constant&#34;)
    # # ax.scatter(np.arange(len(y)), preds_l, c=&#34;red&#34;, alpha=0.3, label=&#34;shrunk leaf&#34;)
    # ax.scatter(np.arange(len(y)), preds_bart_e, c=&#34;green&#34;, alpha=0.3, label=&#34;average&#34;)
    # # preds_all = [preds_org, preds_c, preds_l, preds_bart_e, preds]
    # # shift = 0.5
    # # rng = (np.min([np.min(p) for p in preds_all]) - shift, np.max([np.max(p) for p in preds_all]) + shift)
    # # n_bins = 200
    # # alpha = 0.8
    # # ax.hist(preds_org, color=&#34;orange&#34;, alpha=alpha, label=&#34;bart&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds, color=&#34;purple&#34;, alpha=alpha, label=&#34;shrunk node&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds_c, color=&#34;blue&#34;, alpha=alpha, label=&#34;shrunk constant&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds_l, color=&#34;red&#34;, alpha=alpha, label=&#34;shrunk leaf&#34;, bins=n_bins, range=rng)
    # # ax.hist(preds_bart_e, color=&#34;green&#34;, alpha=alpha, label=&#34;average&#34;, bins=n_bins, range=rng)
    # #
    # # ax.set_xlabel(&#34;Predicted Value&#34;)
    # # ax.set_ylabel(&#34;Count&#34;)
    #
    # plt.title(np.mean(y))
    #
    # plt.legend(loc=&#34;upper left&#34;)
    # plt.savefig(&#34;bart_shrink.png&#34;)
    # # plt.show()
    # #
    # # plt.close()
    #</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.run_chain"><code class="name flex">
<span>def <span class="ident">run_chain</span></span>(<span>model: <a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a>, X: numpy.ndarray, y: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a single chain for a model
Primarily used as a building block for constructing a parallel run of multiple chains</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_chain(model: &#39;SklearnModel&#39;, X: np.ndarray, y: np.ndarray):
    &#34;&#34;&#34;
    Run a single chain for a model
    Primarily used as a building block for constructing a parallel run of multiple chains
    &#34;&#34;&#34;

    # TODO: support for classification F^{-1} (y) ~ N(G(x), 1)

    # if model.classification:
    #     z = np.random.normal(loc=model.predict(X))
    #     z[y == 1] = np.maximum(z[y == 1], 0)
    #     z[y == 0] = np.minimum(z[y == 0], 0)
    #     y = z

    model.model = model._construct_model(X, y)

    return model.sampler.samples(model.model,
                                 model.n_samples,
                                 model.n_burn,
                                 model.thin,
                                 model.store_in_sample_predictions,
                                 model.store_acceptance_trace)</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.shrink_node"><code class="name flex">
<span>def <span class="ident">shrink_node</span></span>(<span>node, reg_param, parent_val, parent_num, cum_sum, scheme, constant)</span>
</code></dt>
<dd>
<div class="desc"><p>Shrink the tree</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shrink_node(node, reg_param, parent_val, parent_num, cum_sum, scheme, constant):
    &#34;&#34;&#34;Shrink the tree
    &#34;&#34;&#34;

    # node.set_value(node.mean_response)

    left = node.left_child
    right = node.right_child
    is_leaf = type(node) == LeafNode
    # if self.prediction_task == &#39;regression&#39;:
    val = node.current_value
    is_root = parent_val is None and parent_num is None
    n_samples = node.n_obs if (scheme != &#34;leaf_based&#34; or is_root) else parent_num

    if is_root:
        val_new = val

    else:
        reg_term = reg_param if scheme == &#34;constant&#34; else reg_param / parent_num

        val_new = (val - parent_val) / (1 + reg_term)

    cum_sum += val_new

    if is_leaf:
        if scheme == &#34;leaf_based&#34;:
            v = constant + (val - constant) / (1 + reg_param / node.n_obs)
            node.set_value(v)
        else:
            node.set_value(cum_sum)

    else:
        shrink_node(left, reg_param, val, parent_num=n_samples, cum_sum=cum_sum, scheme=scheme, constant=constant)
        shrink_node(right, reg_param, val, parent_num=n_samples, cum_sum=cum_sum, scheme=scheme, constant=constant)

    return node</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.shrink_tree"><code class="name flex">
<span>def <span class="ident">shrink_tree</span></span>(<span>tree, reg_param)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shrink_tree(tree, reg_param):
    root = get_root_node(tree)
    tree_d_node = shrink_node(root, reg_param)
    d, l = get_nodes(tree_d_node)
    tree._nodes = d + l
    return tree</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.BART"><code class="flex name class">
<span>class <span class="ident">BART</span></span>
<span>(</span><span>n_trees: int = 200, n_chains: int = 4, sigma_a: float = 0.001, sigma_b: float = 0.001, n_samples: int = 200, n_burn: int = 200, thin: float = 0.1, alpha: float = 0.95, beta: float = 2.0, store_in_sample_predictions: bool = False, store_acceptance_trace: bool = False, tree_sampler: <a title="imodels.experimental.bartpy.samplers.treemutation.TreeMutationSampler" href="samplers/treemutation.html#imodels.experimental.bartpy.samplers.treemutation.TreeMutationSampler">TreeMutationSampler</a> = &lt;imodels.experimental.bartpy.samplers.unconstrainedtree.treemutation.UnconstrainedTreeMutationSampler object&gt;, initializer: Optional[<a title="imodels.experimental.bartpy.initializers.initializer.Initializer" href="initializers/initializer.html#imodels.experimental.bartpy.initializers.initializer.Initializer">Initializer</a>] = None, n_jobs=-1, classification: bool = False, max_rules=None)</span>
</code></dt>
<dd>
<div class="desc"><p>The main access point to building BART models in BartPy</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_trees</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of trees to use, more trees will make a smoother fit, but slow training and fitting</dd>
<dt><strong><code>n_chains</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of independent chains to run
more chains will improve the quality of the samples, but will require more computation</dd>
<dt><strong><code>sigma_a</code></strong> :&ensp;<code>float</code></dt>
<dd>shape parameter of the prior on sigma</dd>
<dt><strong><code>sigma_b</code></strong> :&ensp;<code>float</code></dt>
<dd>scale parameter of the prior on sigma</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>how many recorded samples to take</dd>
<dt><strong><code>n_burn</code></strong> :&ensp;<code>int</code></dt>
<dd>how many samples to run without recording to reach convergence</dd>
<dt><strong><code>thin</code></strong> :&ensp;<code>float</code></dt>
<dd>percentage of samples to store.
use this to save memory when running large models</dd>
<dt><strong><code>p_grow</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of choosing a grow mutation in tree mutation sampling</dd>
<dt><strong><code>p_prune</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of choosing a prune mutation in tree mutation sampling</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>prior parameter on tree structure</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>float</code></dt>
<dd>prior parameter on tree structure</dd>
<dt><strong><code>store_in_sample_predictions</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to store full prediction samples
set to False if you don't need in sample results - saves a lot of memory</dd>
<dt><strong><code>store_acceptance_trace</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to store acceptance rates of the gibbs samples
unless you're very memory constrained, you wouldn't want to set this to false
useful for diagnostics</dd>
<dt><strong><code>tree_sampler</code></strong> :&ensp;<code>TreeMutationSampler</code></dt>
<dd>Method of sampling used on trees
defaults to <code>bartpy.samplers.unconstrainedtree</code></dd>
<dt><strong><code>initializer</code></strong> :&ensp;<code>Initializer</code></dt>
<dd>Class that handles the initialization of tree structure and leaf values</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>how many cores to use when computing MCMC samples
set to <code>-1</code> to use all cores</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BART(SklearnModel):

    @staticmethod
    def _get_n_nodes(trees):
        nodes = 0
        for tree in trees:
            nodes += len(tree.decision_nodes)
        return nodes

    @property
    def sample_complexity(self):
        # samples = self._model_samples
        # trees = [s.trees for s in samples]
        complexities = [self._get_n_nodes(t) for t in self.trees]
        return np.sum(complexities)

    @staticmethod
    def sub_forest(trees, n_nodes):
        nodes = 0
        for i, tree in enumerate(trees):
            nodes += len(tree.decision_nodes)
            if nodes &gt;= n_nodes:
                return trees[0:i + 1]

    @property
    def trees(self):
        trs = [s.trees for s in self._model_samples]
        return trs

    def update_complexity(self, i):
        samples_complexity = [self._get_n_nodes(t) for t in self.trees]

        # complexity_sum = 0
        arg_sort_complexity = np.argsort(samples_complexity)
        self._model_samples = self._model_samples[arg_sort_complexity[:i + 1]]

        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
<li>sklearn.base.RegressorMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.BART.sub_forest"><code class="name flex">
<span>def <span class="ident">sub_forest</span></span>(<span>trees, n_nodes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def sub_forest(trees, n_nodes):
    nodes = 0
    for i, tree in enumerate(trees):
        nodes += len(tree.decision_nodes)
        if nodes &gt;= n_nodes:
            return trees[0:i + 1]</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.BART.sample_complexity"><code class="name">var <span class="ident">sample_complexity</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_complexity(self):
    # samples = self._model_samples
    # trees = [s.trees for s in samples]
    complexities = [self._get_n_nodes(t) for t in self.trees]
    return np.sum(complexities)</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.BART.trees"><code class="name">var <span class="ident">trees</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def trees(self):
    trs = [s.trees for s in self._model_samples]
    return trs</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.BART.update_complexity"><code class="name flex">
<span>def <span class="ident">update_complexity</span></span>(<span>self, i)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_complexity(self, i):
    samples_complexity = [self._get_n_nodes(t) for t in self.trees]

    # complexity_sum = 0
    arg_sort_complexity = np.argsort(samples_complexity)
    self._model_samples = self._model_samples[arg_sort_complexity[:i + 1]]

    return self</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></b></code>:
<ul class="hlist">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.acceptance_trace" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.acceptance_trace">acceptance_trace</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_chains" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_chains">f_chains</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_delayed_chains" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_delayed_chains">f_delayed_chains</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit">fit</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.from_extract" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.from_extract">from_extract</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.l2_error" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.l2_error">l2_error</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.likelihood" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.likelihood">likelihood</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.model_samples" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.model_samples">model_samples</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict">predict</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.prediction_samples" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.prediction_samples">prediction_samples</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.probs" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.probs">probs</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.residuals" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.residuals">residuals</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.rmse" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.rmse">rmse</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.set_score_request" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.set_score_request">set_score_request</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.ExpandedBART"><code class="flex name class">
<span>class <span class="ident">ExpandedBART</span></span>
<span>(</span><span>estimator_)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<p>Inheriting from this class provides default implementations of:</p>
<ul>
<li>setting and getting parameters used by <code>GridSearchCV</code> and friends;</li>
<li>textual and HTML representation displayed in terminals and IDEs;</li>
<li>estimator serialization;</li>
<li>parameters validation;</li>
<li>data validation;</li>
<li>feature names validation.</li>
</ul>
<p>Read more in the :ref:<code>User Guide &lt;rolling_your_own_estimator&gt;</code>.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.base import BaseEstimator
&gt;&gt;&gt; class MyEstimator(BaseEstimator):
...     def __init__(self, *, param=1):
...         self.param = param
...     def fit(self, X, y=None):
...         self.is_fitted_ = True
...         return self
...     def predict(self, X):
...         return np.full(shape=X.shape[0], fill_value=self.param)
&gt;&gt;&gt; estimator = MyEstimator(param=2)
&gt;&gt;&gt; estimator.get_params()
{'param': 2}
&gt;&gt;&gt; X = np.array([[1, 2], [2, 3], [3, 4]])
&gt;&gt;&gt; y = np.array([1, 0, 1])
&gt;&gt;&gt; estimator.fit(X, y).predict(X)
array([2, 2, 2])
&gt;&gt;&gt; estimator.set_params(param=3).fit(X, y).predict(X)
array([3, 3, 3])
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExpandedBART(ImputedBART):

    def fit(self, *args, **kwargs):
        if not self.estimator_.fitted:
            self.estimator_.fit(*args, **kwargs)
        samples = []
        for s in self.estimator_.model_samples:
            for i, tree in enumerate(s._trees):
                s_tree = expand_tree(copy.deepcopy(tree))
                s._trees[i] = s_tree
            samples.append(s)
        self.estimator_._model_samples = samples
        self.fitted_ = True</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodels.experimental.bartpy.sklearnmodel.ImputedBART" href="#imodels.experimental.bartpy.sklearnmodel.ImputedBART">ImputedBART</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.ExpandedBART.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, *args, **kwargs):
    if not self.estimator_.fitted:
        self.estimator_.fit(*args, **kwargs)
    samples = []
    for s in self.estimator_.model_samples:
        for i, tree in enumerate(s._trees):
            s_tree = expand_tree(copy.deepcopy(tree))
            s._trees[i] = s_tree
        samples.append(s)
    self.estimator_._model_samples = samples
    self.fitted_ = True</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.ImputedBART"><code class="flex name class">
<span>class <span class="ident">ImputedBART</span></span>
<span>(</span><span>estimator_)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<p>Inheriting from this class provides default implementations of:</p>
<ul>
<li>setting and getting parameters used by <code>GridSearchCV</code> and friends;</li>
<li>textual and HTML representation displayed in terminals and IDEs;</li>
<li>estimator serialization;</li>
<li>parameters validation;</li>
<li>data validation;</li>
<li>feature names validation.</li>
</ul>
<p>Read more in the :ref:<code>User Guide &lt;rolling_your_own_estimator&gt;</code>.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.base import BaseEstimator
&gt;&gt;&gt; class MyEstimator(BaseEstimator):
...     def __init__(self, *, param=1):
...         self.param = param
...     def fit(self, X, y=None):
...         self.is_fitted_ = True
...         return self
...     def predict(self, X):
...         return np.full(shape=X.shape[0], fill_value=self.param)
&gt;&gt;&gt; estimator = MyEstimator(param=2)
&gt;&gt;&gt; estimator.get_params()
{'param': 2}
&gt;&gt;&gt; X = np.array([[1, 2], [2, 3], [3, 4]])
&gt;&gt;&gt; y = np.array([1, 0, 1])
&gt;&gt;&gt; estimator.fit(X, y).predict(X)
array([2, 2, 2])
&gt;&gt;&gt; estimator.set_params(param=3).fit(X, y).predict(X)
array([3, 3, 3])
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ImputedBART(BaseEstimator):
    def __init__(self, estimator_):
        # super(ShrunkBARTRegressor, self).__init__()
        self.estimator_ = estimator_

    def predict(self, *args, **kwargs):
        return self.estimator_.predict(*args, **kwargs)

    def predict_proba(self, *args, **kwargs):
        if hasattr(self.estimator_, &#39;predict_proba&#39;):
            return self.estimator_.predict_proba(*args, **kwargs)
        else:
            return NotImplemented

    def score(self, *args, **kwargs):
        if hasattr(self.estimator_, &#39;score&#39;):
            return self.estimator_.score(*args, **kwargs)
        else:
            return NotImplemented</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodels.experimental.bartpy.sklearnmodel.ExpandedBART" href="#imodels.experimental.bartpy.sklearnmodel.ExpandedBART">ExpandedBART</a></li>
<li><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBART" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBART">ShrunkBART</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.ImputedBART.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, *args, **kwargs):
    return self.estimator_.predict(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.ImputedBART.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, *args, **kwargs):
    if hasattr(self.estimator_, &#39;predict_proba&#39;):
        return self.estimator_.predict_proba(*args, **kwargs)
    else:
        return NotImplemented</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.ImputedBART.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(self, *args, **kwargs):
    if hasattr(self.estimator_, &#39;score&#39;):
        return self.estimator_.score(*args, **kwargs)
    else:
        return NotImplemented</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.ShrunkBART"><code class="flex name class">
<span>class <span class="ident">ShrunkBART</span></span>
<span>(</span><span>estimator_, reg_param, scheme)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<p>Inheriting from this class provides default implementations of:</p>
<ul>
<li>setting and getting parameters used by <code>GridSearchCV</code> and friends;</li>
<li>textual and HTML representation displayed in terminals and IDEs;</li>
<li>estimator serialization;</li>
<li>parameters validation;</li>
<li>data validation;</li>
<li>feature names validation.</li>
</ul>
<p>Read more in the :ref:<code>User Guide &lt;rolling_your_own_estimator&gt;</code>.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.base import BaseEstimator
&gt;&gt;&gt; class MyEstimator(BaseEstimator):
...     def __init__(self, *, param=1):
...         self.param = param
...     def fit(self, X, y=None):
...         self.is_fitted_ = True
...         return self
...     def predict(self, X):
...         return np.full(shape=X.shape[0], fill_value=self.param)
&gt;&gt;&gt; estimator = MyEstimator(param=2)
&gt;&gt;&gt; estimator.get_params()
{'param': 2}
&gt;&gt;&gt; X = np.array([[1, 2], [2, 3], [3, 4]])
&gt;&gt;&gt; y = np.array([1, 0, 1])
&gt;&gt;&gt; estimator.fit(X, y).predict(X)
array([2, 2, 2])
&gt;&gt;&gt; estimator.set_params(param=3).fit(X, y).predict(X)
array([3, 3, 3])
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ShrunkBART(ImputedBART):

    def __init__(self, estimator_, reg_param, scheme):
        super(ShrunkBART, self).__init__(estimator_)
        self.reg_param = reg_param
        self.scheme = scheme

    def shrink_tree(self, tree):
        root = get_root_node(tree)
        tree_d_node = shrink_node(root, self.reg_param, parent_val=None, parent_num=None, cum_sum=0, scheme=self.scheme,
                                  constant=np.mean(self.estimator_.data.y.values))
        d, l = get_nodes(tree_d_node)
        tree._nodes = d + l
        return tree

    def fit(self, *args, **kwargs):
        if not self.estimator_.fitted:
            self.estimator_.fit(*args, **kwargs)
        samples = []
        for s in self.estimator_.model_samples:
            for i, tree in enumerate(s._trees):
                s_tree = self.shrink_tree(expand_tree(copy.deepcopy(tree)))
                s._trees[i] = s_tree
            samples.append(s)
        self.estimator_._model_samples = samples
        self.fitted_ = True</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodels.experimental.bartpy.sklearnmodel.ImputedBART" href="#imodels.experimental.bartpy.sklearnmodel.ImputedBART">ImputedBART</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV">ShrunkBARTCV</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.ShrunkBART.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, *args, **kwargs):
    if not self.estimator_.fitted:
        self.estimator_.fit(*args, **kwargs)
    samples = []
    for s in self.estimator_.model_samples:
        for i, tree in enumerate(s._trees):
            s_tree = self.shrink_tree(expand_tree(copy.deepcopy(tree)))
            s._trees[i] = s_tree
        samples.append(s)
    self.estimator_._model_samples = samples
    self.fitted_ = True</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.ShrunkBART.shrink_tree"><code class="name flex">
<span>def <span class="ident">shrink_tree</span></span>(<span>self, tree)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shrink_tree(self, tree):
    root = get_root_node(tree)
    tree_d_node = shrink_node(root, self.reg_param, parent_val=None, parent_num=None, cum_sum=0, scheme=self.scheme,
                              constant=np.mean(self.estimator_.data.y.values))
    d, l = get_nodes(tree_d_node)
    tree._nodes = d + l
    return tree</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV"><code class="flex name class">
<span>class <span class="ident">ShrunkBARTCV</span></span>
<span>(</span><span>estimator_: sklearn.base.BaseEstimator, scheme: str, reg_param_list: List[float] = [0.1, 1, 10, 50, 100, 500], cv: int = 3, scoring=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn.</p>
<p>Inheriting from this class provides default implementations of:</p>
<ul>
<li>setting and getting parameters used by <code>GridSearchCV</code> and friends;</li>
<li>textual and HTML representation displayed in terminals and IDEs;</li>
<li>estimator serialization;</li>
<li>parameters validation;</li>
<li>data validation;</li>
<li>feature names validation.</li>
</ul>
<p>Read more in the :ref:<code>User Guide &lt;rolling_your_own_estimator&gt;</code>.</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.base import BaseEstimator
&gt;&gt;&gt; class MyEstimator(BaseEstimator):
...     def __init__(self, *, param=1):
...         self.param = param
...     def fit(self, X, y=None):
...         self.is_fitted_ = True
...         return self
...     def predict(self, X):
...         return np.full(shape=X.shape[0], fill_value=self.param)
&gt;&gt;&gt; estimator = MyEstimator(param=2)
&gt;&gt;&gt; estimator.get_params()
{'param': 2}
&gt;&gt;&gt; X = np.array([[1, 2], [2, 3], [3, 4]])
&gt;&gt;&gt; y = np.array([1, 0, 1])
&gt;&gt;&gt; estimator.fit(X, y).predict(X)
array([2, 2, 2])
&gt;&gt;&gt; estimator.set_params(param=3).fit(X, y).predict(X)
array([3, 3, 3])
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ShrunkBARTCV(ShrunkBART):
    def __init__(self, estimator_: BaseEstimator, scheme: str,
                 reg_param_list: List[float] = [0.1, 1, 10, 50, 100, 500],
                 cv: int = 3, scoring=None):
        super(ShrunkBARTCV, self).__init__(estimator_, None, scheme)
        self.reg_param_list = np.array(reg_param_list)
        self.cv = cv
        self.scoring = scoring

    def fit(self, X, y, *args, **kwargs):
        self.scores_ = []
        for reg_param in self.reg_param_list:
            est = ShrunkBART(deepcopy(self.estimator_), reg_param, self.scheme)
            cv_scores = cross_val_score(est, X, y, cv=self.cv, scoring=self.scoring)
            self.scores_.append(np.mean(cv_scores))
        self.reg_param = self.reg_param_list[np.argmax(self.scores_)]
        super().fit(X=X, y=y)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBART" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBART">ShrunkBART</a></li>
<li><a title="imodels.experimental.bartpy.sklearnmodel.ImputedBART" href="#imodels.experimental.bartpy.sklearnmodel.ImputedBART">ImputedBART</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y, *args, **kwargs):
    self.scores_ = []
    for reg_param in self.reg_param_list:
        est = ShrunkBART(deepcopy(self.estimator_), reg_param, self.scheme)
        cv_scores = cross_val_score(est, X, y, cv=self.cv, scoring=self.scoring)
        self.scores_.append(np.mean(cv_scores))
    self.reg_param = self.reg_param_list[np.argmax(self.scores_)]
    super().fit(X=X, y=y)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel"><code class="flex name class">
<span>class <span class="ident">SklearnModel</span></span>
<span>(</span><span>n_trees: int = 200, n_chains: int = 4, sigma_a: float = 0.001, sigma_b: float = 0.001, n_samples: int = 200, n_burn: int = 200, thin: float = 0.1, alpha: float = 0.95, beta: float = 2.0, store_in_sample_predictions: bool = False, store_acceptance_trace: bool = False, tree_sampler: <a title="imodels.experimental.bartpy.samplers.treemutation.TreeMutationSampler" href="samplers/treemutation.html#imodels.experimental.bartpy.samplers.treemutation.TreeMutationSampler">TreeMutationSampler</a> = &lt;imodels.experimental.bartpy.samplers.unconstrainedtree.treemutation.UnconstrainedTreeMutationSampler object&gt;, initializer: Optional[<a title="imodels.experimental.bartpy.initializers.initializer.Initializer" href="initializers/initializer.html#imodels.experimental.bartpy.initializers.initializer.Initializer">Initializer</a>] = None, n_jobs=-1, classification: bool = False, max_rules=None)</span>
</code></dt>
<dd>
<div class="desc"><p>The main access point to building BART models in BartPy</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_trees</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of trees to use, more trees will make a smoother fit, but slow training and fitting</dd>
<dt><strong><code>n_chains</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of independent chains to run
more chains will improve the quality of the samples, but will require more computation</dd>
<dt><strong><code>sigma_a</code></strong> :&ensp;<code>float</code></dt>
<dd>shape parameter of the prior on sigma</dd>
<dt><strong><code>sigma_b</code></strong> :&ensp;<code>float</code></dt>
<dd>scale parameter of the prior on sigma</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>how many recorded samples to take</dd>
<dt><strong><code>n_burn</code></strong> :&ensp;<code>int</code></dt>
<dd>how many samples to run without recording to reach convergence</dd>
<dt><strong><code>thin</code></strong> :&ensp;<code>float</code></dt>
<dd>percentage of samples to store.
use this to save memory when running large models</dd>
<dt><strong><code>p_grow</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of choosing a grow mutation in tree mutation sampling</dd>
<dt><strong><code>p_prune</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of choosing a prune mutation in tree mutation sampling</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>prior parameter on tree structure</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>float</code></dt>
<dd>prior parameter on tree structure</dd>
<dt><strong><code>store_in_sample_predictions</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to store full prediction samples
set to False if you don't need in sample results - saves a lot of memory</dd>
<dt><strong><code>store_acceptance_trace</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to store acceptance rates of the gibbs samples
unless you're very memory constrained, you wouldn't want to set this to false
useful for diagnostics</dd>
<dt><strong><code>tree_sampler</code></strong> :&ensp;<code>TreeMutationSampler</code></dt>
<dd>Method of sampling used on trees
defaults to <code>bartpy.samplers.unconstrainedtree</code></dd>
<dt><strong><code>initializer</code></strong> :&ensp;<code>Initializer</code></dt>
<dd>Class that handles the initialization of tree structure and leaf values</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>how many cores to use when computing MCMC samples
set to <code>-1</code> to use all cores</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SklearnModel(BaseEstimator, RegressorMixin):
    &#34;&#34;&#34;
    The main access point to building BART models in BartPy

    Parameters
    ----------
    n_trees: int
        the number of trees to use, more trees will make a smoother fit, but slow training and fitting
    n_chains: int
        the number of independent chains to run
        more chains will improve the quality of the samples, but will require more computation
    sigma_a: float
        shape parameter of the prior on sigma
    sigma_b: float
        scale parameter of the prior on sigma
    n_samples: int
        how many recorded samples to take
    n_burn: int
        how many samples to run without recording to reach convergence
    thin: float
        percentage of samples to store.
        use this to save memory when running large models
    p_grow: float
        probability of choosing a grow mutation in tree mutation sampling
    p_prune: float
        probability of choosing a prune mutation in tree mutation sampling
    alpha: float
        prior parameter on tree structure
    beta: float
        prior parameter on tree structure
    store_in_sample_predictions: bool
        whether to store full prediction samples
        set to False if you don&#39;t need in sample results - saves a lot of memory
    store_acceptance_trace: bool
        whether to store acceptance rates of the gibbs samples
        unless you&#39;re very memory constrained, you wouldn&#39;t want to set this to false
        useful for diagnostics
    tree_sampler: TreeMutationSampler
        Method of sampling used on trees
        defaults to `bartpy.samplers.unconstrainedtree`
    initializer: Initializer
        Class that handles the initialization of tree structure and leaf values
    n_jobs: int
        how many cores to use when computing MCMC samples
        set to `-1` to use all cores
    &#34;&#34;&#34;

    def __init__(self,
                 n_trees: int = 200,
                 n_chains: int = 4,
                 sigma_a: float = 0.001,
                 sigma_b: float = 0.001,
                 n_samples: int = 200,
                 n_burn: int = 200,
                 thin: float = 0.1,
                 alpha: float = 0.95,
                 beta: float = 2.,
                 store_in_sample_predictions: bool = False,
                 store_acceptance_trace: bool = False,
                 tree_sampler: TreeMutationSampler = get_tree_sampler(0.5, 0.5),
                 initializer: Optional[Initializer] = None,
                 n_jobs=-1,
                 classification: bool = False,
                 max_rules=None):
        self.n_trees = n_trees
        self.n_chains = n_chains
        self.sigma_a = sigma_a
        self.sigma_b = sigma_b
        self.n_burn = n_burn
        self.n_samples = n_samples
        self.p_grow = 0.5
        self.p_prune = 0.5
        self.alpha = alpha
        self.beta = beta
        self.thin = thin
        self.n_jobs = n_jobs
        self.store_in_sample_predictions = store_in_sample_predictions
        self.store_acceptance_trace = store_acceptance_trace
        self.columns = None
        self.tree_sampler = tree_sampler
        self.initializer = initializer
        self.schedule = SampleSchedule(self.tree_sampler, LeafNodeSampler(), SigmaSampler())
        self.sampler = ModelSampler(self.schedule)
        self.classification = classification
        self.max_rules = max_rules

        self.sigma, self.data, self.model, self._prediction_samples, self._model_samples, self.extract = [None] * 6

    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: np.ndarray) -&gt; &#39;SklearnModel&#39;:
        &#34;&#34;&#34;
        Learn the model based on training data

        Parameters
        ----------
        X: pd.DataFrame
            training covariates
        y: np.ndarray
            training targets

        Returns
        -------
        SklearnModel
            self with trained parameter values
        &#34;&#34;&#34;
        self.model = self._construct_model(X, y)
        self.extract = Parallel(n_jobs=self.n_jobs)(self.f_delayed_chains(X, y))
        self.combined_chains = self._combine_chains(self.extract)
        self._model_samples, self._prediction_samples = self.combined_chains[&#34;model&#34;], self.combined_chains[
            &#34;in_sample_predictions&#34;]
        self._acceptance_trace = self.combined_chains[&#34;acceptance&#34;]
        self._likelihood = self.combined_chains[&#34;likelihood&#34;]
        self._probs = self.combined_chains[&#34;probs&#34;]

        self.fitted_ = True
        return self

    @property
    def fitted(self):
        if hasattr(self, &#34;fitted_&#34;):
            return self.fitted_
        return False

    @property
    def complexity_(self):
        if hasattr(self.initializer, &#34;_tree&#34;):
            estimator = self.initializer._tree
            if hasattr(estimator, &#39;complexity_&#39;):
                return estimator.complexity_

    @staticmethod
    def _combine_chains(extract: List[Chain]) -&gt; Chain:
        keys = list(extract[0].keys())
        combined = {}
        for key in keys:
            combined[key] = np.concatenate([chain[key] for chain in extract], axis=0)
        return combined

    @staticmethod
    def _convert_covariates_to_data(X: np.ndarray, y: np.ndarray) -&gt; Data:
        from copy import deepcopy
        if type(X) == pd.DataFrame:
            X: pd.DataFrame = X
            X = X.values
        return Data(deepcopy(X), deepcopy(y), normalize=True)

    def _construct_model(self, X: np.ndarray, y: np.ndarray) -&gt; Model:
        if len(X) == 0 or X.shape[1] == 0:
            raise ValueError(&#34;Empty covariate matrix passed&#34;)
        self.data = self._convert_covariates_to_data(X, y)
        self.sigma = Sigma(self.sigma_a, self.sigma_b, self.data.y.normalizing_scale, self.classification)
        self.model = Model(self.data,
                           self.sigma,
                           n_trees=self.n_trees,
                           alpha=self.alpha,
                           beta=self.beta,
                           initializer=self.initializer,
                           classification=self.classification)
        n_trees = self.n_trees if self.initializer is None else self.initializer.n_trees
        self.n_trees = n_trees
        return self.model

    def f_delayed_chains(self, X: np.ndarray, y: np.ndarray):
        &#34;&#34;&#34;
        Access point for getting access to delayed methods for running chains
        Useful for when you want to run multiple instances of the model in parallel
        e.g. when calculating a null distribution for feature importance

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array

        Returns
        -------
        List[Callable[[], ChainExtract]]
        &#34;&#34;&#34;
        return [delayed(x)(self, X, y) for x in self.f_chains()]

    def f_chains(self) -&gt; List[Callable[[], Chain]]:
        &#34;&#34;&#34;
        List of methods to run MCMC chains
        Useful for running multiple models in parallel

        Returns
        -------
        List[Callable[[], Extract]]
            List of method to run individual chains
            Length of n_chains
        &#34;&#34;&#34;
        return [delayed_run_chain() for _ in range(self.n_chains)]

    def predict(self, X: np.ndarray = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Predict the target corresponding to the provided covariate matrix
        If X is None, will predict based on training covariates

        Prediction is based on the mean of all samples

        Parameters
        ----------
        X: pd.DataFrame
            covariates to predict from

        Returns
        -------
        np.ndarray
            predictions for the X covariates
        &#34;&#34;&#34;
        if X is None and self.store_in_sample_predictions:
            return self.data.y.unnormalize_y(np.mean(self._prediction_samples, axis=0))
        elif X is None and not self.store_in_sample_predictions:
            raise ValueError(
                &#34;In sample predictions only possible if model.store_in_sample_predictions is `True`.  Either set the parameter to True or pass a non-None X parameter&#34;)
        else:
            predictions = self._out_of_sample_predict(X)
            if self.classification:
                return np.round(predictions, 0)
            return predictions

    def predict_proba(self, X: np.ndarray = None) -&gt; np.ndarray:
        preds = self._out_of_sample_predict(X)
        return np.stack([preds, 1 - preds], axis=1)

    def residuals(self, X=None, y=None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Array of error for each observation

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array

        Returns
        -------
        np.ndarray
            Error for each observation
        &#34;&#34;&#34;
        if y is None:
            return self.model.data.y.unnormalized_y - self.predict(X)
        else:
            return y - self.predict(X)

    def l2_error(self, X=None, y=None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Calculate the squared errors for each row in the covariate matrix

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array
        Returns
        -------
        np.ndarray
            Squared error for each observation
        &#34;&#34;&#34;
        return np.square(self.residuals(X, y))

    def rmse(self, X, y) -&gt; float:
        &#34;&#34;&#34;
        The total RMSE error of the model
        The sum of squared errors over all observations

        Parameters
        ----------
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target array

        Returns
        -------
        float
            The total summed L2 error for the model
        &#34;&#34;&#34;
        return np.sqrt(np.sum(self.l2_error(X, y)))

    def _chain_pred_arr(self, X, chain_number):
        chain_len = int(self.n_samples)
        samples_chain = self._model_samples[chain_number * chain_len: (chain_number + 1) * chain_len]
        predictions_transformed = [x.predict(X) for x in samples_chain]
        return predictions_transformed

    def predict_chain(self, X, chain_number):
        predictions_transformed = self._chain_pred_arr(X, chain_number)
        predictions = self.data.y.unnormalize_y(np.mean(predictions_transformed, axis=0))
        if self.classification:
            predictions = scipy.stats.norm.cdf(predictions)
        return predictions

    def chain_mse_std(self, X, y, chain_number):
        predictions_transformed = self._chain_pred_arr(X, chain_number)
        predictions_std = np.std(
            [mean_squared_error(self.data.y.unnormalize_y(preds), y) for preds in predictions_transformed])
        return predictions_std

    def chain_predictions(self, X, chain_number):
        predictions_transformed = self._chain_pred_arr(X, chain_number)
        preds_arr = [self.data.y.unnormalize_y(preds) for preds in predictions_transformed]
        return preds_arr

    def between_chains_var(self, X):
        all_predictions = np.stack([self.data.y.unnormalize_y(x.predict(X)) for x in self._model_samples], axis=1)

        def _get_var(preds_arr):
            mean_pred = preds_arr.mean(axis=1)
            var = np.mean((preds_arr - np.expand_dims(mean_pred, 1)) ** 2)
            return var

        total_var = _get_var(all_predictions)
        within_chain_var = 0
        for c in range(self.n_chains):
            chain_preds = self._chain_pred_arr(X, c)
            within_chain_var += _get_var(np.stack(chain_preds, axis=1))
        return total_var - within_chain_var

    def _out_of_sample_predict(self, X):
        samples = self._model_samples
        predictions_transformed = [x.predict(X) for x in samples]
        predictions = self.data.y.unnormalize_y(np.mean(predictions_transformed, axis=0))
        if self.classification:
            predictions = scipy.stats.norm.cdf(predictions)
        return predictions

    def fit_predict(self, X, y):
        self.fit(X, y)
        if self.store_in_sample_predictions:
            return self.predict()
        else:
            return self.predict(X)

    @property
    def model_samples(self) -&gt; List[Model]:
        &#34;&#34;&#34;
        Array of the model as it was after each sample.
        Useful for examining for:

         - examining the state of trees, nodes and sigma throughout the sampling
         - out of sample prediction

        Returns None if the model hasn&#39;t been fit

        Returns
        -------
        List[Model]
        &#34;&#34;&#34;
        return self._model_samples

    @property
    def acceptance_trace(self) -&gt; List[Mapping[str, float]]:
        &#34;&#34;&#34;
        List of Mappings from variable name to acceptance rates

        Each entry is the acceptance rate of the variable in each iteration of the model

        Returns
        -------
        List[Mapping[str, float]]
        &#34;&#34;&#34;
        return self._acceptance_trace

    @property
    def likelihood(self) -&gt; List:
        &#34;&#34;&#34;
        List of Mappings from variable name to likelihood

        Each entry is the acceptance rate of the variable in each iteration of the model

        Returns
        -------
        List[Mapping[str, float]]
        &#34;&#34;&#34;
        return self._likelihood

    @property
    def probs(self) -&gt; List:
        &#34;&#34;&#34;
        List of Mappings from variable name to likelihood

        Each entry is the acceptance rate of the variable in each iteration of the model

        Returns
        -------
        List[Mapping[str, float]]
        &#34;&#34;&#34;
        return self._probs

    @property
    def prediction_samples(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Matrix of prediction samples at each point in sampling
        Useful for assessing convergence, calculating point estimates etc.

        Returns
        -------
        np.ndarray
            prediction samples with dimensionality n_samples * n_points
        &#34;&#34;&#34;
        return self.prediction_samples

    def from_extract(self, extract: List[Chain], X: np.ndarray, y: np.ndarray) -&gt; &#39;SklearnModel&#39;:
        &#34;&#34;&#34;
        Create a copy of the model using an extract
        Useful for doing operations on extracts created in external processes like feature selection
        Parameters
        ----------
        extract: Extract
            samples produced by delayed chain methods
        X: np.ndarray
            Covariate matrix
        y: np.ndarray
            Target variable

        Returns
        -------
        SklearnModel
            Copy of the current model with samples
        &#34;&#34;&#34;
        new_model = deepcopy(self)
        combined_chain = self._combine_chains(extract)
        self._model_samples, self._prediction_samples = combined_chain[&#34;model&#34;], combined_chain[&#34;in_sample_predictions&#34;]
        self._acceptance_trace = combined_chain[&#34;acceptance&#34;]
        new_model.data = self._convert_covariates_to_data(X, y)
        return new_model</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin</li>
<li>sklearn.utils._metadata_requests._MetadataRequester</li>
<li>sklearn.base.RegressorMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodels.experimental.bartpy.extensions.baseestimator.ResidualBART" href="extensions/baseestimator.html#imodels.experimental.bartpy.extensions.baseestimator.ResidualBART">ResidualBART</a></li>
<li><a title="imodels.experimental.bartpy.extensions.ols.OLS" href="extensions/ols.html#imodels.experimental.bartpy.extensions.ols.OLS">OLS</a></li>
<li><a title="imodels.experimental.bartpy.sklearnmodel.BART" href="#imodels.experimental.bartpy.sklearnmodel.BART">BART</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.acceptance_trace"><code class="name">var <span class="ident">acceptance_trace</span> : List[Mapping[str, float]]</code></dt>
<dd>
<div class="desc"><p>List of Mappings from variable name to acceptance rates</p>
<p>Each entry is the acceptance rate of the variable in each iteration of the model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Mapping[str, float]]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def acceptance_trace(self) -&gt; List[Mapping[str, float]]:
    &#34;&#34;&#34;
    List of Mappings from variable name to acceptance rates

    Each entry is the acceptance rate of the variable in each iteration of the model

    Returns
    -------
    List[Mapping[str, float]]
    &#34;&#34;&#34;
    return self._acceptance_trace</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.complexity_"><code class="name">var <span class="ident">complexity_</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def complexity_(self):
    if hasattr(self.initializer, &#34;_tree&#34;):
        estimator = self.initializer._tree
        if hasattr(estimator, &#39;complexity_&#39;):
            return estimator.complexity_</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.fitted"><code class="name">var <span class="ident">fitted</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def fitted(self):
    if hasattr(self, &#34;fitted_&#34;):
        return self.fitted_
    return False</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.likelihood"><code class="name">var <span class="ident">likelihood</span> : List</code></dt>
<dd>
<div class="desc"><p>List of Mappings from variable name to likelihood</p>
<p>Each entry is the acceptance rate of the variable in each iteration of the model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Mapping[str, float]]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def likelihood(self) -&gt; List:
    &#34;&#34;&#34;
    List of Mappings from variable name to likelihood

    Each entry is the acceptance rate of the variable in each iteration of the model

    Returns
    -------
    List[Mapping[str, float]]
    &#34;&#34;&#34;
    return self._likelihood</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.model_samples"><code class="name">var <span class="ident">model_samples</span> : List[<a title="imodels.experimental.bartpy.model.Model" href="model.html#imodels.experimental.bartpy.model.Model">Model</a>]</code></dt>
<dd>
<div class="desc"><p>Array of the model as it was after each sample.
Useful for examining for:</p>
<ul>
<li>examining the state of trees, nodes and sigma throughout the sampling</li>
<li>out of sample prediction</li>
</ul>
<p>Returns None if the model hasn't been fit</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Model]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def model_samples(self) -&gt; List[Model]:
    &#34;&#34;&#34;
    Array of the model as it was after each sample.
    Useful for examining for:

     - examining the state of trees, nodes and sigma throughout the sampling
     - out of sample prediction

    Returns None if the model hasn&#39;t been fit

    Returns
    -------
    List[Model]
    &#34;&#34;&#34;
    return self._model_samples</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.prediction_samples"><code class="name">var <span class="ident">prediction_samples</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>Matrix of prediction samples at each point in sampling
Useful for assessing convergence, calculating point estimates etc.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>prediction samples with dimensionality n_samples * n_points</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def prediction_samples(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Matrix of prediction samples at each point in sampling
    Useful for assessing convergence, calculating point estimates etc.

    Returns
    -------
    np.ndarray
        prediction samples with dimensionality n_samples * n_points
    &#34;&#34;&#34;
    return self.prediction_samples</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.probs"><code class="name">var <span class="ident">probs</span> : List</code></dt>
<dd>
<div class="desc"><p>List of Mappings from variable name to likelihood</p>
<p>Each entry is the acceptance rate of the variable in each iteration of the model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Mapping[str, float]]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def probs(self) -&gt; List:
    &#34;&#34;&#34;
    List of Mappings from variable name to likelihood

    Each entry is the acceptance rate of the variable in each iteration of the model

    Returns
    -------
    List[Mapping[str, float]]
    &#34;&#34;&#34;
    return self._probs</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.between_chains_var"><code class="name flex">
<span>def <span class="ident">between_chains_var</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def between_chains_var(self, X):
    all_predictions = np.stack([self.data.y.unnormalize_y(x.predict(X)) for x in self._model_samples], axis=1)

    def _get_var(preds_arr):
        mean_pred = preds_arr.mean(axis=1)
        var = np.mean((preds_arr - np.expand_dims(mean_pred, 1)) ** 2)
        return var

    total_var = _get_var(all_predictions)
    within_chain_var = 0
    for c in range(self.n_chains):
        chain_preds = self._chain_pred_arr(X, c)
        within_chain_var += _get_var(np.stack(chain_preds, axis=1))
    return total_var - within_chain_var</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.chain_mse_std"><code class="name flex">
<span>def <span class="ident">chain_mse_std</span></span>(<span>self, X, y, chain_number)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def chain_mse_std(self, X, y, chain_number):
    predictions_transformed = self._chain_pred_arr(X, chain_number)
    predictions_std = np.std(
        [mean_squared_error(self.data.y.unnormalize_y(preds), y) for preds in predictions_transformed])
    return predictions_std</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.chain_predictions"><code class="name flex">
<span>def <span class="ident">chain_predictions</span></span>(<span>self, X, chain_number)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def chain_predictions(self, X, chain_number):
    predictions_transformed = self._chain_pred_arr(X, chain_number)
    preds_arr = [self.data.y.unnormalize_y(preds) for preds in predictions_transformed]
    return preds_arr</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_chains"><code class="name flex">
<span>def <span class="ident">f_chains</span></span>(<span>self) ‑> List[Callable[[], Mapping[str, Union[List[Any], numpy.ndarray]]]]</span>
</code></dt>
<dd>
<div class="desc"><p>List of methods to run MCMC chains
Useful for running multiple models in parallel</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Callable[[], Extract]]</code></dt>
<dd>List of method to run individual chains
Length of n_chains</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def f_chains(self) -&gt; List[Callable[[], Chain]]:
    &#34;&#34;&#34;
    List of methods to run MCMC chains
    Useful for running multiple models in parallel

    Returns
    -------
    List[Callable[[], Extract]]
        List of method to run individual chains
        Length of n_chains
    &#34;&#34;&#34;
    return [delayed_run_chain() for _ in range(self.n_chains)]</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_delayed_chains"><code class="name flex">
<span>def <span class="ident">f_delayed_chains</span></span>(<span>self, X: numpy.ndarray, y: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Access point for getting access to delayed methods for running chains
Useful for when you want to run multiple instances of the model in parallel
e.g. when calculating a null distribution for feature importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Covariate matrix</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Target array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Callable[[], ChainExtract]]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def f_delayed_chains(self, X: np.ndarray, y: np.ndarray):
    &#34;&#34;&#34;
    Access point for getting access to delayed methods for running chains
    Useful for when you want to run multiple instances of the model in parallel
    e.g. when calculating a null distribution for feature importance

    Parameters
    ----------
    X: np.ndarray
        Covariate matrix
    y: np.ndarray
        Target array

    Returns
    -------
    List[Callable[[], ChainExtract]]
    &#34;&#34;&#34;
    return [delayed(x)(self, X, y) for x in self.f_chains()]</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: Union[numpy.ndarray, pandas.core.frame.DataFrame], y: numpy.ndarray) ‑> <a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></span>
</code></dt>
<dd>
<div class="desc"><p>Learn the model based on training data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>training covariates</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>training targets</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></code></dt>
<dd>self with trained parameter values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X: Union[np.ndarray, pd.DataFrame], y: np.ndarray) -&gt; &#39;SklearnModel&#39;:
    &#34;&#34;&#34;
    Learn the model based on training data

    Parameters
    ----------
    X: pd.DataFrame
        training covariates
    y: np.ndarray
        training targets

    Returns
    -------
    SklearnModel
        self with trained parameter values
    &#34;&#34;&#34;
    self.model = self._construct_model(X, y)
    self.extract = Parallel(n_jobs=self.n_jobs)(self.f_delayed_chains(X, y))
    self.combined_chains = self._combine_chains(self.extract)
    self._model_samples, self._prediction_samples = self.combined_chains[&#34;model&#34;], self.combined_chains[
        &#34;in_sample_predictions&#34;]
    self._acceptance_trace = self.combined_chains[&#34;acceptance&#34;]
    self._likelihood = self.combined_chains[&#34;likelihood&#34;]
    self._probs = self.combined_chains[&#34;probs&#34;]

    self.fitted_ = True
    return self</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit_predict"><code class="name flex">
<span>def <span class="ident">fit_predict</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_predict(self, X, y):
    self.fit(X, y)
    if self.store_in_sample_predictions:
        return self.predict()
    else:
        return self.predict(X)</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.from_extract"><code class="name flex">
<span>def <span class="ident">from_extract</span></span>(<span>self, extract: List[Mapping[str, Union[List[Any], numpy.ndarray]]], X: numpy.ndarray, y: numpy.ndarray) ‑> <a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create a copy of the model using an extract
Useful for doing operations on extracts created in external processes like feature selection
Parameters</p>
<hr>
<dl>
<dt><strong><code>extract</code></strong> :&ensp;<code>Extract</code></dt>
<dd>samples produced by delayed chain methods</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Covariate matrix</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Target variable</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></code></dt>
<dd>Copy of the current model with samples</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_extract(self, extract: List[Chain], X: np.ndarray, y: np.ndarray) -&gt; &#39;SklearnModel&#39;:
    &#34;&#34;&#34;
    Create a copy of the model using an extract
    Useful for doing operations on extracts created in external processes like feature selection
    Parameters
    ----------
    extract: Extract
        samples produced by delayed chain methods
    X: np.ndarray
        Covariate matrix
    y: np.ndarray
        Target variable

    Returns
    -------
    SklearnModel
        Copy of the current model with samples
    &#34;&#34;&#34;
    new_model = deepcopy(self)
    combined_chain = self._combine_chains(extract)
    self._model_samples, self._prediction_samples = combined_chain[&#34;model&#34;], combined_chain[&#34;in_sample_predictions&#34;]
    self._acceptance_trace = combined_chain[&#34;acceptance&#34;]
    new_model.data = self._convert_covariates_to_data(X, y)
    return new_model</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.l2_error"><code class="name flex">
<span>def <span class="ident">l2_error</span></span>(<span>self, X=None, y=None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the squared errors for each row in the covariate matrix</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Covariate matrix</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Target array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Squared error for each observation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l2_error(self, X=None, y=None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Calculate the squared errors for each row in the covariate matrix

    Parameters
    ----------
    X: np.ndarray
        Covariate matrix
    y: np.ndarray
        Target array
    Returns
    -------
    np.ndarray
        Squared error for each observation
    &#34;&#34;&#34;
    return np.square(self.residuals(X, y))</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X: numpy.ndarray = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Predict the target corresponding to the provided covariate matrix
If X is None, will predict based on training covariates</p>
<p>Prediction is based on the mean of all samples</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>covariates to predict from</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>predictions for the X covariates</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X: np.ndarray = None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Predict the target corresponding to the provided covariate matrix
    If X is None, will predict based on training covariates

    Prediction is based on the mean of all samples

    Parameters
    ----------
    X: pd.DataFrame
        covariates to predict from

    Returns
    -------
    np.ndarray
        predictions for the X covariates
    &#34;&#34;&#34;
    if X is None and self.store_in_sample_predictions:
        return self.data.y.unnormalize_y(np.mean(self._prediction_samples, axis=0))
    elif X is None and not self.store_in_sample_predictions:
        raise ValueError(
            &#34;In sample predictions only possible if model.store_in_sample_predictions is `True`.  Either set the parameter to True or pass a non-None X parameter&#34;)
    else:
        predictions = self._out_of_sample_predict(X)
        if self.classification:
            return np.round(predictions, 0)
        return predictions</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict_chain"><code class="name flex">
<span>def <span class="ident">predict_chain</span></span>(<span>self, X, chain_number)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_chain(self, X, chain_number):
    predictions_transformed = self._chain_pred_arr(X, chain_number)
    predictions = self.data.y.unnormalize_y(np.mean(predictions_transformed, axis=0))
    if self.classification:
        predictions = scipy.stats.norm.cdf(predictions)
    return predictions</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X: numpy.ndarray = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X: np.ndarray = None) -&gt; np.ndarray:
    preds = self._out_of_sample_predict(X)
    return np.stack([preds, 1 - preds], axis=1)</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.residuals"><code class="name flex">
<span>def <span class="ident">residuals</span></span>(<span>self, X=None, y=None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Array of error for each observation</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Covariate matrix</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Target array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Error for each observation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def residuals(self, X=None, y=None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Array of error for each observation

    Parameters
    ----------
    X: np.ndarray
        Covariate matrix
    y: np.ndarray
        Target array

    Returns
    -------
    np.ndarray
        Error for each observation
    &#34;&#34;&#34;
    if y is None:
        return self.model.data.y.unnormalized_y - self.predict(X)
    else:
        return y - self.predict(X)</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.rmse"><code class="name flex">
<span>def <span class="ident">rmse</span></span>(<span>self, X, y) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>The total RMSE error of the model
The sum of squared errors over all observations</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Covariate matrix</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Target array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The total summed L2 error for the model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rmse(self, X, y) -&gt; float:
    &#34;&#34;&#34;
    The total RMSE error of the model
    The sum of squared errors over all observations

    Parameters
    ----------
    X: np.ndarray
        Covariate matrix
    y: np.ndarray
        Target array

    Returns
    -------
    float
        The total summed L2 error for the model
    &#34;&#34;&#34;
    return np.sqrt(np.sum(self.l2_error(X, y)))</code></pre>
</details>
</dd>
<dt id="imodels.experimental.bartpy.sklearnmodel.SklearnModel.set_score_request"><code class="name flex">
<span>def <span class="ident">set_score_request</span></span>(<span>self: <a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a>, *, sample_weight: Union[bool, ForwardRef(None), str] = '$UNCHANGED$') ‑> <a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></span>
</code></dt>
<dd>
<div class="desc"><p>Request metadata passed to the <code>score</code> method.</p>
<p>Note that this method is only relevant if
<code>enable_metadata_routing=True</code> (see :func:<code>sklearn.set_config</code>).
Please see :ref:<code>User Guide &lt;metadata_routing&gt;</code> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul>
<li>
<p><code>True</code>: metadata is requested, and passed to <code>score</code> if provided. The request is ignored if metadata is not provided.</p>
</li>
<li>
<p><code>False</code>: metadata is not requested and the meta-estimator will not pass it to <code>score</code>.</p>
</li>
<li>
<p><code>None</code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p>
</li>
<li>
<p><code>str</code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p>
</li>
</ul>
<p>The default (<code>sklearn.utils.metadata_routing.UNCHANGED</code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;1.3</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
:class:<code>~sklearn.pipeline.Pipeline</code>. Otherwise it has no effect.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>str, True, False,</code> or <code>None</code>,
default=<code>sklearn.utils.metadata_routing.UNCHANGED</code></dt>
<dd>Metadata routing for <code>sample_weight</code> parameter in <code>score</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>The updated object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(*args, **kw):
    &#34;&#34;&#34;Updates the request for provided parameters

    This docstring is overwritten below.
    See REQUESTER_DOC for expected functionality
    &#34;&#34;&#34;
    if not _routing_enabled():
        raise RuntimeError(
            &#34;This method is only available when metadata routing is enabled.&#34;
            &#34; You can enable it using&#34;
            &#34; sklearn.set_config(enable_metadata_routing=True).&#34;
        )

    if self.validate_keys and (set(kw) - set(self.keys)):
        raise TypeError(
            f&#34;Unexpected args: {set(kw) - set(self.keys)} in {self.name}. &#34;
            f&#34;Accepted arguments are: {set(self.keys)}&#34;
        )

    # This makes it possible to use the decorated method as an unbound method,
    # for instance when monkeypatching.
    # https://github.com/scikit-learn/scikit-learn/issues/28632
    if instance is None:
        _instance = args[0]
        args = args[1:]
    else:
        _instance = instance

    # Replicating python&#39;s behavior when positional args are given other than
    # `self`, and `self` is only allowed if this method is unbound.
    if args:
        raise TypeError(
            f&#34;set_{self.name}_request() takes 0 positional argument but&#34;
            f&#34; {len(args)} were given&#34;
        )

    requests = _instance._get_metadata_request()
    method_metadata_request = getattr(requests, self.name)

    for prop, alias in kw.items():
        if alias is not UNCHANGED:
            method_metadata_request.add_request(param=prop, alias=alias)
    _instance._metadata_request = requests

    return _instance</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index 🔍</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodels.experimental.bartpy" href="index.html">imodels.experimental.bartpy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.delayed_run_chain" href="#imodels.experimental.bartpy.sklearnmodel.delayed_run_chain">delayed_run_chain</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.expand_node" href="#imodels.experimental.bartpy.sklearnmodel.expand_node">expand_node</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.expand_tree" href="#imodels.experimental.bartpy.sklearnmodel.expand_tree">expand_tree</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.get_nodes" href="#imodels.experimental.bartpy.sklearnmodel.get_nodes">get_nodes</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.get_root_node" href="#imodels.experimental.bartpy.sklearnmodel.get_root_node">get_root_node</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.main" href="#imodels.experimental.bartpy.sklearnmodel.main">main</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.run_chain" href="#imodels.experimental.bartpy.sklearnmodel.run_chain">run_chain</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.shrink_node" href="#imodels.experimental.bartpy.sklearnmodel.shrink_node">shrink_node</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.shrink_tree" href="#imodels.experimental.bartpy.sklearnmodel.shrink_tree">shrink_tree</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodels.experimental.bartpy.sklearnmodel.BART" href="#imodels.experimental.bartpy.sklearnmodel.BART">BART</a></code></h4>
<ul class="">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.BART.sample_complexity" href="#imodels.experimental.bartpy.sklearnmodel.BART.sample_complexity">sample_complexity</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.BART.sub_forest" href="#imodels.experimental.bartpy.sklearnmodel.BART.sub_forest">sub_forest</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.BART.trees" href="#imodels.experimental.bartpy.sklearnmodel.BART.trees">trees</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.BART.update_complexity" href="#imodels.experimental.bartpy.sklearnmodel.BART.update_complexity">update_complexity</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodels.experimental.bartpy.sklearnmodel.ExpandedBART" href="#imodels.experimental.bartpy.sklearnmodel.ExpandedBART">ExpandedBART</a></code></h4>
<ul class="">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.ExpandedBART.fit" href="#imodels.experimental.bartpy.sklearnmodel.ExpandedBART.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodels.experimental.bartpy.sklearnmodel.ImputedBART" href="#imodels.experimental.bartpy.sklearnmodel.ImputedBART">ImputedBART</a></code></h4>
<ul class="">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.ImputedBART.predict" href="#imodels.experimental.bartpy.sklearnmodel.ImputedBART.predict">predict</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.ImputedBART.predict_proba" href="#imodels.experimental.bartpy.sklearnmodel.ImputedBART.predict_proba">predict_proba</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.ImputedBART.score" href="#imodels.experimental.bartpy.sklearnmodel.ImputedBART.score">score</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBART" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBART">ShrunkBART</a></code></h4>
<ul class="">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBART.fit" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBART.fit">fit</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBART.shrink_tree" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBART.shrink_tree">shrink_tree</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV">ShrunkBARTCV</a></code></h4>
<ul class="">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV.fit" href="#imodels.experimental.bartpy.sklearnmodel.ShrunkBARTCV.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel">SklearnModel</a></code></h4>
<ul class="two-column">
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.acceptance_trace" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.acceptance_trace">acceptance_trace</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.between_chains_var" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.between_chains_var">between_chains_var</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.chain_mse_std" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.chain_mse_std">chain_mse_std</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.chain_predictions" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.chain_predictions">chain_predictions</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.complexity_" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.complexity_">complexity_</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_chains" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_chains">f_chains</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_delayed_chains" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.f_delayed_chains">f_delayed_chains</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit">fit</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit_predict" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.fit_predict">fit_predict</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.fitted" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.fitted">fitted</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.from_extract" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.from_extract">from_extract</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.l2_error" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.l2_error">l2_error</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.likelihood" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.likelihood">likelihood</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.model_samples" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.model_samples">model_samples</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict">predict</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict_chain" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict_chain">predict_chain</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict_proba" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.predict_proba">predict_proba</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.prediction_samples" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.prediction_samples">prediction_samples</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.probs" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.probs">probs</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.residuals" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.residuals">residuals</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.rmse" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.rmse">rmse</a></code></li>
<li><code><a title="imodels.experimental.bartpy.sklearnmodel.SklearnModel.set_score_request" href="#imodels.experimental.bartpy.sklearnmodel.SklearnModel.set_score_request">set_score_request</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img align="center" width=100% src="https://csinva.io/imodels/img/anim.gif"> </img></p>
<!-- add wave animation -->
</nav>
</main>
<footer id="footer">
</footer>
</body>
</html>
<!-- add github corner -->
<a href="https://github.com/csinva/imodels" class="github-corner" aria-label="View source on GitHub"><svg width="120" height="120" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="m128.3,109.0 c113.8,99.7 119.0,89.6 119.0,89.6 c122.0,82.7 120.5,78.6 120.5,78.6 c119.2,72.0 123.4,76.3 123.4,76.3 c127.3,80.9 125.5,87.3 125.5,87.3 c122.9,97.6 130.6,101.9 134.4,103.2" fill="currentcolor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<!-- add wave animation stylesheet -->
<link rel="stylesheet" href="github.css">