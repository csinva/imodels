<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>imodels.rule_set.boosted_rules API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodels.rule_set.boosted_rules</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from copy import deepcopy
from functools import partial

import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin, MetaEstimatorMixin
from sklearn.preprocessing import normalize
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils.multiclass import check_classification_targets, unique_labels
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted

from imodels.rule_set.rule_set import RuleSet
from imodels.rule_set.slipper_util import SlipperBaseEstimator
from imodels.util.convert import tree_to_code, tree_to_rules, dict_to_rule
from imodels.util.rule import Rule, get_feature_dict, replace_feature_name


class BoostedRulesClassifier(RuleSet, BaseEstimator, MetaEstimatorMixin, ClassifierMixin):
    &#39;&#39;&#39;An easy-interpretable classifier optimizing simple logical rules.
    Currently limited to only binary classification.
    
    Params
    ------
    estimator: object with fit and predict methods
        Defaults to DecisionTreeClassifier with AdaBoost.
        For SLIPPER, should pass estimator=imodels.SlipperBaseEstimator
    &#39;&#39;&#39;

    def __init__(self, n_estimators=10, estimator=partial(DecisionTreeClassifier, max_depth=1)):
        self.n_estimators = n_estimators
        self.estimator = estimator

    def fit(self, X, y, feature_names=None, sample_weight=None):
        &#34;&#34;&#34;Fit the model according to the given training data.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        y : array-like, shape (n_samples,)
            Target vector relative to X. Has to follow the convention 0 for
            normal data, 1 for anomalies.

        sample_weight : array-like, shape (n_samples,) optional
            Array of weights that are assigned to individual samples, typically
            the amount in case of transactions data. Used to grow regression
            trees producing further rules to be tested.
            If not provided, then each sample is given unit weight.

        Returns
        -------
        self : object
            Returns self.
        &#34;&#34;&#34;

        X, y = check_X_y(X, y)
        check_classification_targets(y)
        self.n_features_ = X.shape[1]
        self.classes_ = unique_labels(y)

        self.feature_dict_ = get_feature_dict(X.shape[1], feature_names)
        self.feature_placeholders = list(self.feature_dict_.keys())
        self.feature_names = list(self.feature_dict_.values())

        n_train = y.shape[0]
        w = np.ones(n_train) / n_train
        self.estimators_ = []
        self.estimator_weights_ = []
        self.estimator_errors_ = []
        self.feature_names = feature_names
        for _ in range(self.n_estimators):
            # Fit a classifier with the specific weights
            clf = self.estimator()
            clf.fit(X, y, sample_weight=w)  # uses w as the sampling weight!
            preds = clf.predict(X)

            # Indicator function
            miss = preds != y

            # Equivalent with 1/-1 to update weights
            miss2 = np.ones(miss.size)
            miss2[~miss] = -1

            # Error
            err_m = np.dot(w, miss) / sum(w)
            if err_m &lt; 1e-3:
                return self

            # Alpha
            alpha_m = 0.5 * np.log((1 - err_m) / float(err_m))

            # New weights
            w = np.multiply(w, np.exp([float(x) * alpha_m
                                       for x in miss2]))

            self.estimators_.append(deepcopy(clf))
            self.estimator_weights_.append(alpha_m)
            self.estimator_errors_.append(err_m)

        rules = []

        for est, est_weight in zip(self.estimators_, self.estimator_weights_):
            if type(clf) == DecisionTreeClassifier:
                est_rules_values = tree_to_rules(est, self.feature_placeholders, prediction_values=True)
                est_rules = list(map(lambda x: x[0], est_rules_values))

                # BRS scores are difference between class 1 % and class 0 % in a node
                est_values = np.array(list(map(lambda x: x[1], est_rules_values)))
                rule_scores = (est_values[:, 1] - est_values[:, 0]) / est_values.sum(axis=1)

                compos_score = est_weight * rule_scores
                rules += [Rule(r, args=[w]) for (r, w) in zip(est_rules, compos_score)]

            if type(clf) == SlipperBaseEstimator:
                # SLIPPER uses uniform confidence over in rule observations
                est_rule = dict_to_rule(est.rule, est.feature_dict)
                rules += [Rule(est_rule, args=[est_weight])]

        self.rules_without_feature_names_ = rules
        self.rules_ = [
            replace_feature_name(rule, self.feature_dict_) for rule in self.rules_without_feature_names_
        ]
        self.complexity_ = self._get_complexity()
        return self

    def predict_proba(self, X):
        &#39;&#39;&#39; Predict probabilities for X &#39;&#39;&#39;
        check_is_fitted(self)
        X = check_array(X)

        # Add to prediction
        n_train = X.shape[0]
        n_estimators = len(self.estimators_)
        n_classes = 2  # hard-coded for now!
        preds = np.zeros((n_train, n_classes))
        for i in range(n_estimators):
            preds += self.estimator_weights_[i] * self.estimators_[i].predict_proba(X)
        pred_values = preds / n_estimators
        return normalize(pred_values, norm=&#39;l1&#39;)

    def predict(self, X):
        &#34;&#34;&#34;Predict outcome for X
        &#34;&#34;&#34;
        check_is_fitted(self)
        X = check_array(X)
        return self.eval_weighted_rule_sum(X) &gt; 0

    def __str__(self):
        try:
            s = &#39;Mined rules:\n&#39;
            for est in self.estimators_:
                s += &#39;\t&#39; + tree_to_code(est, self.feature_names)
            return s
        except:
            return f&#39;BoostedRules with {len(self.estimators_)} estimators&#39;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodels.rule_set.boosted_rules.BoostedRulesClassifier"><code class="flex name class">
<span>class <span class="ident">BoostedRulesClassifier</span></span>
<span>(</span><span>n_estimators=10, estimator=functools.partial(&lt;class &#x27;sklearn.tree._classes.DecisionTreeClassifier&#x27;&gt;, max_depth=1))</span>
</code></dt>
<dd>
<section class="desc"><p>An easy-interpretable classifier optimizing simple logical rules.
Currently limited to only binary classification.</p>
<h2 id="params">Params</h2>
<dl>
<dt><strong><code>estimator</code></strong> :&ensp;<code>object</code> <code>with</code> <code>fit</code> <code>and</code> <code>predict</code> <code>methods</code></dt>
<dd>Defaults to DecisionTreeClassifier with AdaBoost.
For SLIPPER, should pass estimator=imodels.SlipperBaseEstimator</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BoostedRulesClassifier(RuleSet, BaseEstimator, MetaEstimatorMixin, ClassifierMixin):
    &#39;&#39;&#39;An easy-interpretable classifier optimizing simple logical rules.
    Currently limited to only binary classification.
    
    Params
    ------
    estimator: object with fit and predict methods
        Defaults to DecisionTreeClassifier with AdaBoost.
        For SLIPPER, should pass estimator=imodels.SlipperBaseEstimator
    &#39;&#39;&#39;

    def __init__(self, n_estimators=10, estimator=partial(DecisionTreeClassifier, max_depth=1)):
        self.n_estimators = n_estimators
        self.estimator = estimator

    def fit(self, X, y, feature_names=None, sample_weight=None):
        &#34;&#34;&#34;Fit the model according to the given training data.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        y : array-like, shape (n_samples,)
            Target vector relative to X. Has to follow the convention 0 for
            normal data, 1 for anomalies.

        sample_weight : array-like, shape (n_samples,) optional
            Array of weights that are assigned to individual samples, typically
            the amount in case of transactions data. Used to grow regression
            trees producing further rules to be tested.
            If not provided, then each sample is given unit weight.

        Returns
        -------
        self : object
            Returns self.
        &#34;&#34;&#34;

        X, y = check_X_y(X, y)
        check_classification_targets(y)
        self.n_features_ = X.shape[1]
        self.classes_ = unique_labels(y)

        self.feature_dict_ = get_feature_dict(X.shape[1], feature_names)
        self.feature_placeholders = list(self.feature_dict_.keys())
        self.feature_names = list(self.feature_dict_.values())

        n_train = y.shape[0]
        w = np.ones(n_train) / n_train
        self.estimators_ = []
        self.estimator_weights_ = []
        self.estimator_errors_ = []
        self.feature_names = feature_names
        for _ in range(self.n_estimators):
            # Fit a classifier with the specific weights
            clf = self.estimator()
            clf.fit(X, y, sample_weight=w)  # uses w as the sampling weight!
            preds = clf.predict(X)

            # Indicator function
            miss = preds != y

            # Equivalent with 1/-1 to update weights
            miss2 = np.ones(miss.size)
            miss2[~miss] = -1

            # Error
            err_m = np.dot(w, miss) / sum(w)
            if err_m &lt; 1e-3:
                return self

            # Alpha
            alpha_m = 0.5 * np.log((1 - err_m) / float(err_m))

            # New weights
            w = np.multiply(w, np.exp([float(x) * alpha_m
                                       for x in miss2]))

            self.estimators_.append(deepcopy(clf))
            self.estimator_weights_.append(alpha_m)
            self.estimator_errors_.append(err_m)

        rules = []

        for est, est_weight in zip(self.estimators_, self.estimator_weights_):
            if type(clf) == DecisionTreeClassifier:
                est_rules_values = tree_to_rules(est, self.feature_placeholders, prediction_values=True)
                est_rules = list(map(lambda x: x[0], est_rules_values))

                # BRS scores are difference between class 1 % and class 0 % in a node
                est_values = np.array(list(map(lambda x: x[1], est_rules_values)))
                rule_scores = (est_values[:, 1] - est_values[:, 0]) / est_values.sum(axis=1)

                compos_score = est_weight * rule_scores
                rules += [Rule(r, args=[w]) for (r, w) in zip(est_rules, compos_score)]

            if type(clf) == SlipperBaseEstimator:
                # SLIPPER uses uniform confidence over in rule observations
                est_rule = dict_to_rule(est.rule, est.feature_dict)
                rules += [Rule(est_rule, args=[est_weight])]

        self.rules_without_feature_names_ = rules
        self.rules_ = [
            replace_feature_name(rule, self.feature_dict_) for rule in self.rules_without_feature_names_
        ]
        self.complexity_ = self._get_complexity()
        return self

    def predict_proba(self, X):
        &#39;&#39;&#39; Predict probabilities for X &#39;&#39;&#39;
        check_is_fitted(self)
        X = check_array(X)

        # Add to prediction
        n_train = X.shape[0]
        n_estimators = len(self.estimators_)
        n_classes = 2  # hard-coded for now!
        preds = np.zeros((n_train, n_classes))
        for i in range(n_estimators):
            preds += self.estimator_weights_[i] * self.estimators_[i].predict_proba(X)
        pred_values = preds / n_estimators
        return normalize(pred_values, norm=&#39;l1&#39;)

    def predict(self, X):
        &#34;&#34;&#34;Predict outcome for X
        &#34;&#34;&#34;
        check_is_fitted(self)
        X = check_array(X)
        return self.eval_weighted_rule_sum(X) &gt; 0

    def __str__(self):
        try:
            s = &#39;Mined rules:\n&#39;
            for est in self.estimators_:
                s += &#39;\t&#39; + tree_to_code(est, self.feature_names)
            return s
        except:
            return f&#39;BoostedRules with {len(self.estimators_)} estimators&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodels.rule_set.rule_set.RuleSet" href="rule_set.html#imodels.rule_set.rule_set.RuleSet">RuleSet</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.MetaEstimatorMixin</li>
<li>sklearn.base.ClassifierMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="imodels.rule_set.slipper.SlipperClassifier" href="slipper.html#imodels.rule_set.slipper.SlipperClassifier">SlipperClassifier</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.rule_set.boosted_rules.BoostedRulesClassifier.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, feature_names=None, sample_weight=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit the model according to the given training data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>, <code>n_features</code>)</dt>
<dd>Training vector, where n_samples is the number of samples and
n_features is the number of features.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>,)</dt>
<dd>Target vector relative to X. Has to follow the convention 0 for
normal data, 1 for anomalies.</dd>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> (<code>n_samples</code>,) optional</dt>
<dd>Array of weights that are assigned to individual samples, typically
the amount in case of transactions data. Used to grow regression
trees producing further rules to be tested.
If not provided, then each sample is given unit weight.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Returns self.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y, feature_names=None, sample_weight=None):
    &#34;&#34;&#34;Fit the model according to the given training data.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Training vector, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape (n_samples,)
        Target vector relative to X. Has to follow the convention 0 for
        normal data, 1 for anomalies.

    sample_weight : array-like, shape (n_samples,) optional
        Array of weights that are assigned to individual samples, typically
        the amount in case of transactions data. Used to grow regression
        trees producing further rules to be tested.
        If not provided, then each sample is given unit weight.

    Returns
    -------
    self : object
        Returns self.
    &#34;&#34;&#34;

    X, y = check_X_y(X, y)
    check_classification_targets(y)
    self.n_features_ = X.shape[1]
    self.classes_ = unique_labels(y)

    self.feature_dict_ = get_feature_dict(X.shape[1], feature_names)
    self.feature_placeholders = list(self.feature_dict_.keys())
    self.feature_names = list(self.feature_dict_.values())

    n_train = y.shape[0]
    w = np.ones(n_train) / n_train
    self.estimators_ = []
    self.estimator_weights_ = []
    self.estimator_errors_ = []
    self.feature_names = feature_names
    for _ in range(self.n_estimators):
        # Fit a classifier with the specific weights
        clf = self.estimator()
        clf.fit(X, y, sample_weight=w)  # uses w as the sampling weight!
        preds = clf.predict(X)

        # Indicator function
        miss = preds != y

        # Equivalent with 1/-1 to update weights
        miss2 = np.ones(miss.size)
        miss2[~miss] = -1

        # Error
        err_m = np.dot(w, miss) / sum(w)
        if err_m &lt; 1e-3:
            return self

        # Alpha
        alpha_m = 0.5 * np.log((1 - err_m) / float(err_m))

        # New weights
        w = np.multiply(w, np.exp([float(x) * alpha_m
                                   for x in miss2]))

        self.estimators_.append(deepcopy(clf))
        self.estimator_weights_.append(alpha_m)
        self.estimator_errors_.append(err_m)

    rules = []

    for est, est_weight in zip(self.estimators_, self.estimator_weights_):
        if type(clf) == DecisionTreeClassifier:
            est_rules_values = tree_to_rules(est, self.feature_placeholders, prediction_values=True)
            est_rules = list(map(lambda x: x[0], est_rules_values))

            # BRS scores are difference between class 1 % and class 0 % in a node
            est_values = np.array(list(map(lambda x: x[1], est_rules_values)))
            rule_scores = (est_values[:, 1] - est_values[:, 0]) / est_values.sum(axis=1)

            compos_score = est_weight * rule_scores
            rules += [Rule(r, args=[w]) for (r, w) in zip(est_rules, compos_score)]

        if type(clf) == SlipperBaseEstimator:
            # SLIPPER uses uniform confidence over in rule observations
            est_rule = dict_to_rule(est.rule, est.feature_dict)
            rules += [Rule(est_rule, args=[est_weight])]

    self.rules_without_feature_names_ = rules
    self.rules_ = [
        replace_feature_name(rule, self.feature_dict_) for rule in self.rules_without_feature_names_
    ]
    self.complexity_ = self._get_complexity()
    return self</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boosted_rules.BoostedRulesClassifier.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict outcome for X</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;Predict outcome for X
    &#34;&#34;&#34;
    check_is_fitted(self)
    X = check_array(X)
    return self.eval_weighted_rule_sum(X) &gt; 0</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boosted_rules.BoostedRulesClassifier.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict probabilities for X</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X):
    &#39;&#39;&#39; Predict probabilities for X &#39;&#39;&#39;
    check_is_fitted(self)
    X = check_array(X)

    # Add to prediction
    n_train = X.shape[0]
    n_estimators = len(self.estimators_)
    n_classes = 2  # hard-coded for now!
    preds = np.zeros((n_train, n_classes))
    for i in range(n_estimators):
        preds += self.estimator_weights_[i] * self.estimators_[i].predict_proba(X)
    pred_values = preds / n_estimators
    return normalize(pred_values, norm=&#39;l1&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodels.rule_set" href="index.html">imodels.rule_set</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodels.rule_set.boosted_rules.BoostedRulesClassifier" href="#imodels.rule_set.boosted_rules.BoostedRulesClassifier">BoostedRulesClassifier</a></code></h4>
<ul class="">
<li><code><a title="imodels.rule_set.boosted_rules.BoostedRulesClassifier.fit" href="#imodels.rule_set.boosted_rules.BoostedRulesClassifier.fit">fit</a></code></li>
<li><code><a title="imodels.rule_set.boosted_rules.BoostedRulesClassifier.predict" href="#imodels.rule_set.boosted_rules.BoostedRulesClassifier.predict">predict</a></code></li>
<li><code><a title="imodels.rule_set.boosted_rules.BoostedRulesClassifier.predict_proba" href="#imodels.rule_set.boosted_rules.BoostedRulesClassifier.predict_proba">predict_proba</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>