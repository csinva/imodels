<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>imodels.rule_set.boa API documentation</title>
<meta name="description" content="Original implementation at https://github.com/wangtongada/BOA" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>imodels.rule_set.boa</code></h1>
</header>
<section id="section-intro">
<p>Original implementation at <a href="https://github.com/wangtongada/BOA">https://github.com/wangtongada/BOA</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;Original implementation at https://github.com/wangtongada/BOA
&#39;&#39;&#39;

import itertools
import operator
from bisect import bisect_left
from collections import defaultdict
from copy import deepcopy
from itertools import combinations
from random import sample

import numpy as np
import pandas as pd
from mlxtend.frequent_patterns import fpgrowth
from numpy.random import random
from scipy.sparse import csc_matrix
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.multiclass import check_classification_targets
from sklearn.utils.validation import check_X_y, check_is_fitted

from imodels.rule_set.rule_set import RuleSet


class BOAClassifier(RuleSet, BaseEstimator, ClassifierMixin):
    &#39;&#39;&#39;Bayesian or-of-and algorithm.
    Generates patterns that satisfy the minimum support and maximum length and then select the Nrules rules that have the highest entropy.
    In function SA_patternbased, each local maximum is stored in maps and the best BOA is returned.
    Remember here the BOA contains only the index of selected rules from Nrules self.rules_
    &#39;&#39;&#39;

    def __init__(self, n_rules: int = 2000,
                 supp=5, maxlen: int = 10,
                 num_iterations=5000, num_chains=3, q=0.1,
                 alpha_pos=100, beta_pos=1, alpha_neg=1, beta_neg=100,
                 alpha_l=None, beta_l=None,
                 discretization_method=&#39;randomforest&#39;, ):
        &#39;&#39;&#39;
        Params
        ------
        n_rules
            number of rules to be used in SA_patternbased and also the output of generate_rules
        supp
            The higher this supp, the &#39;larger&#39; a pattern is. 5% is a generally good number
        maxlen
            maximum length of a pattern
        num_iterations
            number of iterations in each chain
        num_chains
            number of chains in the simulated annealing search algorithm
        q
        alpha_1
            $\rho = alpha/(alpha+beta)$. Make sure $\rho$ is close to one when choosing alpha and beta
            The alpha and beta parameters alter the prior distributions for different rules
        beta_pos
        alpha_neg
        beta_neg
        alpha_l
        beta_l
        discretization_method
            discretization method
        &#39;&#39;&#39;
        self.n_rules = n_rules
        self.supp = supp
        self.maxlen = maxlen

        self.num_iterations = num_iterations
        self.num_chains = num_chains
        self.q = q

        self.alpha_pos = alpha_pos
        self.beta_pos = beta_pos
        self.alpha_neg = alpha_neg
        self.beta_neg = beta_neg
        self.discretization_method = discretization_method

        self.alpha_l = alpha_l
        self.beta_l = beta_l

    def fit(self, X, y, feature_names: list = None, init=[], verbose=False):
        &#39;&#39;&#39;
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
            Training data
        y : array_like, shape = [n_samples]
            Labels

        feature_names : array_like, shape = [n_features], optional (default: [])
            String labels for each feature.
            If empty and X is a DataFrame, column labels are used.
            If empty and X is not a DataFrame, then features are simply enumerated
        &#39;&#39;&#39;
        # check inputs
        # todo: change this so it works for np arrays in addition to just pd dataframe
        self.attr_level_num = defaultdict(int)  # any missing value defaults to 0
        self.attr_names = []

        # get feature names
        if feature_names is None:
            if isinstance(X, pd.DataFrame):
                feature_names = X.columns
            else:
                feature_names = [&#39;X&#39; + str(i) for i in range(X.shape[1])]

        # checks
        X, y = check_X_y(X, y)  # converts df to ndarray
        check_classification_targets(y)
        assert len(feature_names) == X.shape[1], &#39;feature_names should be same size as X.shape[1]&#39;

        # convert to pandas DataFrame
        X = pd.DataFrame(X, columns=feature_names)

        for i, name in enumerate(X.columns):
            self.attr_level_num[name] += 1
            self.attr_names.append(name)
        self.attr_names_orig = deepcopy(self.attr_names)
        self.attr_names = list(set(self.attr_names))

        # set up patterns
        self.set_pattern_space()

        # parameter checking
        if self.alpha_l is None or self.beta_l is None or len(self.alpha_l) != self.maxlen or len(
                self.beta_l) != self.maxlen:
            print(&#39;No or wrong input for alpha_l and beta_l - the model will use default parameters.&#39;)
            self.C = [1.0 / self.maxlen] * self.maxlen
            self.C.insert(0, -1)
            self.alpha_l = [10] * (self.maxlen + 1)
            self.beta_l = [10 * self.pattern_space[i] / self.C[i] for i in range(self.maxlen + 1)]
        else:
            self.alpha_l = [1] + list(self.alpha_l)
            self.beta_l = [1] + list(self.beta_l)

        # setup
        self.generate_rules(X, y)
        n_rules_current = len(self.rules_)
        self.rules_len_list = [len(rule) for rule in self.rules_]
        maps = defaultdict(list)
        T0 = 1000  # initial temperature for simulated annealing
        split = 0.7 * self.num_iterations

        # run simulated annealing
        for chain in range(self.num_chains):
            # initialize with a random pattern set
            if init != []:
                rules_curr = init.copy()
            else:
                assert n_rules_current &gt; 1, f&#39;Only {n_rules_current} potential rules found, change hyperparams to allow for more&#39;
                N = sample(range(1, min(8, n_rules_current), 1), 1)[0]
                rules_curr = sample(range(n_rules_current), N)
            rules_curr_norm = self.normalize(rules_curr)
            pt_curr = -100000000000
            maps[chain].append(
                [-1, [pt_curr / 3, pt_curr / 3, pt_curr / 3], rules_curr, [self.rules_[i] for i in rules_curr]])

            for iter in range(self.num_iterations):
                if iter &gt;= split:
                    p = np.array(range(1 + len(maps[chain])))
                    p = np.array(list(accumulate(p)))
                    p = p / p[-1]
                    index = find_lt(p, random())
                    rules_curr = maps[chain][index][2].copy()
                    rules_curr_norm = maps[chain][index][2].copy()

                # propose new rules
                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(), self.q, y)

                # compute probability of new rules
                cfmatrix, prob = self.compute_prob(rules_new, y)
                T = T0 ** (1 - iter / self.num_iterations)  # temperature for simulated annealing
                pt_new = sum(prob)
                alpha = np.exp(float(pt_new - pt_curr) / T)

                if pt_new &gt; sum(maps[chain][-1][1]):
                    maps[chain].append([iter, prob, rules_new, [self.rules_[i] for i in rules_new]])
                    if verbose:
                        print((
                            &#39;\n** chain = {}, max at iter = {} ** \n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}&#39;
                            &#39;\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\n&#39;).format(
                            chain, iter, (cfmatrix[0] + cfmatrix[2] + 0.0) / len(y), cfmatrix[0], cfmatrix[1],
                            cfmatrix[2], cfmatrix[3], sum(prob), prob[0], prob[1], prob[2])
                        )
                        self.print_rules(rules_new)
                        print(rules_new)
                if random() &lt;= alpha:
                    rules_curr_norm, rules_curr, pt_curr = rules_norm.copy(), rules_new.copy(), pt_new
        pt_max = [sum(maps[chain][-1][1]) for chain in range(self.num_chains)]
        index = pt_max.index(max(pt_max))
        self.rules_ = maps[index][-1][3]
        return self

    def __str__(self):
        return &#39; &#39;.join(str(r) for r in self.rules_)

    def predict(self, X):
        check_is_fitted(self)
        if isinstance(X, np.ndarray):
            df = pd.DataFrame(X, columns=self.attr_names_orig)
        else:
            df = X
        Z = [[]] * len(self.rules_)
        dfn = 1 - df  # df has negative associations
        dfn.columns = [name.strip() + &#39;_neg&#39; for name in df.columns]
        df = pd.concat([df, dfn], axis=1)
        for i, rule in enumerate(self.rules_):
            Z[i] = (np.sum(df[list(rule)], axis=1) == len(rule)).astype(int)
        Yhat = (np.sum(Z, axis=0) &gt; 0).astype(int)
        return Yhat

    def predict_proba(self, X):
        raise Exception(&#39;BOA does not support predicted probabilities.&#39;)

    def set_pattern_space(self):
        &#34;&#34;&#34;Compute the rule space from the levels in each attribute
        &#34;&#34;&#34;
        # add feat_neg to each existing feature feat
        for item in self.attr_names:
            self.attr_level_num[item + &#39;_neg&#39;] = self.attr_level_num[item]
        tmp = [item + &#39;_neg&#39; for item in self.attr_names]
        self.attr_names.extend(tmp)

        # set up pattern_space
        self.pattern_space = np.zeros(self.maxlen + 1)
        for k in range(1, self.maxlen + 1, 1):
            for subset in combinations(self.attr_names, k):
                tmp = 1
                for i in subset:
                    tmp = tmp * self.attr_level_num[i]
                # print(&#39;subset&#39;, subset, &#39;tmp&#39;, tmp, &#39;k&#39;, k)
                self.pattern_space[k] = self.pattern_space[k] + tmp

    def generate_rules(self, X, y):
        &#39;&#39;&#39;This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top n_rules rules that make data have the biggest decrease in entropy
        there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen&lt;=3, fpgrowth can generates rules much faster than randomforest.
        If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories.
        &#39;&#39;&#39;

        df = 1 - X  # df has negative associations
        df.columns = [name.strip() + &#39;_neg&#39; for name in X.columns]
        df = pd.concat([X, df], axis=1)
        if self.discretization_method == &#39;fpgrowth&#39; and self.maxlen &lt;= 3:
            itemMatrix = [[item for item in df.columns if row[item] == 1] for i, row in df.iterrows()]
            pindex = np.where(y == 1)[0]
            rules = fpgrowth([itemMatrix[i] for i in pindex], supp=self.supp, zmin=1, zmax=self.maxlen)
            rules = [tuple(np.sort(rule[0])) for rule in rules]
            rules = list(set(rules))
        else:
            &#39;&#39;&#39;todo: replace this with imodels.RFDiscretizer
            &#39;&#39;&#39;
            rules = []
            for length in range(1, self.maxlen + 1, 1):
                n_estimators = min(pow(df.shape[1], length), 4000)
                clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=length)
                clf.fit(X, y)
                for n in range(n_estimators):
                    rules.extend(extract_rules(clf.estimators_[n], df.columns))
            rules = [list(x) for x in set(tuple(x) for x in rules)]
        self.rules_ = rules

        # select the top n_rules rules using secondary criteria, information gain
        self.screen_rules(df, y)  # updates self.rules_
        self.set_pattern_space()

    def screen_rules(self, df, y):
        &#39;&#39;&#39;Screening rules using information gain
        &#39;&#39;&#39;
        item_ind_dict = {}
        for i, name in enumerate(df.columns):
            item_ind_dict[name] = i
        indices = np.array(
            list(itertools.chain.from_iterable([[item_ind_dict[x] for x in rule] for rule in self.rules_])))
        len_rules = [len(rule) for rule in self.rules_]
        indptr = list(accumulate(len_rules))
        indptr.insert(0, 0)
        indptr = np.array(indptr)
        data = np.ones(len(indices))
        rule_matrix = csc_matrix((data, indices, indptr), shape=(len(df.columns), len(self.rules_)))
        mat = np.matrix(df) @ rule_matrix
        len_matrix = np.array([len_rules] * df.shape[0])
        Z = (mat == len_matrix).astype(int)
        Zpos = [Z[i] for i in np.where(y &gt; 0)][0]
        TP = np.array(np.sum(Zpos, axis=0).tolist()[0])
        supp_select = np.where(TP &gt;= self.supp * sum(y) / 100)[0]
        FP = np.array(np.sum(Z, axis=0))[0] - TP
        TN = len(y) - np.sum(y) - FP
        FN = np.sum(y) - TP
        p1 = TP.astype(float) / (TP + FP)
        p2 = FN.astype(float) / (FN + TN)
        pp = (TP + FP).astype(float) / (TP + FP + TN + FN)
        cond_entropy = -pp * (p1 * np.log(p1) + (1 - p1) * np.log(1 - p1)) - (1 - pp) * (
                p2 * np.log(p2) + (1 - p2) * np.log(1 - p2))
        cond_entropy[p1 * (1 - p1) == 0] = -((1 - pp) * (p2 * np.log(p2) + (1 - p2) * np.log(1 - p2)))[
            p1 * (1 - p1) == 0]
        cond_entropy[p2 * (1 - p2) == 0] = -(pp * (p1 * np.log(p1) + (1 - p1) * np.log(1 - p1)))[p2 * (1 - p2) == 0]
        cond_entropy[p1 * (1 - p1) * p2 * (1 - p2) == 0] = 0
        select = np.argsort(cond_entropy[supp_select])[::-1][-self.n_rules:]
        self.rules_ = [self.rules_[i] for i in supp_select[select]]
        self.RMatrix = np.array(Z[:, supp_select[select]])

    def propose(self, rules_curr, rules_norm, q, y):
        nRules = len(self.rules_)
        yhat = (np.sum(self.RMatrix[:, rules_curr], axis=1) &gt; 0).astype(int)
        incorr = np.where(y != yhat)[0]
        N = len(rules_curr)

        if len(incorr) == 0:
            # BOA correctly classified all points but there could be redundant patterns, so cleaning is needed
            move = [&#39;clean&#39;]
        else:
            ex = sample(incorr.tolist(), 1)[0]
            t = random()
            if y[ex] == 1 or N == 1:
                if t &lt; 1.0 / 2 or N == 1:
                    move = [&#39;add&#39;]  # action: add
                else:
                    move = [&#39;cut&#39;, &#39;add&#39;]  # action: replace
            else:
                if t &lt; 1.0 / 2:
                    move = [&#39;cut&#39;]  # action: cut
                else:
                    move = [&#39;cut&#39;, &#39;add&#39;]  # action: replace
        if move[0] == &#39;cut&#39;:
            &#34;&#34;&#34; cut &#34;&#34;&#34;
            if random() &lt; q:
                candidate = list(set(np.where(self.RMatrix[ex, :] == 1)[0]).intersection(rules_curr))
                if len(candidate) == 0:
                    candidate = rules_curr
                cut_rule = sample(candidate, 1)[0]
            else:
                p = []
                all_sum = np.sum(self.RMatrix[:, rules_curr], axis=1)
                for index, rule in enumerate(rules_curr):
                    yhat = ((all_sum - np.array(self.RMatrix[:, rule])) &gt; 0).astype(int)
                    TP, FP, TN, FN = get_confusion_matrix(yhat, y)
                    p.append(TP.astype(float) / (TP + FP + 1))
                p = [x - min(p) for x in p]
                p = np.exp(p)
                p = np.insert(p, 0, 0)
                p = np.array(list(accumulate(p)))
                if p[-1] == 0:
                    index = sample(range(len(rules_curr)), 1)[0]
                else:
                    p = p / p[-1]
                index = find_lt(p, random())
                cut_rule = rules_curr[index]
            rules_curr.remove(cut_rule)
            rules_norm = self.normalize(rules_curr)
            move.remove(&#39;cut&#39;)

        if len(move) &gt; 0 and move[0] == &#39;add&#39;:
            &#34;&#34;&#34; add &#34;&#34;&#34;
            if random() &lt; q:
                add_rule = sample(range(nRules), 1)[0]
            else:
                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:, rules_curr], axis=1) &lt; 1)[0])
                mat = np.multiply(self.RMatrix[Yhat_neg_index, :].transpose(), y[Yhat_neg_index])
                TP = np.sum(mat, axis=1)
                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index, :], axis=0) - TP))
                p = (TP.astype(float) / (TP + FP + 1))
                p[rules_curr] = 0
                add_rule = sample(np.where(p == max(p))[0].tolist(), 1)[0]
            if add_rule not in rules_curr:
                rules_curr.append(add_rule)
                rules_norm = self.normalize(rules_curr)

        if len(move) &gt; 0 and move[0] == &#39;clean&#39;:
            remove = []
            for i, rule in enumerate(rules_norm):
                yhat = (np.sum(
                    self.RMatrix[:, [rule for j, rule in enumerate(rules_norm) if (j != i and j not in remove)]],
                    axis=1) &gt; 0).astype(int)
                TP, FP, TN, FN = get_confusion_matrix(yhat, y)
                if TP + FP == 0:
                    remove.append(i)
            for x in remove:
                rules_norm.remove(x)
            return rules_curr, rules_norm
        return rules_curr, rules_norm

    def compute_prob(self, rules, y):
        Yhat = (np.sum(self.RMatrix[:, rules], axis=1) &gt; 0).astype(int)
        TP, FP, TN, FN = get_confusion_matrix(Yhat, y)
        Kn_count = list(np.bincount([self.rules_len_list[x] for x in rules], minlength=self.maxlen + 1))
        prior_ChsRules = sum([log_betabin(Kn_count[i], self.pattern_space[i], self.alpha_l[i], self.beta_l[i]) for i in
                              range(1, len(Kn_count), 1)])
        likelihood_1 = log_betabin(TP, TP + FP, self.alpha_pos, self.beta_pos)
        likelihood_2 = log_betabin(TN, FN + TN, self.alpha_neg, self.beta_neg)
        return [TP, FP, TN, FN], [prior_ChsRules, likelihood_1, likelihood_2]

    def normalize_add(self, rules_new, rule_index):
        rules = rules_new.copy()
        for rule in rules_new:
            if set(self.rules_[rule]).issubset(self.rules_[rule_index]):
                return rules_new.copy()
            if set(self.rules_[rule_index]).issubset(self.rules_[rule]):
                rules.remove(rule)
        rules.append(rule_index)
        return rules

    def normalize(self, rules_new):
        try:
            rules_len = [len(self.rules_[index]) for index in rules_new]
            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]
            p1 = 0
            while p1 &lt; len(rules):
                for p2 in range(p1 + 1, len(rules), 1):
                    if set(self.rules_[rules[p2]]).issubset(set(self.rules_[rules[p1]])):
                        rules.remove(rules[p1])
                        p1 -= 1
                        break
                p1 += 1
            return rules
        except:
            return rules_new.copy()

    def print_rules(self, rules_max):
        for rule_index in rules_max:
            print(self.rules_[rule_index])


def accumulate(iterable, func=operator.add):
    &#39;&#39;&#39;Return running totals
    Ex. accumulate([1,2,3,4,5]) --&gt; 1 3 6 10 15
    Ex. accumulate([1,2,3,4,5], operator.mul) --&gt; 1 2 6 24 120
    &#39;&#39;&#39;
    it = iter(iterable)
    total = next(it)
    yield total
    for element in it:
        total = func(total, element)
        yield total


def find_lt(a, x):
    &#34;&#34;&#34; Find rightmost value less than x&#34;&#34;&#34;
    i = bisect_left(a, x)
    if i:
        return int(i - 1)
    print(&#39;in find_lt,{}&#39;.format(a))
    raise ValueError


def log_gampoiss(k, alpha, beta):
    import math
    k = int(k)
    return math.lgamma(k + alpha) + alpha * np.log(beta) - math.lgamma(alpha) - math.lgamma(k + 1) - (
            alpha + k) * np.log(1 + beta)


def log_betabin(k, n, alpha, beta):
    import math
    try:
        const = math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta)
    except:
        print(&#39;alpha = {}, beta = {}&#39;.format(alpha, beta))
    if isinstance(k, list) or isinstance(k, np.ndarray):
        if len(k) != len(n):
            print(&#39;length of k is %d and length of n is %d&#39; % (len(k), len(n)))
            raise ValueError
        lbeta = []
        for ki, ni in zip(k, n):
            lbeta.append(math.lgamma(ki + alpha) + math.lgamma(ni - ki + beta) - math.lgamma(ni + alpha + beta) + const)
        return np.array(lbeta)
    else:
        return math.lgamma(k + alpha) + math.lgamma(n - k + beta) - math.lgamma(n + alpha + beta) + const


def get_confusion_matrix(Yhat, Y):
    if len(Yhat) != len(Y):
        raise NameError(&#39;Yhat has different length&#39;)
    TP = np.dot(np.array(Y), np.array(Yhat))
    FP = np.sum(Yhat) - TP
    TN = len(Y) - np.sum(Y) - FP
    FN = len(Yhat) - np.sum(Yhat) - TN
    return TP, FP, TN, FN


def extract_rules(tree, feature_names):
    left = tree.tree_.children_left
    right = tree.tree_.children_right
    features = [feature_names[i] for i in tree.tree_.feature]

    # get ids of child nodes
    idx = np.argwhere(left == -1)[:, 0]

    def recurse(left, right, child, lineage=None):
        if lineage is None:
            lineage = []
        if child in left:
            parent = np.where(left == child)[0].item()
            suffix = &#39;_neg&#39;
        else:
            parent = np.where(right == child)[0].item()
            suffix = &#39;&#39;
        lineage.append((features[parent].strip() + suffix))

        if parent == 0:
            lineage.reverse()
            return lineage
        else:
            return recurse(left, right, parent, lineage)

    rules = []
    for child in idx:
        rule = []
        for node in recurse(left, right, child):
            rule.append(node)
        rules.append(rule)
    return rules</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="imodels.rule_set.boa.accumulate"><code class="name flex">
<span>def <span class="ident">accumulate</span></span>(<span>iterable, func=&lt;built-in function add&gt;)</span>
</code></dt>
<dd>
<section class="desc"><p>Return running totals
Ex. accumulate([1,2,3,4,5]) &ndash;&gt; 1 3 6 10 15
Ex. accumulate([1,2,3,4,5], operator.mul) &ndash;&gt; 1 2 6 24 120</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accumulate(iterable, func=operator.add):
    &#39;&#39;&#39;Return running totals
    Ex. accumulate([1,2,3,4,5]) --&gt; 1 3 6 10 15
    Ex. accumulate([1,2,3,4,5], operator.mul) --&gt; 1 2 6 24 120
    &#39;&#39;&#39;
    it = iter(iterable)
    total = next(it)
    yield total
    for element in it:
        total = func(total, element)
        yield total</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.extract_rules"><code class="name flex">
<span>def <span class="ident">extract_rules</span></span>(<span>tree, feature_names)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_rules(tree, feature_names):
    left = tree.tree_.children_left
    right = tree.tree_.children_right
    features = [feature_names[i] for i in tree.tree_.feature]

    # get ids of child nodes
    idx = np.argwhere(left == -1)[:, 0]

    def recurse(left, right, child, lineage=None):
        if lineage is None:
            lineage = []
        if child in left:
            parent = np.where(left == child)[0].item()
            suffix = &#39;_neg&#39;
        else:
            parent = np.where(right == child)[0].item()
            suffix = &#39;&#39;
        lineage.append((features[parent].strip() + suffix))

        if parent == 0:
            lineage.reverse()
            return lineage
        else:
            return recurse(left, right, parent, lineage)

    rules = []
    for child in idx:
        rule = []
        for node in recurse(left, right, child):
            rule.append(node)
        rules.append(rule)
    return rules</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.find_lt"><code class="name flex">
<span>def <span class="ident">find_lt</span></span>(<span>a, x)</span>
</code></dt>
<dd>
<section class="desc"><p>Find rightmost value less than x</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_lt(a, x):
    &#34;&#34;&#34; Find rightmost value less than x&#34;&#34;&#34;
    i = bisect_left(a, x)
    if i:
        return int(i - 1)
    print(&#39;in find_lt,{}&#39;.format(a))
    raise ValueError</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.get_confusion_matrix"><code class="name flex">
<span>def <span class="ident">get_confusion_matrix</span></span>(<span>Yhat, Y)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_confusion_matrix(Yhat, Y):
    if len(Yhat) != len(Y):
        raise NameError(&#39;Yhat has different length&#39;)
    TP = np.dot(np.array(Y), np.array(Yhat))
    FP = np.sum(Yhat) - TP
    TN = len(Y) - np.sum(Y) - FP
    FN = len(Yhat) - np.sum(Yhat) - TN
    return TP, FP, TN, FN</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.log_betabin"><code class="name flex">
<span>def <span class="ident">log_betabin</span></span>(<span>k, n, alpha, beta)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_betabin(k, n, alpha, beta):
    import math
    try:
        const = math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta)
    except:
        print(&#39;alpha = {}, beta = {}&#39;.format(alpha, beta))
    if isinstance(k, list) or isinstance(k, np.ndarray):
        if len(k) != len(n):
            print(&#39;length of k is %d and length of n is %d&#39; % (len(k), len(n)))
            raise ValueError
        lbeta = []
        for ki, ni in zip(k, n):
            lbeta.append(math.lgamma(ki + alpha) + math.lgamma(ni - ki + beta) - math.lgamma(ni + alpha + beta) + const)
        return np.array(lbeta)
    else:
        return math.lgamma(k + alpha) + math.lgamma(n - k + beta) - math.lgamma(n + alpha + beta) + const</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.log_gampoiss"><code class="name flex">
<span>def <span class="ident">log_gampoiss</span></span>(<span>k, alpha, beta)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_gampoiss(k, alpha, beta):
    import math
    k = int(k)
    return math.lgamma(k + alpha) + alpha * np.log(beta) - math.lgamma(alpha) - math.lgamma(k + 1) - (
            alpha + k) * np.log(1 + beta)</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.random"><code class="name flex">
<span>def <span class="ident">random</span></span>(<span>...)</span>
</code></dt>
<dd>
<section class="desc"><p>random(size=None)</p>
<p>Return random floats in the half-open interval [0.0, 1.0). Alias for
<code>random_sample</code> to ease forward-porting to the new random API.</p></section>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="imodels.rule_set.boa.BOAClassifier"><code class="flex name class">
<span>class <span class="ident">BOAClassifier</span></span>
<span>(</span><span>n_rules=2000, supp=5, maxlen=10, num_iterations=5000, num_chains=3, q=0.1, alpha_pos=100, beta_pos=1, alpha_neg=1, beta_neg=100, alpha_l=None, beta_l=None, discretization_method='randomforest')</span>
</code></dt>
<dd>
<section class="desc"><p>Bayesian or-of-and algorithm.
Generates patterns that satisfy the minimum support and maximum length and then select the Nrules rules that have the highest entropy.
In function SA_patternbased, each local maximum is stored in maps and the best BOA is returned.
Remember here the BOA contains only the index of selected rules from Nrules self.rules_</p>
<h2 id="params">Params</h2>
<dl>
<dt><strong><code>n_rules</code></strong></dt>
<dd>number of rules to be used in SA_patternbased and also the output of generate_rules</dd>
<dt><strong><code>supp</code></strong></dt>
<dd>The higher this supp, the 'larger' a pattern is. 5% is a generally good number</dd>
<dt><strong><code>maxlen</code></strong></dt>
<dd>maximum length of a pattern</dd>
<dt><strong><code>num_iterations</code></strong></dt>
<dd>number of iterations in each chain</dd>
<dt><strong><code>num_chains</code></strong></dt>
<dd>number of chains in the simulated annealing search algorithm</dd>
<dt><strong><code>q</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>alpha_1</code></strong></dt>
<dd>$
ho = alpha/(alpha+beta)$. Make sure $
ho$ is close to one when choosing alpha and beta
The alpha and beta parameters alter the prior distributions for different rules</dd>
<dt><strong><code>beta_pos</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>alpha_neg</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>beta_neg</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>alpha_l</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>beta_l</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>discretization_method</code></strong></dt>
<dd>discretization method</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BOAClassifier(RuleSet, BaseEstimator, ClassifierMixin):
    &#39;&#39;&#39;Bayesian or-of-and algorithm.
    Generates patterns that satisfy the minimum support and maximum length and then select the Nrules rules that have the highest entropy.
    In function SA_patternbased, each local maximum is stored in maps and the best BOA is returned.
    Remember here the BOA contains only the index of selected rules from Nrules self.rules_
    &#39;&#39;&#39;

    def __init__(self, n_rules: int = 2000,
                 supp=5, maxlen: int = 10,
                 num_iterations=5000, num_chains=3, q=0.1,
                 alpha_pos=100, beta_pos=1, alpha_neg=1, beta_neg=100,
                 alpha_l=None, beta_l=None,
                 discretization_method=&#39;randomforest&#39;, ):
        &#39;&#39;&#39;
        Params
        ------
        n_rules
            number of rules to be used in SA_patternbased and also the output of generate_rules
        supp
            The higher this supp, the &#39;larger&#39; a pattern is. 5% is a generally good number
        maxlen
            maximum length of a pattern
        num_iterations
            number of iterations in each chain
        num_chains
            number of chains in the simulated annealing search algorithm
        q
        alpha_1
            $\rho = alpha/(alpha+beta)$. Make sure $\rho$ is close to one when choosing alpha and beta
            The alpha and beta parameters alter the prior distributions for different rules
        beta_pos
        alpha_neg
        beta_neg
        alpha_l
        beta_l
        discretization_method
            discretization method
        &#39;&#39;&#39;
        self.n_rules = n_rules
        self.supp = supp
        self.maxlen = maxlen

        self.num_iterations = num_iterations
        self.num_chains = num_chains
        self.q = q

        self.alpha_pos = alpha_pos
        self.beta_pos = beta_pos
        self.alpha_neg = alpha_neg
        self.beta_neg = beta_neg
        self.discretization_method = discretization_method

        self.alpha_l = alpha_l
        self.beta_l = beta_l

    def fit(self, X, y, feature_names: list = None, init=[], verbose=False):
        &#39;&#39;&#39;
        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
            Training data
        y : array_like, shape = [n_samples]
            Labels

        feature_names : array_like, shape = [n_features], optional (default: [])
            String labels for each feature.
            If empty and X is a DataFrame, column labels are used.
            If empty and X is not a DataFrame, then features are simply enumerated
        &#39;&#39;&#39;
        # check inputs
        # todo: change this so it works for np arrays in addition to just pd dataframe
        self.attr_level_num = defaultdict(int)  # any missing value defaults to 0
        self.attr_names = []

        # get feature names
        if feature_names is None:
            if isinstance(X, pd.DataFrame):
                feature_names = X.columns
            else:
                feature_names = [&#39;X&#39; + str(i) for i in range(X.shape[1])]

        # checks
        X, y = check_X_y(X, y)  # converts df to ndarray
        check_classification_targets(y)
        assert len(feature_names) == X.shape[1], &#39;feature_names should be same size as X.shape[1]&#39;

        # convert to pandas DataFrame
        X = pd.DataFrame(X, columns=feature_names)

        for i, name in enumerate(X.columns):
            self.attr_level_num[name] += 1
            self.attr_names.append(name)
        self.attr_names_orig = deepcopy(self.attr_names)
        self.attr_names = list(set(self.attr_names))

        # set up patterns
        self.set_pattern_space()

        # parameter checking
        if self.alpha_l is None or self.beta_l is None or len(self.alpha_l) != self.maxlen or len(
                self.beta_l) != self.maxlen:
            print(&#39;No or wrong input for alpha_l and beta_l - the model will use default parameters.&#39;)
            self.C = [1.0 / self.maxlen] * self.maxlen
            self.C.insert(0, -1)
            self.alpha_l = [10] * (self.maxlen + 1)
            self.beta_l = [10 * self.pattern_space[i] / self.C[i] for i in range(self.maxlen + 1)]
        else:
            self.alpha_l = [1] + list(self.alpha_l)
            self.beta_l = [1] + list(self.beta_l)

        # setup
        self.generate_rules(X, y)
        n_rules_current = len(self.rules_)
        self.rules_len_list = [len(rule) for rule in self.rules_]
        maps = defaultdict(list)
        T0 = 1000  # initial temperature for simulated annealing
        split = 0.7 * self.num_iterations

        # run simulated annealing
        for chain in range(self.num_chains):
            # initialize with a random pattern set
            if init != []:
                rules_curr = init.copy()
            else:
                assert n_rules_current &gt; 1, f&#39;Only {n_rules_current} potential rules found, change hyperparams to allow for more&#39;
                N = sample(range(1, min(8, n_rules_current), 1), 1)[0]
                rules_curr = sample(range(n_rules_current), N)
            rules_curr_norm = self.normalize(rules_curr)
            pt_curr = -100000000000
            maps[chain].append(
                [-1, [pt_curr / 3, pt_curr / 3, pt_curr / 3], rules_curr, [self.rules_[i] for i in rules_curr]])

            for iter in range(self.num_iterations):
                if iter &gt;= split:
                    p = np.array(range(1 + len(maps[chain])))
                    p = np.array(list(accumulate(p)))
                    p = p / p[-1]
                    index = find_lt(p, random())
                    rules_curr = maps[chain][index][2].copy()
                    rules_curr_norm = maps[chain][index][2].copy()

                # propose new rules
                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(), self.q, y)

                # compute probability of new rules
                cfmatrix, prob = self.compute_prob(rules_new, y)
                T = T0 ** (1 - iter / self.num_iterations)  # temperature for simulated annealing
                pt_new = sum(prob)
                alpha = np.exp(float(pt_new - pt_curr) / T)

                if pt_new &gt; sum(maps[chain][-1][1]):
                    maps[chain].append([iter, prob, rules_new, [self.rules_[i] for i in rules_new]])
                    if verbose:
                        print((
                            &#39;\n** chain = {}, max at iter = {} ** \n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}&#39;
                            &#39;\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\n&#39;).format(
                            chain, iter, (cfmatrix[0] + cfmatrix[2] + 0.0) / len(y), cfmatrix[0], cfmatrix[1],
                            cfmatrix[2], cfmatrix[3], sum(prob), prob[0], prob[1], prob[2])
                        )
                        self.print_rules(rules_new)
                        print(rules_new)
                if random() &lt;= alpha:
                    rules_curr_norm, rules_curr, pt_curr = rules_norm.copy(), rules_new.copy(), pt_new
        pt_max = [sum(maps[chain][-1][1]) for chain in range(self.num_chains)]
        index = pt_max.index(max(pt_max))
        self.rules_ = maps[index][-1][3]
        return self

    def __str__(self):
        return &#39; &#39;.join(str(r) for r in self.rules_)

    def predict(self, X):
        check_is_fitted(self)
        if isinstance(X, np.ndarray):
            df = pd.DataFrame(X, columns=self.attr_names_orig)
        else:
            df = X
        Z = [[]] * len(self.rules_)
        dfn = 1 - df  # df has negative associations
        dfn.columns = [name.strip() + &#39;_neg&#39; for name in df.columns]
        df = pd.concat([df, dfn], axis=1)
        for i, rule in enumerate(self.rules_):
            Z[i] = (np.sum(df[list(rule)], axis=1) == len(rule)).astype(int)
        Yhat = (np.sum(Z, axis=0) &gt; 0).astype(int)
        return Yhat

    def predict_proba(self, X):
        raise Exception(&#39;BOA does not support predicted probabilities.&#39;)

    def set_pattern_space(self):
        &#34;&#34;&#34;Compute the rule space from the levels in each attribute
        &#34;&#34;&#34;
        # add feat_neg to each existing feature feat
        for item in self.attr_names:
            self.attr_level_num[item + &#39;_neg&#39;] = self.attr_level_num[item]
        tmp = [item + &#39;_neg&#39; for item in self.attr_names]
        self.attr_names.extend(tmp)

        # set up pattern_space
        self.pattern_space = np.zeros(self.maxlen + 1)
        for k in range(1, self.maxlen + 1, 1):
            for subset in combinations(self.attr_names, k):
                tmp = 1
                for i in subset:
                    tmp = tmp * self.attr_level_num[i]
                # print(&#39;subset&#39;, subset, &#39;tmp&#39;, tmp, &#39;k&#39;, k)
                self.pattern_space[k] = self.pattern_space[k] + tmp

    def generate_rules(self, X, y):
        &#39;&#39;&#39;This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top n_rules rules that make data have the biggest decrease in entropy
        there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen&lt;=3, fpgrowth can generates rules much faster than randomforest.
        If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories.
        &#39;&#39;&#39;

        df = 1 - X  # df has negative associations
        df.columns = [name.strip() + &#39;_neg&#39; for name in X.columns]
        df = pd.concat([X, df], axis=1)
        if self.discretization_method == &#39;fpgrowth&#39; and self.maxlen &lt;= 3:
            itemMatrix = [[item for item in df.columns if row[item] == 1] for i, row in df.iterrows()]
            pindex = np.where(y == 1)[0]
            rules = fpgrowth([itemMatrix[i] for i in pindex], supp=self.supp, zmin=1, zmax=self.maxlen)
            rules = [tuple(np.sort(rule[0])) for rule in rules]
            rules = list(set(rules))
        else:
            &#39;&#39;&#39;todo: replace this with imodels.RFDiscretizer
            &#39;&#39;&#39;
            rules = []
            for length in range(1, self.maxlen + 1, 1):
                n_estimators = min(pow(df.shape[1], length), 4000)
                clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=length)
                clf.fit(X, y)
                for n in range(n_estimators):
                    rules.extend(extract_rules(clf.estimators_[n], df.columns))
            rules = [list(x) for x in set(tuple(x) for x in rules)]
        self.rules_ = rules

        # select the top n_rules rules using secondary criteria, information gain
        self.screen_rules(df, y)  # updates self.rules_
        self.set_pattern_space()

    def screen_rules(self, df, y):
        &#39;&#39;&#39;Screening rules using information gain
        &#39;&#39;&#39;
        item_ind_dict = {}
        for i, name in enumerate(df.columns):
            item_ind_dict[name] = i
        indices = np.array(
            list(itertools.chain.from_iterable([[item_ind_dict[x] for x in rule] for rule in self.rules_])))
        len_rules = [len(rule) for rule in self.rules_]
        indptr = list(accumulate(len_rules))
        indptr.insert(0, 0)
        indptr = np.array(indptr)
        data = np.ones(len(indices))
        rule_matrix = csc_matrix((data, indices, indptr), shape=(len(df.columns), len(self.rules_)))
        mat = np.matrix(df) @ rule_matrix
        len_matrix = np.array([len_rules] * df.shape[0])
        Z = (mat == len_matrix).astype(int)
        Zpos = [Z[i] for i in np.where(y &gt; 0)][0]
        TP = np.array(np.sum(Zpos, axis=0).tolist()[0])
        supp_select = np.where(TP &gt;= self.supp * sum(y) / 100)[0]
        FP = np.array(np.sum(Z, axis=0))[0] - TP
        TN = len(y) - np.sum(y) - FP
        FN = np.sum(y) - TP
        p1 = TP.astype(float) / (TP + FP)
        p2 = FN.astype(float) / (FN + TN)
        pp = (TP + FP).astype(float) / (TP + FP + TN + FN)
        cond_entropy = -pp * (p1 * np.log(p1) + (1 - p1) * np.log(1 - p1)) - (1 - pp) * (
                p2 * np.log(p2) + (1 - p2) * np.log(1 - p2))
        cond_entropy[p1 * (1 - p1) == 0] = -((1 - pp) * (p2 * np.log(p2) + (1 - p2) * np.log(1 - p2)))[
            p1 * (1 - p1) == 0]
        cond_entropy[p2 * (1 - p2) == 0] = -(pp * (p1 * np.log(p1) + (1 - p1) * np.log(1 - p1)))[p2 * (1 - p2) == 0]
        cond_entropy[p1 * (1 - p1) * p2 * (1 - p2) == 0] = 0
        select = np.argsort(cond_entropy[supp_select])[::-1][-self.n_rules:]
        self.rules_ = [self.rules_[i] for i in supp_select[select]]
        self.RMatrix = np.array(Z[:, supp_select[select]])

    def propose(self, rules_curr, rules_norm, q, y):
        nRules = len(self.rules_)
        yhat = (np.sum(self.RMatrix[:, rules_curr], axis=1) &gt; 0).astype(int)
        incorr = np.where(y != yhat)[0]
        N = len(rules_curr)

        if len(incorr) == 0:
            # BOA correctly classified all points but there could be redundant patterns, so cleaning is needed
            move = [&#39;clean&#39;]
        else:
            ex = sample(incorr.tolist(), 1)[0]
            t = random()
            if y[ex] == 1 or N == 1:
                if t &lt; 1.0 / 2 or N == 1:
                    move = [&#39;add&#39;]  # action: add
                else:
                    move = [&#39;cut&#39;, &#39;add&#39;]  # action: replace
            else:
                if t &lt; 1.0 / 2:
                    move = [&#39;cut&#39;]  # action: cut
                else:
                    move = [&#39;cut&#39;, &#39;add&#39;]  # action: replace
        if move[0] == &#39;cut&#39;:
            &#34;&#34;&#34; cut &#34;&#34;&#34;
            if random() &lt; q:
                candidate = list(set(np.where(self.RMatrix[ex, :] == 1)[0]).intersection(rules_curr))
                if len(candidate) == 0:
                    candidate = rules_curr
                cut_rule = sample(candidate, 1)[0]
            else:
                p = []
                all_sum = np.sum(self.RMatrix[:, rules_curr], axis=1)
                for index, rule in enumerate(rules_curr):
                    yhat = ((all_sum - np.array(self.RMatrix[:, rule])) &gt; 0).astype(int)
                    TP, FP, TN, FN = get_confusion_matrix(yhat, y)
                    p.append(TP.astype(float) / (TP + FP + 1))
                p = [x - min(p) for x in p]
                p = np.exp(p)
                p = np.insert(p, 0, 0)
                p = np.array(list(accumulate(p)))
                if p[-1] == 0:
                    index = sample(range(len(rules_curr)), 1)[0]
                else:
                    p = p / p[-1]
                index = find_lt(p, random())
                cut_rule = rules_curr[index]
            rules_curr.remove(cut_rule)
            rules_norm = self.normalize(rules_curr)
            move.remove(&#39;cut&#39;)

        if len(move) &gt; 0 and move[0] == &#39;add&#39;:
            &#34;&#34;&#34; add &#34;&#34;&#34;
            if random() &lt; q:
                add_rule = sample(range(nRules), 1)[0]
            else:
                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:, rules_curr], axis=1) &lt; 1)[0])
                mat = np.multiply(self.RMatrix[Yhat_neg_index, :].transpose(), y[Yhat_neg_index])
                TP = np.sum(mat, axis=1)
                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index, :], axis=0) - TP))
                p = (TP.astype(float) / (TP + FP + 1))
                p[rules_curr] = 0
                add_rule = sample(np.where(p == max(p))[0].tolist(), 1)[0]
            if add_rule not in rules_curr:
                rules_curr.append(add_rule)
                rules_norm = self.normalize(rules_curr)

        if len(move) &gt; 0 and move[0] == &#39;clean&#39;:
            remove = []
            for i, rule in enumerate(rules_norm):
                yhat = (np.sum(
                    self.RMatrix[:, [rule for j, rule in enumerate(rules_norm) if (j != i and j not in remove)]],
                    axis=1) &gt; 0).astype(int)
                TP, FP, TN, FN = get_confusion_matrix(yhat, y)
                if TP + FP == 0:
                    remove.append(i)
            for x in remove:
                rules_norm.remove(x)
            return rules_curr, rules_norm
        return rules_curr, rules_norm

    def compute_prob(self, rules, y):
        Yhat = (np.sum(self.RMatrix[:, rules], axis=1) &gt; 0).astype(int)
        TP, FP, TN, FN = get_confusion_matrix(Yhat, y)
        Kn_count = list(np.bincount([self.rules_len_list[x] for x in rules], minlength=self.maxlen + 1))
        prior_ChsRules = sum([log_betabin(Kn_count[i], self.pattern_space[i], self.alpha_l[i], self.beta_l[i]) for i in
                              range(1, len(Kn_count), 1)])
        likelihood_1 = log_betabin(TP, TP + FP, self.alpha_pos, self.beta_pos)
        likelihood_2 = log_betabin(TN, FN + TN, self.alpha_neg, self.beta_neg)
        return [TP, FP, TN, FN], [prior_ChsRules, likelihood_1, likelihood_2]

    def normalize_add(self, rules_new, rule_index):
        rules = rules_new.copy()
        for rule in rules_new:
            if set(self.rules_[rule]).issubset(self.rules_[rule_index]):
                return rules_new.copy()
            if set(self.rules_[rule_index]).issubset(self.rules_[rule]):
                rules.remove(rule)
        rules.append(rule_index)
        return rules

    def normalize(self, rules_new):
        try:
            rules_len = [len(self.rules_[index]) for index in rules_new]
            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]
            p1 = 0
            while p1 &lt; len(rules):
                for p2 in range(p1 + 1, len(rules), 1):
                    if set(self.rules_[rules[p2]]).issubset(set(self.rules_[rules[p1]])):
                        rules.remove(rules[p1])
                        p1 -= 1
                        break
                p1 += 1
            return rules
        except:
            return rules_new.copy()

    def print_rules(self, rules_max):
        for rule_index in rules_max:
            print(self.rules_[rule_index])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="imodels.rule_set.rule_set.RuleSet" href="rule_set.html#imodels.rule_set.rule_set.RuleSet">RuleSet</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.ClassifierMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="imodels.rule_set.boa.BOAClassifier.compute_prob"><code class="name flex">
<span>def <span class="ident">compute_prob</span></span>(<span>self, rules, y)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_prob(self, rules, y):
    Yhat = (np.sum(self.RMatrix[:, rules], axis=1) &gt; 0).astype(int)
    TP, FP, TN, FN = get_confusion_matrix(Yhat, y)
    Kn_count = list(np.bincount([self.rules_len_list[x] for x in rules], minlength=self.maxlen + 1))
    prior_ChsRules = sum([log_betabin(Kn_count[i], self.pattern_space[i], self.alpha_l[i], self.beta_l[i]) for i in
                          range(1, len(Kn_count), 1)])
    likelihood_1 = log_betabin(TP, TP + FP, self.alpha_pos, self.beta_pos)
    likelihood_2 = log_betabin(TN, FN + TN, self.alpha_neg, self.beta_neg)
    return [TP, FP, TN, FN], [prior_ChsRules, likelihood_1, likelihood_2]</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, feature_names=None, init=[], verbose=False)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array</code>-<code>like</code>, <code>shape</code> = [<code>n_samples</code>, <code>n_features</code>]</dt>
<dd>Training data</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array_like</code>, <code>shape</code> = [<code>n_samples</code>]</dt>
<dd>Labels</dd>
<dt><strong><code>feature_names</code></strong> :&ensp;<code>array_like</code>, <code>shape</code> = [<code>n_features</code>], optional (default: [])</dt>
<dd>String labels for each feature.
If empty and X is a DataFrame, column labels are used.
If empty and X is not a DataFrame, then features are simply enumerated</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y, feature_names: list = None, init=[], verbose=False):
    &#39;&#39;&#39;
    Parameters
    ----------
    X : array-like, shape = [n_samples, n_features]
        Training data
    y : array_like, shape = [n_samples]
        Labels

    feature_names : array_like, shape = [n_features], optional (default: [])
        String labels for each feature.
        If empty and X is a DataFrame, column labels are used.
        If empty and X is not a DataFrame, then features are simply enumerated
    &#39;&#39;&#39;
    # check inputs
    # todo: change this so it works for np arrays in addition to just pd dataframe
    self.attr_level_num = defaultdict(int)  # any missing value defaults to 0
    self.attr_names = []

    # get feature names
    if feature_names is None:
        if isinstance(X, pd.DataFrame):
            feature_names = X.columns
        else:
            feature_names = [&#39;X&#39; + str(i) for i in range(X.shape[1])]

    # checks
    X, y = check_X_y(X, y)  # converts df to ndarray
    check_classification_targets(y)
    assert len(feature_names) == X.shape[1], &#39;feature_names should be same size as X.shape[1]&#39;

    # convert to pandas DataFrame
    X = pd.DataFrame(X, columns=feature_names)

    for i, name in enumerate(X.columns):
        self.attr_level_num[name] += 1
        self.attr_names.append(name)
    self.attr_names_orig = deepcopy(self.attr_names)
    self.attr_names = list(set(self.attr_names))

    # set up patterns
    self.set_pattern_space()

    # parameter checking
    if self.alpha_l is None or self.beta_l is None or len(self.alpha_l) != self.maxlen or len(
            self.beta_l) != self.maxlen:
        print(&#39;No or wrong input for alpha_l and beta_l - the model will use default parameters.&#39;)
        self.C = [1.0 / self.maxlen] * self.maxlen
        self.C.insert(0, -1)
        self.alpha_l = [10] * (self.maxlen + 1)
        self.beta_l = [10 * self.pattern_space[i] / self.C[i] for i in range(self.maxlen + 1)]
    else:
        self.alpha_l = [1] + list(self.alpha_l)
        self.beta_l = [1] + list(self.beta_l)

    # setup
    self.generate_rules(X, y)
    n_rules_current = len(self.rules_)
    self.rules_len_list = [len(rule) for rule in self.rules_]
    maps = defaultdict(list)
    T0 = 1000  # initial temperature for simulated annealing
    split = 0.7 * self.num_iterations

    # run simulated annealing
    for chain in range(self.num_chains):
        # initialize with a random pattern set
        if init != []:
            rules_curr = init.copy()
        else:
            assert n_rules_current &gt; 1, f&#39;Only {n_rules_current} potential rules found, change hyperparams to allow for more&#39;
            N = sample(range(1, min(8, n_rules_current), 1), 1)[0]
            rules_curr = sample(range(n_rules_current), N)
        rules_curr_norm = self.normalize(rules_curr)
        pt_curr = -100000000000
        maps[chain].append(
            [-1, [pt_curr / 3, pt_curr / 3, pt_curr / 3], rules_curr, [self.rules_[i] for i in rules_curr]])

        for iter in range(self.num_iterations):
            if iter &gt;= split:
                p = np.array(range(1 + len(maps[chain])))
                p = np.array(list(accumulate(p)))
                p = p / p[-1]
                index = find_lt(p, random())
                rules_curr = maps[chain][index][2].copy()
                rules_curr_norm = maps[chain][index][2].copy()

            # propose new rules
            rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(), self.q, y)

            # compute probability of new rules
            cfmatrix, prob = self.compute_prob(rules_new, y)
            T = T0 ** (1 - iter / self.num_iterations)  # temperature for simulated annealing
            pt_new = sum(prob)
            alpha = np.exp(float(pt_new - pt_curr) / T)

            if pt_new &gt; sum(maps[chain][-1][1]):
                maps[chain].append([iter, prob, rules_new, [self.rules_[i] for i in rules_new]])
                if verbose:
                    print((
                        &#39;\n** chain = {}, max at iter = {} ** \n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}&#39;
                        &#39;\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\n&#39;).format(
                        chain, iter, (cfmatrix[0] + cfmatrix[2] + 0.0) / len(y), cfmatrix[0], cfmatrix[1],
                        cfmatrix[2], cfmatrix[3], sum(prob), prob[0], prob[1], prob[2])
                    )
                    self.print_rules(rules_new)
                    print(rules_new)
            if random() &lt;= alpha:
                rules_curr_norm, rules_curr, pt_curr = rules_norm.copy(), rules_new.copy(), pt_new
    pt_max = [sum(maps[chain][-1][1]) for chain in range(self.num_chains)]
    index = pt_max.index(max(pt_max))
    self.rules_ = maps[index][-1][3]
    return self</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.generate_rules"><code class="name flex">
<span>def <span class="ident">generate_rules</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<section class="desc"><p>This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top n_rules rules that make data have the biggest decrease in entropy
there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen&lt;=3, fpgrowth can generates rules much faster than randomforest.
If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_rules(self, X, y):
    &#39;&#39;&#39;This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top n_rules rules that make data have the biggest decrease in entropy
    there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen&lt;=3, fpgrowth can generates rules much faster than randomforest.
    If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories.
    &#39;&#39;&#39;

    df = 1 - X  # df has negative associations
    df.columns = [name.strip() + &#39;_neg&#39; for name in X.columns]
    df = pd.concat([X, df], axis=1)
    if self.discretization_method == &#39;fpgrowth&#39; and self.maxlen &lt;= 3:
        itemMatrix = [[item for item in df.columns if row[item] == 1] for i, row in df.iterrows()]
        pindex = np.where(y == 1)[0]
        rules = fpgrowth([itemMatrix[i] for i in pindex], supp=self.supp, zmin=1, zmax=self.maxlen)
        rules = [tuple(np.sort(rule[0])) for rule in rules]
        rules = list(set(rules))
    else:
        &#39;&#39;&#39;todo: replace this with imodels.RFDiscretizer
        &#39;&#39;&#39;
        rules = []
        for length in range(1, self.maxlen + 1, 1):
            n_estimators = min(pow(df.shape[1], length), 4000)
            clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=length)
            clf.fit(X, y)
            for n in range(n_estimators):
                rules.extend(extract_rules(clf.estimators_[n], df.columns))
        rules = [list(x) for x in set(tuple(x) for x in rules)]
    self.rules_ = rules

    # select the top n_rules rules using secondary criteria, information gain
    self.screen_rules(df, y)  # updates self.rules_
    self.set_pattern_space()</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.normalize"><code class="name flex">
<span>def <span class="ident">normalize</span></span>(<span>self, rules_new)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize(self, rules_new):
    try:
        rules_len = [len(self.rules_[index]) for index in rules_new]
        rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]
        p1 = 0
        while p1 &lt; len(rules):
            for p2 in range(p1 + 1, len(rules), 1):
                if set(self.rules_[rules[p2]]).issubset(set(self.rules_[rules[p1]])):
                    rules.remove(rules[p1])
                    p1 -= 1
                    break
            p1 += 1
        return rules
    except:
        return rules_new.copy()</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.normalize_add"><code class="name flex">
<span>def <span class="ident">normalize_add</span></span>(<span>self, rules_new, rule_index)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_add(self, rules_new, rule_index):
    rules = rules_new.copy()
    for rule in rules_new:
        if set(self.rules_[rule]).issubset(self.rules_[rule_index]):
            return rules_new.copy()
        if set(self.rules_[rule_index]).issubset(self.rules_[rule]):
            rules.remove(rule)
    rules.append(rule_index)
    return rules</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    check_is_fitted(self)
    if isinstance(X, np.ndarray):
        df = pd.DataFrame(X, columns=self.attr_names_orig)
    else:
        df = X
    Z = [[]] * len(self.rules_)
    dfn = 1 - df  # df has negative associations
    dfn.columns = [name.strip() + &#39;_neg&#39; for name in df.columns]
    df = pd.concat([df, dfn], axis=1)
    for i, rule in enumerate(self.rules_):
        Z[i] = (np.sum(df[list(rule)], axis=1) == len(rule)).astype(int)
    Yhat = (np.sum(Z, axis=0) &gt; 0).astype(int)
    return Yhat</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X):
    raise Exception(&#39;BOA does not support predicted probabilities.&#39;)</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.print_rules"><code class="name flex">
<span>def <span class="ident">print_rules</span></span>(<span>self, rules_max)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_rules(self, rules_max):
    for rule_index in rules_max:
        print(self.rules_[rule_index])</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.propose"><code class="name flex">
<span>def <span class="ident">propose</span></span>(<span>self, rules_curr, rules_norm, q, y)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def propose(self, rules_curr, rules_norm, q, y):
    nRules = len(self.rules_)
    yhat = (np.sum(self.RMatrix[:, rules_curr], axis=1) &gt; 0).astype(int)
    incorr = np.where(y != yhat)[0]
    N = len(rules_curr)

    if len(incorr) == 0:
        # BOA correctly classified all points but there could be redundant patterns, so cleaning is needed
        move = [&#39;clean&#39;]
    else:
        ex = sample(incorr.tolist(), 1)[0]
        t = random()
        if y[ex] == 1 or N == 1:
            if t &lt; 1.0 / 2 or N == 1:
                move = [&#39;add&#39;]  # action: add
            else:
                move = [&#39;cut&#39;, &#39;add&#39;]  # action: replace
        else:
            if t &lt; 1.0 / 2:
                move = [&#39;cut&#39;]  # action: cut
            else:
                move = [&#39;cut&#39;, &#39;add&#39;]  # action: replace
    if move[0] == &#39;cut&#39;:
        &#34;&#34;&#34; cut &#34;&#34;&#34;
        if random() &lt; q:
            candidate = list(set(np.where(self.RMatrix[ex, :] == 1)[0]).intersection(rules_curr))
            if len(candidate) == 0:
                candidate = rules_curr
            cut_rule = sample(candidate, 1)[0]
        else:
            p = []
            all_sum = np.sum(self.RMatrix[:, rules_curr], axis=1)
            for index, rule in enumerate(rules_curr):
                yhat = ((all_sum - np.array(self.RMatrix[:, rule])) &gt; 0).astype(int)
                TP, FP, TN, FN = get_confusion_matrix(yhat, y)
                p.append(TP.astype(float) / (TP + FP + 1))
            p = [x - min(p) for x in p]
            p = np.exp(p)
            p = np.insert(p, 0, 0)
            p = np.array(list(accumulate(p)))
            if p[-1] == 0:
                index = sample(range(len(rules_curr)), 1)[0]
            else:
                p = p / p[-1]
            index = find_lt(p, random())
            cut_rule = rules_curr[index]
        rules_curr.remove(cut_rule)
        rules_norm = self.normalize(rules_curr)
        move.remove(&#39;cut&#39;)

    if len(move) &gt; 0 and move[0] == &#39;add&#39;:
        &#34;&#34;&#34; add &#34;&#34;&#34;
        if random() &lt; q:
            add_rule = sample(range(nRules), 1)[0]
        else:
            Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:, rules_curr], axis=1) &lt; 1)[0])
            mat = np.multiply(self.RMatrix[Yhat_neg_index, :].transpose(), y[Yhat_neg_index])
            TP = np.sum(mat, axis=1)
            FP = np.array((np.sum(self.RMatrix[Yhat_neg_index, :], axis=0) - TP))
            p = (TP.astype(float) / (TP + FP + 1))
            p[rules_curr] = 0
            add_rule = sample(np.where(p == max(p))[0].tolist(), 1)[0]
        if add_rule not in rules_curr:
            rules_curr.append(add_rule)
            rules_norm = self.normalize(rules_curr)

    if len(move) &gt; 0 and move[0] == &#39;clean&#39;:
        remove = []
        for i, rule in enumerate(rules_norm):
            yhat = (np.sum(
                self.RMatrix[:, [rule for j, rule in enumerate(rules_norm) if (j != i and j not in remove)]],
                axis=1) &gt; 0).astype(int)
            TP, FP, TN, FN = get_confusion_matrix(yhat, y)
            if TP + FP == 0:
                remove.append(i)
        for x in remove:
            rules_norm.remove(x)
        return rules_curr, rules_norm
    return rules_curr, rules_norm</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.screen_rules"><code class="name flex">
<span>def <span class="ident">screen_rules</span></span>(<span>self, df, y)</span>
</code></dt>
<dd>
<section class="desc"><p>Screening rules using information gain</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def screen_rules(self, df, y):
    &#39;&#39;&#39;Screening rules using information gain
    &#39;&#39;&#39;
    item_ind_dict = {}
    for i, name in enumerate(df.columns):
        item_ind_dict[name] = i
    indices = np.array(
        list(itertools.chain.from_iterable([[item_ind_dict[x] for x in rule] for rule in self.rules_])))
    len_rules = [len(rule) for rule in self.rules_]
    indptr = list(accumulate(len_rules))
    indptr.insert(0, 0)
    indptr = np.array(indptr)
    data = np.ones(len(indices))
    rule_matrix = csc_matrix((data, indices, indptr), shape=(len(df.columns), len(self.rules_)))
    mat = np.matrix(df) @ rule_matrix
    len_matrix = np.array([len_rules] * df.shape[0])
    Z = (mat == len_matrix).astype(int)
    Zpos = [Z[i] for i in np.where(y &gt; 0)][0]
    TP = np.array(np.sum(Zpos, axis=0).tolist()[0])
    supp_select = np.where(TP &gt;= self.supp * sum(y) / 100)[0]
    FP = np.array(np.sum(Z, axis=0))[0] - TP
    TN = len(y) - np.sum(y) - FP
    FN = np.sum(y) - TP
    p1 = TP.astype(float) / (TP + FP)
    p2 = FN.astype(float) / (FN + TN)
    pp = (TP + FP).astype(float) / (TP + FP + TN + FN)
    cond_entropy = -pp * (p1 * np.log(p1) + (1 - p1) * np.log(1 - p1)) - (1 - pp) * (
            p2 * np.log(p2) + (1 - p2) * np.log(1 - p2))
    cond_entropy[p1 * (1 - p1) == 0] = -((1 - pp) * (p2 * np.log(p2) + (1 - p2) * np.log(1 - p2)))[
        p1 * (1 - p1) == 0]
    cond_entropy[p2 * (1 - p2) == 0] = -(pp * (p1 * np.log(p1) + (1 - p1) * np.log(1 - p1)))[p2 * (1 - p2) == 0]
    cond_entropy[p1 * (1 - p1) * p2 * (1 - p2) == 0] = 0
    select = np.argsort(cond_entropy[supp_select])[::-1][-self.n_rules:]
    self.rules_ = [self.rules_[i] for i in supp_select[select]]
    self.RMatrix = np.array(Z[:, supp_select[select]])</code></pre>
</details>
</dd>
<dt id="imodels.rule_set.boa.BOAClassifier.set_pattern_space"><code class="name flex">
<span>def <span class="ident">set_pattern_space</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute the rule space from the levels in each attribute</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_pattern_space(self):
    &#34;&#34;&#34;Compute the rule space from the levels in each attribute
    &#34;&#34;&#34;
    # add feat_neg to each existing feature feat
    for item in self.attr_names:
        self.attr_level_num[item + &#39;_neg&#39;] = self.attr_level_num[item]
    tmp = [item + &#39;_neg&#39; for item in self.attr_names]
    self.attr_names.extend(tmp)

    # set up pattern_space
    self.pattern_space = np.zeros(self.maxlen + 1)
    for k in range(1, self.maxlen + 1, 1):
        for subset in combinations(self.attr_names, k):
            tmp = 1
            for i in subset:
                tmp = tmp * self.attr_level_num[i]
            # print(&#39;subset&#39;, subset, &#39;tmp&#39;, tmp, &#39;k&#39;, k)
            self.pattern_space[k] = self.pattern_space[k] + tmp</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="imodels.rule_set" href="index.html">imodels.rule_set</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="imodels.rule_set.boa.accumulate" href="#imodels.rule_set.boa.accumulate">accumulate</a></code></li>
<li><code><a title="imodels.rule_set.boa.extract_rules" href="#imodels.rule_set.boa.extract_rules">extract_rules</a></code></li>
<li><code><a title="imodels.rule_set.boa.find_lt" href="#imodels.rule_set.boa.find_lt">find_lt</a></code></li>
<li><code><a title="imodels.rule_set.boa.get_confusion_matrix" href="#imodels.rule_set.boa.get_confusion_matrix">get_confusion_matrix</a></code></li>
<li><code><a title="imodels.rule_set.boa.log_betabin" href="#imodels.rule_set.boa.log_betabin">log_betabin</a></code></li>
<li><code><a title="imodels.rule_set.boa.log_gampoiss" href="#imodels.rule_set.boa.log_gampoiss">log_gampoiss</a></code></li>
<li><code><a title="imodels.rule_set.boa.random" href="#imodels.rule_set.boa.random">random</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="imodels.rule_set.boa.BOAClassifier" href="#imodels.rule_set.boa.BOAClassifier">BOAClassifier</a></code></h4>
<ul class="two-column">
<li><code><a title="imodels.rule_set.boa.BOAClassifier.compute_prob" href="#imodels.rule_set.boa.BOAClassifier.compute_prob">compute_prob</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.fit" href="#imodels.rule_set.boa.BOAClassifier.fit">fit</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.generate_rules" href="#imodels.rule_set.boa.BOAClassifier.generate_rules">generate_rules</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.normalize" href="#imodels.rule_set.boa.BOAClassifier.normalize">normalize</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.normalize_add" href="#imodels.rule_set.boa.BOAClassifier.normalize_add">normalize_add</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.predict" href="#imodels.rule_set.boa.BOAClassifier.predict">predict</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.predict_proba" href="#imodels.rule_set.boa.BOAClassifier.predict_proba">predict_proba</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.print_rules" href="#imodels.rule_set.boa.BOAClassifier.print_rules">print_rules</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.propose" href="#imodels.rule_set.boa.BOAClassifier.propose">propose</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.screen_rules" href="#imodels.rule_set.boa.BOAClassifier.screen_rules">screen_rules</a></code></li>
<li><code><a title="imodels.rule_set.boa.BOAClassifier.set_pattern_space" href="#imodels.rule_set.boa.BOAClassifier.set_pattern_space">set_pattern_space</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>